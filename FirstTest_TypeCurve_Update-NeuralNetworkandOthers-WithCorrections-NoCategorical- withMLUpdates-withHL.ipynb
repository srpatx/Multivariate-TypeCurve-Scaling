{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89a3c47-5906-4d5b-b19b-37584e59c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import tempfile\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import shap\n",
    "import folium\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy import stats, optimize\n",
    "from scipy.stats import zscore\n",
    "from shapely.geometry import LineString\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from folium.plugins import Draw\n",
    "from folium import Map, Marker\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, LabelEncoder, RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, DMatrix, train as xgb_train\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Embedding, Flatten, Conv1D, MaxPooling1D, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback,ModelCheckpoint\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "from IPython.display import display, HTML, IFrame, clear_output\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.gridspec as gridspec\n",
    "from PIL import Image\n",
    "from textwrap import wrap\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from fpdf import FPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd46999-2011-4d3e-92e4-277b4367ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from XGBoost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*deprecated.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*not used.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*No visible GPU is found.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2726567-fe88-4b4a-b402-793090b83e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".jp-OutputArea-output {\n",
       "    overflow-y: auto;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".jp-OutputArea-output {\n",
    "    overflow-y: auto;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fd9d9b-e4c6-4d81-82d8-31ba056b01e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              UWI             UWI      NNAZ_1_UWI      NNAZ_2_UWI  \\\n",
      "0  42317410760000  42317410760000  42317408170000  42317410790000   \n",
      "1  42317410770000  42317410770000  42317410800000  42317410790000   \n",
      "2  42317410780000  42317410780000  42317410730000  42317410740000   \n",
      "3  42317410790000  42317410790000  42317410760000  42317410770000   \n",
      "4  42317410800000  42317410800000  42317414210100  42317410770000   \n",
      "\n",
      "       NNAZ_3_UWI      NNAZ_4_UWI      NNAZ_5_UWI      NNAZ_6_UWI  \\\n",
      "0  42317408110000  42317410770000  42317410770000  42317410770000   \n",
      "1  42317414210100  42317410760000  42317410760000  42317410760000   \n",
      "2  42317410930000  42317440000000  42317440000000  42317440000000   \n",
      "3  42317408170000  42317410800000  42317410800000  42317410800000   \n",
      "4  42317414230000  42317410790000  42317410790000  42317410790000   \n",
      "\n",
      "       NNSZ_1_UWI      NNSZ_2_UWI  \n",
      "0  42317408170000  42317410790000  \n",
      "1  42317410800000  42317410790000  \n",
      "2  42317410730000  42317410930000  \n",
      "3  42317410760000  42317410770000  \n",
      "4  42317414210100  42317410770000  \n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "#file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\Sample data from Vesna vLandedWells.xlsx' \n",
    "# # file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\Prakhar_new.csv'\n",
    "# df = pd.read_excel(file_path)\n",
    "# Define the file path\n",
    "file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\Prakhar_Testnew2.xlsx'\n",
    "\n",
    "# Define the UWI columns to be read as strings\n",
    "uwi_columns = ['UWI', 'NNAZ_1_UWI', 'NNAZ_2_UWI', 'NNAZ_3_UWI',\n",
    "               'NNAZ_4_UWI', 'NNAZ_5_UWI', 'NNAZ_6_UWI', \n",
    "               'NNSZ_1_UWI', 'NNSZ_2_UWI']\n",
    "\n",
    "# Create a dtype dictionary where UWI columns are set to str\n",
    "dtype_dict = {col: str for col in uwi_columns}\n",
    "\n",
    "# Load the CSV file with the specified dtype\n",
    "df = pd.read_excel(file_path, dtype=dtype_dict)\n",
    "\n",
    "# Convert UWI columns to ensure they are correctly formatted as strings\n",
    "df['UWI'] = df['UWI'].apply(lambda x: '{:.0f}'.format(float(x)) if pd.notnull(x) else '')\n",
    "for col in uwi_columns:\n",
    "    df[col] = df[col].apply(lambda x: '{:.0f}'.format(float(x)) if pd.notnull(x) else '')\n",
    "\n",
    "# Display some of the UWI column values to verify\n",
    "print(df[['UWI'] + uwi_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27cd4c8-677a-40f2-a96b-b83f344f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the first Excel sheet (cumulative data)\n",
    "# file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\Cumulative Prod for sample data.xlsx'\n",
    "# cumulative_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac77be5a-c012-4abd-a49c-8028dcce15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(df, cumulative_df, on='UWI10', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d592bc28-5b48-4e47-9a18-9a507bac41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns based on the specified formulas\n",
    "df['Cumulative oil mbo'] = df['CumLiquid'] / 1000\n",
    "df['Cumulative gas mmcf'] = df['CumGas'] / 1000\n",
    "df['Cumulative water mbbl'] = df['CumWater'] / 1000\n",
    "df['FluidPerFoot_bblft'] = df['FluidPerFoot'] / 42\n",
    "df['FORMATION_CONDENSED'] = df['FORMATION_CONDENSE']\n",
    "df = df[(df['FluidPerFoot_bblft'] <= 300) & (df['ProppantPerFoot'] <= 7000)]\n",
    "# Drop the original columns\n",
    "df.drop(columns=['CumLiquid', 'CumGas', 'CumWater', 'FluidPerFoot'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc669e9-1d5c-4884-bdbc-e14e37e13e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify rows in df that do not have a match in cumulative_df\n",
    "# no_match_df = df[df['Cumulative oil mbo'].isnull() & \n",
    "#                         df['Cumulative gas mmcf'].isnull() & \n",
    "#                         df['Cumulative water mbbl'].isnull()]\n",
    "# # Print the UWI10 values that did not have a match\n",
    "# if no_match_df.empty:\n",
    "#     print(\"All UWI10 values in the detailed well data have corresponding matches in the cumulative data.\")\n",
    "# else:\n",
    "#     print(\"UWI10 values in the detailed well data that did not have a corresponding match in the cumulative data:\")\n",
    "#     print(no_match_df['UWI10'].tolist())\n",
    "\n",
    "#     # Print the rows with no matching cumulative data\n",
    "#     print(\"\\nRows with no matching cumulative data:\")\n",
    "#     print(no_match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04a0816-4229-4b04-a560-18380c57c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def robust_parse(x):\n",
    "#     if pd.isna(x):\n",
    "#         return [None] * 7  # Handle NaNs by returning a list of Nones\n",
    "#     try:\n",
    "#         if isinstance(x, str):\n",
    "#             return ast.literal_eval(x)  # Attempt to parse string as a Python literal\n",
    "#         else:\n",
    "#             return [x] + [None] * 6  # If x is not a string, return it as the first element with None padding\n",
    "#     except Exception as e:\n",
    "#         #print(f\"Failed to parse: {x} with error {e}\")  # Optionally log the error for debugging\n",
    "#         return [None] * 7  # Return None values if parsing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a5b336-146c-47fb-bcb1-6dcadef9dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to robustly parse the values\n",
    "def robust_parse(x):\n",
    "    if pd.isna(x):\n",
    "        return [None] * 7  # Handle NaNs by returning a list of Nones\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return ast.literal_eval(x)  # Attempt to parse string as a Python literal\n",
    "        else:\n",
    "            return [x] + [None] * 6  # If x is not a string, return it as the first element with None padding\n",
    "    except Exception as e:\n",
    "        return [None] * 7  # Return None values if parsing fails\n",
    "\n",
    "# Function to split parameter columns into new columns\n",
    "def split_parameters(df, param_columns):\n",
    "    new_columns = []\n",
    "    for col in param_columns:\n",
    "        expanded_cols = [f'{col}_Method', f'{col}_BuildupRate', f'{col}_MonthsInProd', \n",
    "                         f'{col}_InitialProd', f'{col}_DiCoefficient', f'{col}_BCoefficient', \n",
    "                         f'{col}_LimDeclineRate']\n",
    "        temp_df = pd.DataFrame(df[col].apply(robust_parse).tolist(), columns=expanded_cols)\n",
    "        # Convert numeric columns to float\n",
    "        for num_col in expanded_cols:\n",
    "            if num_col.endswith('_Method'):\n",
    "                temp_df[num_col] = temp_df[num_col].astype(str)  # Ensuring 'Method' is of string type\n",
    "            else:\n",
    "                temp_df[num_col] = pd.to_numeric(temp_df[num_col], errors='coerce')  # Convert to numeric and handle errors\n",
    "        df = pd.concat([df, temp_df], axis=1)\n",
    "        new_columns.extend(expanded_cols)\n",
    "    df.drop(columns=param_columns, inplace=True)\n",
    "    return df, new_columns\n",
    "\n",
    "# Function to handle missing values differently for categorical and numerical columns\n",
    "def handle_missing_values(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    for column in df.columns:\n",
    "        if column in categorical_columns:\n",
    "            df[column] = df[column].fillna('Unknown')  # Using 'Unknown' for categorical data\n",
    "        else:\n",
    "            df[column] = df[column].fillna(0)  # Assuming 0 is a reasonable fill for numerical data\n",
    "    return df\n",
    "\n",
    "# Function to drop rows where any of the new parameter columns have missing data\n",
    "def drop_na_parameter_columns(df, new_columns):\n",
    "    df.dropna(subset=[col for col in new_columns if 'Params' in col], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to group by 'Typecurve' and count occurrences, and sum the 'UWI10' values\n",
    "def group_and_summarize(df):\n",
    "    dfnew = df[['UWI10', 'Typecurve']].drop_duplicates()\n",
    "    grouped_df = dfnew.groupby('Typecurve', as_index=False).count()\n",
    "    uwi10_sum = grouped_df['UWI10'].sum()\n",
    "    return grouped_df, uwi10_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f257ec1f-0a35-4f2c-a83d-3b8dbec4a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Typecurve  UWI10\n",
      "0    CoyoteValley_North     96\n",
      "1    CoyoteValley_South    273\n",
      "2        FarEast_Howard    109\n",
      "3                  Hutt    184\n",
      "4           Mercury_600     75\n",
      "..                  ...    ...\n",
      "109            proj96_A     73\n",
      "110            proj96_B    262\n",
      "111              proj97    121\n",
      "112              proj98    246\n",
      "113              proj99    105\n",
      "\n",
      "[114 rows x 2 columns]\n",
      "13599\n"
     ]
    }
   ],
   "source": [
    "# Columns to be parsed\n",
    "param_columns = [\n",
    "    'Oil_Params_P20', 'Gas_Params_P20', 'Oil_Params_P35', 'Gas_Params_P35', \n",
    "    'Oil_Params_P50', 'Gas_Params_P50', 'Oil_Params_P65', 'Gas_Params_P65', \n",
    "    'Oil_Params_P80', 'Gas_Params_P80', 'Water_Params_P50'\n",
    "]\n",
    "# # Apply robust parsing to each column\n",
    "# for col in param_columns:\n",
    "#     df[col] = df[col].apply(robust_parse)\n",
    "# # Split each parameter into its own column\n",
    "# new_columns = []\n",
    "# for col in param_columns:\n",
    "#     expanded_cols = [f'{col}_Method', f'{col}_BuildupRate', f'{col}_MonthsInProd', \n",
    "#                      f'{col}_InitialProd', f'{col}_DiCoefficient', f'{col}_BCoefficient', \n",
    "#                      f'{col}_LimDeclineRate']\n",
    "#     temp_df = pd.DataFrame(df[col].tolist(), columns=expanded_cols)\n",
    "#     # Convert numeric columns to float\n",
    "#     for num_col in expanded_cols:\n",
    "#         if num_col.endswith('_Method'):\n",
    "#             temp_df[num_col] = temp_df[num_col].astype(str)  # Ensuring 'Method' is of string type\n",
    "#         else:\n",
    "#             temp_df[num_col] = pd.to_numeric(temp_df[num_col], errors='coerce')  # Convert to numeric and handle errors\n",
    "#     df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "# # Drop the original parameter columns\n",
    "# df.drop(columns=param_columns, inplace=True)\n",
    "# Apply the functions in sequence\n",
    "df, new_columns = split_parameters(df, param_columns)\n",
    "df = handle_missing_values(df)\n",
    "df = drop_na_parameter_columns(df, new_columns)\n",
    "grouped_df, uwi10_sum = group_and_summarize(df)\n",
    "\n",
    "# Display the results\n",
    "print(grouped_df)\n",
    "print(uwi10_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a909ff39-cecf-4078-888d-11a4cf63649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handling NaN values differently for categorical and numerical columns\n",
    "# categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# for column in df.columns:\n",
    "#     if column in categorical_columns:\n",
    "#         df[column] = df[column].fillna('Unknown')  # Using 'Unknown' for categorical data\n",
    "#     else:\n",
    "#         df[column] = df[column].fillna(0)  # Assuming 0 is a reasonable fill for numerical data\n",
    "# # Drop rows where any of the new parameter columns have missing data\n",
    "# df.dropna(subset=[col for col in df.columns if 'Params' in col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df43928-3ba5-4fdd-838a-71caf85b5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df.columns\n",
    "# dfnew= df[['UWI10','Typecurve']].drop_duplicates()\n",
    "# dfnew.groupby('Typecurve',as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8cb77d1-22ef-403d-b45f-4eeceb02e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouping by 'Typecurve' and counting the occurrences\n",
    "# grouped_df = dfnew.groupby('Typecurve', as_index=False).count()\n",
    "\n",
    "# # Summing the values in the 'UWI10' column\n",
    "# uwi10_sum = grouped_df['UWI10'].sum()\n",
    "# uwi10_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35675888-c9d3-4300-aa7c-c237e540b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_zero_rows(df, columns):\n",
    "    \"\"\"\n",
    "    Drops rows from the DataFrame where specified columns have zero values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    columns (list of str): The columns to check for zero values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with rows dropped where specified columns have zero values.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        initial_count = len(df)\n",
    "        df = df[df[column] != 0]\n",
    "        after_drop_count = len(df)\n",
    "        print(f\"Data dropped after removing zero '{column}': {initial_count - after_drop_count}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74248b96-003c-4329-80be-fa3044a0e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_absolute_values(df):\n",
    "    \"\"\"\n",
    "    Converts columns with '_HZDIST' and '_VTDIST' in their headers to absolute values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with columns to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with specified columns converted to absolute values.\n",
    "    \"\"\"\n",
    "    columns_to_convert = [col for col in df.columns if '_HZDIST' in col or '_VTDIST' in col]\n",
    "    \n",
    "    df[columns_to_convert] = df[columns_to_convert].abs()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c506c07d-4928-4ec7-aa0b-0fc432c75dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_water_params(df):\n",
    "    # Define the oil and water parameter columns\n",
    "    oil_columns = {\n",
    "        'InitialProd': 'Oil_Params_P50_InitialProd',\n",
    "        'DiCoefficient': 'Oil_Params_P50_DiCoefficient',\n",
    "        'BCoefficient': 'Oil_Params_P50_BCoefficient',\n",
    "        'BuildupRate': 'Oil_Params_P50_BuildupRate',\n",
    "        'MonthsInProd': 'Oil_Params_P50_MonthsInProd',\n",
    "        'LimDeclineRate': 'Oil_Params_P50_LimDeclineRate'\n",
    "    }\n",
    "\n",
    "    water_columns = {\n",
    "        'InitialProd': 'Water_Params_P50_InitialProd',\n",
    "        'DiCoefficient': 'Water_Params_P50_DiCoefficient',\n",
    "        'BCoefficient': 'Water_Params_P50_BCoefficient',\n",
    "        'BuildupRate': 'Water_Params_P50_BuildupRate',\n",
    "        'MonthsInProd': 'Water_Params_P50_MonthsInProd',\n",
    "        'LimDeclineRate': 'Water_Params_P50_LimDeclineRate',\n",
    "        'EUR_30yr_Actual_Water_P50_MBBL': 'EUR_30yr_Actual_Water_P50_MBBL'\n",
    "    }\n",
    "\n",
    "    # Replace missing water values with 3 times the corresponding oil values or same as oil values\n",
    "    for param, oil_col in oil_columns.items():\n",
    "        water_col = water_columns.get(param)\n",
    "        if water_col:\n",
    "            if param == 'InitialProd':\n",
    "                df[water_col] = df.apply(\n",
    "                    lambda row: row[oil_col] * 3 if row[water_col] == 0 else row[water_col], axis=1\n",
    "                )\n",
    "            else:\n",
    "                df[water_col] = df.apply(\n",
    "                    lambda row: row[oil_col] if row[water_col] == 0 else row[water_col], axis=1\n",
    "                )\n",
    "            \n",
    "    df['EUR_30yr_Actual_Water_P50_MBBL'] = df.apply(\n",
    "        lambda row: row['EUR_30yr_Actual_Oil_P50_MBO'] * 3 if row['EUR_30yr_Actual_Water_P50_MBBL'] == 0 else row['EUR_30yr_Actual_Water_P50_MBBL'], axis=1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c1dc55-00a6-42b9-9e57-f5e991554a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace zeros with the P50 value for the same category\n",
    "def replace_zeros_with_P50(df):\n",
    "    # Replace for EUR values\n",
    "    phases = ['Oil', 'Gas', 'Water']  # Assuming Water is also needed; adjust as necessary\n",
    "    years = ['30yr']  # Adjust or extend if there are other year ranges\n",
    "    \n",
    "    for phase in phases:\n",
    "        for year in years:\n",
    "            p50_col = f'EUR_{year}_Actual_{phase}_P50_' + ('MBO' if phase != 'Gas' else 'MMCF')\n",
    "            if phase == 'Water':\n",
    "                p50_col = f'EUR_{year}_Actual_{phase}_P50_MBBL'  # Assuming water is measured in MBBL\n",
    "\n",
    "            for p in ['P20', 'P35', 'P65', 'P80']:\n",
    "                p_col = f'EUR_{year}_Actual_{phase}_{p}_' + ('MBO' if phase != 'Gas' else 'MMCF')\n",
    "                if phase == 'Water':\n",
    "                    p_col = f'EUR_{year}_Actual_{phase}_{p}_MBBL'\n",
    "                \n",
    "                if p_col in df.columns and p50_col in df.columns:\n",
    "                    df.loc[df[p_col] == 0, p_col] = df[p50_col]\n",
    "\n",
    "    # Replace for parameters\n",
    "    params = ['Method', 'BuildupRate', 'MonthsInProd', 'InitialProd', 'DiCoefficient', 'BCoefficient', 'LimDeclineRate']\n",
    "    ###Log\n",
    "    for phase in ['Oil', 'Gas', 'Water']:  # Assuming Water parameters are also needed\n",
    "        for param in params:\n",
    "            p50_col = f'{phase}_Params_P50_{param}'\n",
    "            for p in ['P20', 'P35', 'P65', 'P80']:\n",
    "                p_col = f'{phase}_Params_{p}_{param}'\n",
    "                if p_col in df.columns and p50_col in df.columns:\n",
    "                    df.loc[df[p_col] == 0, p_col] = df[p50_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872caf2e-cb97-4353-ab45-e61c1ae07d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dropped after removing zero 'FluidPerFoot_bblft': 1935\n",
      "Data dropped after removing zero 'ProppantPerFoot': 0\n",
      "Data dropped after removing zero 'EUR_30yr_Actual_Gas_P50_MMCF': 4\n",
      "Data dropped after removing zero 'EUR_30yr_Actual_Oil_P50_MBO': 1\n",
      "Data dropped after removing zero 'HEELPOINT_LAT': 339\n"
     ]
    }
   ],
   "source": [
    "# columns_to_check = ['FluidPerFoot_bblft', 'ProppantPerFoot', 'EUR_30yr_Actual_Gas_P50_MMCF', \n",
    "#                     'EUR_30yr_Actual_Oil_P50_MBO', 'EUR_30yr_Actual_Water_P50_MBBL', 'HEELPOINT_LAT']\n",
    "columns_to_check = ['FluidPerFoot_bblft', 'ProppantPerFoot', 'EUR_30yr_Actual_Gas_P50_MMCF', \n",
    "                    'EUR_30yr_Actual_Oil_P50_MBO', 'HEELPOINT_LAT']\n",
    "# Dropping columns with missing values in columns_to_check list\n",
    "df = drop_zero_rows(df, columns_to_check)\n",
    "#Converting _HZDIST and _VTDIST columns to absolute values\n",
    "df = convert_to_absolute_values(df)\n",
    "# Replace missing water parameters when missing\n",
    "df = replace_missing_water_params(df)\n",
    "# Replace missing other P parameters with P50 values if missing\n",
    "replace_zeros_with_P50(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce231359-cf53-44a8-96d2-9734aadbcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_well_map(df, output_html='wells_map.html'):\n",
    "    \"\"\"\n",
    "    Creates a folium map with well data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with well data.\n",
    "    output_html (str): The filename for the output HTML map.\n",
    "    \n",
    "    Returns:\n",
    "    folium.Map: The generated folium map.\n",
    "    \"\"\"\n",
    "    # Drop rows where latitude or longitude values are missing\n",
    "    df.dropna(subset=['HEELPOINT_LAT', 'HEELPOINT_LON', 'MIDPOINT_LAT', 'MIDPOINT_LON', 'TOEPOINT_LAT', 'TOEPOINT_LON'], inplace=True)\n",
    "    \n",
    "    # Create a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['HEELPOINT_LON'], df['HEELPOINT_LAT']))\n",
    "    \n",
    "    # Create a LineString for each well\n",
    "    gdf['line'] = gdf.apply(lambda row: LineString([\n",
    "        (row['HEELPOINT_LON'], row['HEELPOINT_LAT']),\n",
    "        (row['MIDPOINT_LON'], row['MIDPOINT_LAT']),\n",
    "        (row['TOEPOINT_LON'], row['TOEPOINT_LAT'])\n",
    "    ]), axis=1)\n",
    "    \n",
    "    # Create a GeoDataFrame with the LineStrings\n",
    "    line_gdf = gpd.GeoDataFrame(gdf, geometry='line')\n",
    "    \n",
    "    # Calculate bounds to set the map's initial view\n",
    "    bounds = line_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    center = [(bounds[1] + bounds[3]) / 2, (bounds[0] + bounds[2]) / 2]  # [(miny + maxy)/2, (minx + maxx)/2]\n",
    "    \n",
    "    # Create a folium map centered on the calculated center\n",
    "    m = folium.Map(location=center, zoom_start=10)\n",
    "    \n",
    "    # Fit map to bounds\n",
    "    m.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n",
    "    \n",
    "    # Add the lines to the map\n",
    "    for _, row in line_gdf.iterrows():\n",
    "        line_points = [\n",
    "            (row['HEELPOINT_LAT'], row['HEELPOINT_LON']),\n",
    "            (row['MIDPOINT_LAT'], row['MIDPOINT_LON']),\n",
    "            (row['TOEPOINT_LAT'], row['TOEPOINT_LON'])\n",
    "        ]\n",
    "        folium.PolyLine(line_points, color='blue').add_to(m)\n",
    "        # Add marker for midpoint with a popup showing coordinates or other info\n",
    "        folium.Marker(location=line_points[1], popup=f'Well ID: {row[\"WellName\"]}', tooltip='Click for info').add_to(m)\n",
    "    \n",
    "    # Add draw control to the map to allow for area selection\n",
    "    draw = Draw(export=True)\n",
    "    m.add_child(draw)\n",
    "    \n",
    "    # Save the map to an HTML file\n",
    "    m.save(output_html)\n",
    "    \n",
    "    return m\n",
    "#display(IFrame('wells_map.html', width=700, height=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ceeea1a-3cab-4871-9bc6-6249d4425104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = create_well_map(df)\n",
    "#m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dada01b1-a802-4f62-af3b-e07545964bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_neighbor_eur_cumulative(df):\n",
    "#     if df is None:\n",
    "#         raise ValueError(\"The input DataFrame is None. Please provide a valid DataFrame.\")\n",
    "\n",
    "#     # Check if the UWI column exists\n",
    "#     if 'UWI' not in df.columns:\n",
    "#         raise ValueError(\"The input DataFrame does not contain the 'UWI' column.\")\n",
    "\n",
    "#     # Define the EUR columns we're interested in\n",
    "#     eur_oil_columns = ['EUR_30yr_Actual_Oil_P20_MBO', 'EUR_30yr_Actual_Oil_P35_MBO', \n",
    "#                        'EUR_30yr_Actual_Oil_P50_MBO', 'EUR_30yr_Actual_Oil_P65_MBO', \n",
    "#                        'EUR_30yr_Actual_Oil_P80_MBO']\n",
    "#     eur_gas_columns = ['EUR_30yr_Actual_Gas_P20_MMCF', 'EUR_30yr_Actual_Gas_P35_MMCF', \n",
    "#                        'EUR_30yr_Actual_Gas_P50_MMCF', 'EUR_30yr_Actual_Gas_P65_MMCF', \n",
    "#                        'EUR_30yr_Actual_Gas_P80_MMCF']\n",
    "#     cumulative_columns = ['Cumulative oil mbo', 'Cumulative gas mmcf', 'Cumulative water mbbl']\n",
    "\n",
    "#     # Check if the required EUR columns exist\n",
    "#     missing_columns = [col for col in eur_oil_columns + eur_gas_columns + cumulative_columns if col not in df.columns]\n",
    "#     if missing_columns:\n",
    "#         raise ValueError(f\"The input DataFrame is missing the following required columns: {missing_columns}\")\n",
    "\n",
    "#     # Create a mapping DataFrame that will be used for mapping EUR values\n",
    "#     eur_cumulative_map = df.set_index('UWI')[eur_oil_columns + eur_gas_columns + cumulative_columns].fillna(0).copy()\n",
    "\n",
    "#     # Create a dictionary to hold the new columns\n",
    "#     new_columns = {}\n",
    "\n",
    "#     # Iterate over NNAZ and NNSZ columns\n",
    "#     for prefix in ['NNAZ', 'NNSZ']:\n",
    "#         num_cols = 6 if prefix == 'NNAZ' else 2  # Assuming 6 NNAZ and 2 NNSZ columns\n",
    "#         for i in range(1, num_cols + 1):\n",
    "#             uwi_col = f'{prefix}_{i}_UWI'\n",
    "#             if uwi_col in df.columns:  # Ensure the UWI column exists\n",
    "#                 for eur_col in eur_oil_columns + eur_gas_columns + cumulative_columns:\n",
    "#                     new_col_name = f'{prefix}_{i}_{eur_col}'\n",
    "#                     # Use the map function to add EUR values from the eur_cumulative_map DataFrame\n",
    "#                     new_columns[new_col_name] = df[uwi_col].map(eur_cumulative_map[eur_col])\n",
    "\n",
    "#     # Concatenate the new columns to the original dataframe\n",
    "#     df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "#     return df\n",
    "# Define bounds for each basin in a dictionary.\n",
    "# The keys are basin names, and the values are tuples of (lat_min, lat_max, lon_min, lon_max).\n",
    "basin_bounds = {\n",
    "    'Midland': {'lat_range': (29, 34), 'lon_range': (-110, -109)}\n",
    "    # Add more basins with their geographic bounds \n",
    "}\n",
    "def assign_basin_tc(row):\n",
    "    # Check if BasinTC is 0 or missing (use pd.isna() and explicitly handle pd.NA)\n",
    "    if pd.isna(row['BasinTC'])== 'Unknown' or row['BasinTC'] == 0:\n",
    "        matched_basin = None\n",
    "        for basin, bounds in basin_bounds.items():\n",
    "            # Ensure all comparisons are done within bounds\n",
    "            if ((bounds['lat_range'][0] <= row['HEELPOINT_LAT'] <= bounds['lat_range'][1] and\n",
    "                 bounds['lon_range'][0] <= row['HEELPOINT_LON'] <= bounds['lon_range'][1]) or\n",
    "                (bounds['lat_range'][0] <= row['MIDPOINT_LAT'] <= bounds['lat_range'][1] and\n",
    "                 bounds['lon_range'][0] <= row['MIDPOINT_LON'] <= bounds['lon_range'][1]) or\n",
    "                (bounds['lat_range'][0] <= row['TOEPOINT_LAT'] <= bounds['lat_range'][1] and\n",
    "                 bounds['lon_range'][0] <= row['TOEPOINT_LON'] <= bounds['lon_range'][1])):\n",
    "                matched_basin = basin\n",
    "                break\n",
    "        return matched_basin if matched_basin else row['BasinTC']\n",
    "    return row['BasinTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14661522-593b-4fb5-8e12-5086cec19165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_neighbor_eur_cumulative(df):\n",
    "#     if df is None:\n",
    "#         raise ValueError(\"The input DataFrame is None. Please provide a valid DataFrame.\")\n",
    "\n",
    "#     # Check if the UWI column exists\n",
    "#     if 'UWI' not in df.columns:\n",
    "#         raise ValueError(\"The input DataFrame does not contain the 'UWI' column.\")\n",
    "\n",
    "#     # Define the EUR columns we're interested in\n",
    "#     eur_oil_columns = ['EUR_30yr_Actual_Oil_P20_MBO', 'EUR_30yr_Actual_Oil_P35_MBO', \n",
    "#                        'EUR_30yr_Actual_Oil_P50_MBO', 'EUR_30yr_Actual_Oil_P65_MBO', \n",
    "#                        'EUR_30yr_Actual_Oil_P80_MBO']\n",
    "#     eur_gas_columns = ['EUR_30yr_Actual_Gas_P20_MMCF', 'EUR_30yr_Actual_Gas_P35_MMCF', \n",
    "#                        'EUR_30yr_Actual_Gas_P50_MMCF', 'EUR_30yr_Actual_Gas_P65_MMCF', \n",
    "#                        'EUR_30yr_Actual_Gas_P80_MMCF']\n",
    "#     cumulative_columns = ['Cumulative oil mbo', 'Cumulative gas mmcf', 'Cumulative water mbbl']\n",
    "\n",
    "#     # Check if the required EUR columns exist\n",
    "#     missing_columns = [col for col in eur_oil_columns + eur_gas_columns + cumulative_columns if col not in df.columns]\n",
    "#     if missing_columns:\n",
    "#         raise ValueError(f\"The input DataFrame is missing the following required columns: {missing_columns}\")\n",
    "\n",
    "#     # Create a mapping DataFrame that will be used for mapping EUR values\n",
    "#     eur_cumulative_map = df.set_index('UWI')[eur_oil_columns + eur_gas_columns + cumulative_columns].fillna(0).copy()\n",
    "    \n",
    "#     # Ensure the index is unique\n",
    "#     if not eur_cumulative_map.index.is_unique:\n",
    "#         eur_cumulative_map = eur_cumulative_map.groupby(eur_cumulative_map.index).first()\n",
    "\n",
    "#     # Create a dictionary to hold the new columns\n",
    "#     new_columns = {}\n",
    "\n",
    "#     # Iterate over NNAZ and NNSZ columns\n",
    "#     for prefix in ['NNAZ', 'NNSZ']:\n",
    "#         num_cols = 6 if prefix == 'NNAZ' else 2  # Assuming 6 NNAZ and 2 NNSZ columns\n",
    "#         for i in range(1, num_cols + 1):\n",
    "#             uwi_col = f'{prefix}_{i}_UWI'\n",
    "#             if uwi_col in df.columns:  # Ensure the UWI column exists\n",
    "#                 for eur_col in eur_oil_columns + eur_gas_columns + cumulative_columns:\n",
    "#                     new_col_name = f'{prefix}_{i}_{eur_col}'\n",
    "#                     # Use the map function to add EUR values from the eur_cumulative_map DataFrame\n",
    "#                     new_columns[new_col_name] = df[uwi_col].map(eur_cumulative_map[eur_col])\n",
    "\n",
    "#     # Concatenate the new columns to the original dataframe\n",
    "#     df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df331bb2-cf34-4d1f-8753-24905ff9ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_parameters(df, horizontal_well_length_column, apply_scaling=True):\n",
    "    \"\"\"\n",
    "    Scales the specified parameters by the horizontal well length using the provided formula.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the relevant columns.\n",
    "        horizontal_well_length_column (str): The name of the column containing the horizontal well length.\n",
    "        apply_scaling (bool): Whether to apply the scaling. If True, scaling is applied and the horizontal well length column is removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with scaled values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the columns to be scaled based on the provided conditions\n",
    "    scaling_columns = [col for col in df.columns if 'InitialProd' in col or\n",
    "                       'Cumulative oil mbo' in col or\n",
    "                       'Cumulative gas mmcf' in col or\n",
    "                       'Cumulative water mbbl' in col or\n",
    "                       'EUR_30yr_Actual_Oil' in col or\n",
    "                       'EUR_30yr_Actual_Gas' in col or\n",
    "                       'EUR_30yr_Actual_Water' in col]\n",
    "\n",
    "    if apply_scaling:\n",
    "        # Apply the scaling formula to each relevant column\n",
    "        for col in scaling_columns:\n",
    "            df[col] = df.apply(lambda row: row[col] * (\n",
    "                (1 - 0.03 * (10000 - row[horizontal_well_length_column]) / 2500) * \n",
    "                10000 / row[horizontal_well_length_column]), axis=1)\n",
    "\n",
    "        # Drop the horizontal well length column if scaling is applied\n",
    "        df = df.drop(columns=[horizontal_well_length_column])\n",
    "\n",
    "    return df\n",
    "    \n",
    "def add_neighbor_eur_cumulative(df):\n",
    "    if df is None:\n",
    "        raise ValueError(\"The input DataFrame is None. Please provide a valid DataFrame.\")\n",
    "\n",
    "    # Check if the UWI and CompletionDate columns exist\n",
    "    if 'UWI' not in df.columns:\n",
    "        raise ValueError(\"The input DataFrame does not contain the 'UWI' column.\")\n",
    "    if 'CompletionDate' not in df.columns:\n",
    "        raise ValueError(\"The input DataFrame does not contain the 'CompletionDate' column.\")\n",
    "\n",
    "    # Define the EUR columns we're interested in\n",
    "    eur_oil_columns = ['EUR_30yr_Actual_Oil_P20_MBO', 'EUR_30yr_Actual_Oil_P35_MBO', \n",
    "                       'EUR_30yr_Actual_Oil_P50_MBO', 'EUR_30yr_Actual_Oil_P65_MBO', \n",
    "                       'EUR_30yr_Actual_Oil_P80_MBO']\n",
    "    eur_gas_columns = ['EUR_30yr_Actual_Gas_P20_MMCF', 'EUR_30yr_Actual_Gas_P35_MMCF', \n",
    "                       'EUR_30yr_Actual_Gas_P50_MMCF', 'EUR_30yr_Actual_Gas_P65_MMCF', \n",
    "                       'EUR_30yr_Actual_Gas_P80_MMCF']\n",
    "    cumulative_columns = ['Cumulative oil mbo', 'Cumulative gas mmcf', 'Cumulative water mbbl']\n",
    "\n",
    "    # Check if the required EUR columns exist\n",
    "    missing_columns = [col for col in eur_oil_columns + eur_gas_columns + cumulative_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The input DataFrame is missing the following required columns: {missing_columns}\")\n",
    "\n",
    "    # Ensure CompletionDate is in datetime format and drop rows with missing dates\n",
    "    # df['CompletionDate'] = pd.to_datetime(df['CompletionDate'], errors='coerce')\n",
    "    # df = df.dropna(subset=['CompletionDate'])\n",
    "\n",
    "    # Create a mapping DataFrame that will be used for mapping EUR values and CompletionDate\n",
    "    eur_cumulative_map = df.set_index('UWI')[eur_oil_columns + eur_gas_columns + cumulative_columns + ['CompletionDate']].fillna(0).copy()\n",
    "    \n",
    "    # Ensure the index is unique\n",
    "    if not eur_cumulative_map.index.is_unique:\n",
    "        eur_cumulative_map = eur_cumulative_map.groupby(eur_cumulative_map.index).first()\n",
    "\n",
    "    # Create a dictionary to hold the new columns\n",
    "    new_columns = {}\n",
    "\n",
    "    # Iterate over NNAZ and NNSZ columns\n",
    "    for prefix in ['NNAZ', 'NNSZ']:\n",
    "        num_cols = 6 if prefix == 'NNAZ' else 2  # Assuming 6 NNAZ and 2 NNSZ columns\n",
    "        for i in range(1, num_cols + 1):\n",
    "            uwi_col = f'{prefix}_{i}_UWI'\n",
    "            if uwi_col in df.columns:  # Ensure the UWI column exists\n",
    "                for eur_col in eur_oil_columns + eur_gas_columns + cumulative_columns:\n",
    "                    new_col_name = f'{prefix}_{i}_{eur_col}'\n",
    "                    # Use the map function to add EUR values from the eur_cumulative_map DataFrame\n",
    "                    new_columns[new_col_name] = df[uwi_col].map(eur_cumulative_map[eur_col]).fillna(0)\n",
    "                \n",
    "                # # Calculate TimeProduced\n",
    "                # time_produced_col = f'{prefix}_{i}_TimeProduced'\n",
    "                # completion_dates = df[uwi_col].map(eur_cumulative_map['CompletionDate'])\n",
    "                # time_produced = (pd.Timestamp.now() - completion_dates).dt.days / 30.44  # Convert days to months\n",
    "                # new_columns[time_produced_col] = time_produced.fillna(0)\n",
    "\n",
    "    # Concatenate the new columns to the original dataframe\n",
    "    df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd4a1a0-79da-4ece-8e23-0929926004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to fill zero parameters\n",
    "def fill_zero_parameters(df):\n",
    "    for prefix in ['NNAZ', 'NNSZ']:\n",
    "        num_cols = 6 if prefix == 'NNAZ' else 2\n",
    "        for i in range(1, num_cols + 1):\n",
    "            hzd_col = f'{prefix}_{i}_HZDIST'\n",
    "            true_col = f'{prefix}_{i}_TRUEDIST'\n",
    "            vt_col = f'{prefix}_{i}_VTDIST'\n",
    "            \n",
    "            if hzd_col in df.columns:\n",
    "                # Replace zero values in _HZDIST with 5280\n",
    "                df[hzd_col] = df[hzd_col].replace(0, 5280)\n",
    "            \n",
    "            if true_col in df.columns and vt_col in df.columns:\n",
    "                # Calculate _TRUEDIST using sqrt(_HZDIST^2 + _VTDIST^2)\n",
    "                df[true_col] = np.sqrt(df[hzd_col] ** 2 + df[vt_col] ** 2)\n",
    "    return df\n",
    "# Define the function to check for mismatches, TRUEDIST values < 100, and drop those rows\n",
    "def check_and_clean_parameters(df):\n",
    "    mismatches = {}\n",
    "    tru_dist_below_100 = {}\n",
    "    tru_dist_below_100_indices = set()\n",
    "    \n",
    "    for prefix in ['NNAZ', 'NNSZ']:\n",
    "        num_cols = 6 if prefix == 'NNAZ' else 2\n",
    "        for i in range(1, num_cols + 1):\n",
    "            hzd_col = f'{prefix}_{i}_HZDIST'\n",
    "            true_col = f'{prefix}_{i}_TRUEDIST'\n",
    "            vt_col = f'{prefix}_{i}_VTDIST'\n",
    "            \n",
    "            if hzd_col in df.columns and true_col in df.columns and vt_col in df.columns:\n",
    "                # Check for mismatches\n",
    "                calculated_true_dist = np.sqrt(df[hzd_col] ** 2 + df[vt_col] ** 2)\n",
    "                mismatched_indices = df[(df[true_col] != 0) & (df[true_col] != calculated_true_dist)].index\n",
    "                mismatches[true_col] = mismatched_indices.tolist()\n",
    "\n",
    "                # Check for TRUEDIST values < 100\n",
    "                tru_dist_below_100_indices.update(df[df[true_col] < 100].index)\n",
    "                tru_dist_below_100[true_col] = df[df[true_col] < 100].index.tolist()\n",
    "    \n",
    "    # Drop rows with TRUEDIST values < 100\n",
    "    df_cleaned = df.drop(list(tru_dist_below_100_indices))\n",
    "    \n",
    "    return mismatches, tru_dist_below_100, df_cleaned\n",
    "# Function to check for zeros in the specified columns\n",
    "def check_for_zeros(df):\n",
    "    zero_columns = []\n",
    "    for prefix in ['NNAZ', 'NNSZ']:\n",
    "        num_cols = 6 if prefix == 'NNAZ' else 2\n",
    "        for i in range(1, num_cols + 1):\n",
    "            hzd_col = f'{prefix}_{i}_HZDIST'\n",
    "            true_col = f'{prefix}_{i}_TRUEDIST'\n",
    "            if hzd_col in df.columns and (df[hzd_col] == 0).any():\n",
    "                zero_columns.append(hzd_col)\n",
    "            if true_col in df.columns and (df[true_col] == 0).any():\n",
    "                zero_columns.append(true_col)\n",
    "    return zero_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1eec734-24e6-4751-8a77-62beecb4da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display columns and let the user select the ones to drop\n",
    "def select_and_drop_columns(df):\n",
    "    # Display the columns with indices\n",
    "    print(\"Available columns:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i}: {col}\")\n",
    "\n",
    "    # Ask the user to input the indices of columns to drop, separated by commas\n",
    "    selected_indices = input(\"Enter the indices of columns you want to drop, separated by commas: \")\n",
    "    selected_indices = list(map(int, selected_indices.split(',')))\n",
    "\n",
    "    # Drop the selected columns\n",
    "    columns_to_drop = [df.columns[i] for i in selected_indices]\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bb65679-0cde-4873-b349-124e385e9534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\1188512361.py:64: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  eur_cumulative_map = df.set_index('UWI')[eur_oil_columns + eur_gas_columns + cumulative_columns + ['CompletionDate']].fillna(0).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns contain zeros after replacement.\n"
     ]
    }
   ],
   "source": [
    "apply_scaling=False\n",
    "df = scale_parameters(df, 'HORIZONTIAL_WELL_LENGTH', apply_scaling)\n",
    "df = add_neighbor_eur_cumulative(df)\n",
    "# Apply the function to each row\n",
    "df['BasinTC'] = df.apply(assign_basin_tc, axis=1)\n",
    "# Drop rows where 'BasinTC' is 'Unknown'\n",
    "df = df[df['BasinTC'] != 'Unknown']\n",
    "# Assume 'df' is your DataFrame\n",
    "df = fill_zero_parameters(df)\n",
    "zero_columns = check_for_zeros(df)\n",
    "# Check for mismatches, TRUEDIST values < 100, and clean the DataFrame\n",
    "mismatches, tru_dist_below_100, df = check_and_clean_parameters(df)\n",
    "\n",
    "if zero_columns:\n",
    "    print(f\"Columns with zeros after replacement: {zero_columns}\")\n",
    "else:\n",
    "    print(\"No columns contain zeros after replacement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d2bf8b4-5624-43dd-a712-723081e53485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and pass the DataFrame\n",
    "#df = select_and_drop_columns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bd45723-7bc8-4a1a-a01a-29ea70744a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    " columns_to_drop=['UWI10', 'CompletionDate' ,'UWI', 'WellName','NNAZ_1_UWI','NNAZ_2_UWI','NNAZ_3_UWI','NNAZ_4_UWI','NNAZ_5_UWI',\n",
    "                 'NNAZ_6_UWI','NNSZ_1_UWI',\n",
    " 'NNSZ_2_UWI','LeaseName', 'WellNumber', 'CurrentOperatorName', 'OriginalOperatorName', 'DrillingContractorName', 'PermitDate', \n",
    "                 'SpudDate','FORMATION_CONDENSE', 'Unique_PDP_ID','EUR_30yr_Actual_Oil_P20_MBO',\n",
    " 'EUR_30yr_Actual_Gas_P20_MMCF', 'EUR_30yr_Actual_Oil_P35_MBO', 'EUR_30yr_Actual_Gas_P35_MMCF', 'EUR_30yr_Actual_Oil_P50_MBO', \n",
    "                 'EUR_30yr_Actual_Gas_P50_MMCF',\n",
    " 'EUR_30yr_Actual_Oil_P65_MBO', 'EUR_30yr_Actual_Gas_P65_MMCF', 'EUR_30yr_Actual_Oil_P80_MBO', 'EUR_30yr_Actual_Gas_P80_MMCF',\n",
    "                 'EUR_30yr_Actual_Water_P50_MBBL','WELL_TORTUOSITY','DEPTH_TO_TOP_2Q',\n",
    " 'DEPTH_TO_TOP_3Q', 'DEPTH_TO_TOP_4Q', 'AZIMUTH','DEPTH_ABOVE_ZONE_2Q',  'DEPTH_ABOVE_ZONE_3Q', 'DEPTH_ABOVE_ZONE_4Q','Cumulative oil mbo', \n",
    " 'Cumulative gas mmcf', 'Cumulative water mbbl','AVERAGE_INCLINATION','DEPTH_TO_TOP_1Q','HEELPOINT_DEPTH','TOEPOINT_DEPTH', 'NNSZ_1_FORMATION',\n",
    " 'NNSZ_2_FORMATION','NNAZ_1_FORMATION', 'NNAZ_2_FORMATION', 'NNAZ_3_FORMATION', 'NNAZ_4_FORMATION', 'NNAZ_5_FORMATION', 'NNAZ_6_FORMATION'\n",
    "                  ,\n",
    " 'NNAZ_3_EUR_30yr_Actual_Oil_P20_MBO', 'NNAZ_3_EUR_30yr_Actual_Oil_P35_MBO', 'NNAZ_3_EUR_30yr_Actual_Oil_P50_MBO',\n",
    " 'NNAZ_3_EUR_30yr_Actual_Oil_P65_MBO', 'NNAZ_3_EUR_30yr_Actual_Oil_P80_MBO', 'NNAZ_3_EUR_30yr_Actual_Gas_P20_MMCF',\n",
    " 'NNAZ_3_EUR_30yr_Actual_Gas_P35_MMCF', 'NNAZ_3_EUR_30yr_Actual_Gas_P50_MMCF', 'NNAZ_3_EUR_30yr_Actual_Gas_P65_MMCF',\n",
    " 'NNAZ_3_EUR_30yr_Actual_Gas_P80_MMCF', 'NNAZ_3_Cumulative oil mbo', 'NNAZ_3_Cumulative gas mmcf', 'NNAZ_3_Cumulative water mbbl',\n",
    " 'NNAZ_4_EUR_30yr_Actual_Oil_P20_MBO', 'NNAZ_4_EUR_30yr_Actual_Oil_P35_MBO', 'NNAZ_4_EUR_30yr_Actual_Oil_P50_MBO',\n",
    " 'NNAZ_4_EUR_30yr_Actual_Oil_P65_MBO', 'NNAZ_4_EUR_30yr_Actual_Oil_P80_MBO', 'NNAZ_4_EUR_30yr_Actual_Gas_P20_MMCF',\n",
    " 'NNAZ_4_EUR_30yr_Actual_Gas_P35_MMCF', 'NNAZ_4_EUR_30yr_Actual_Gas_P50_MMCF', 'NNAZ_4_EUR_30yr_Actual_Gas_P65_MMCF',\n",
    " 'NNAZ_4_EUR_30yr_Actual_Gas_P80_MMCF', 'NNAZ_4_Cumulative oil mbo', 'NNAZ_4_Cumulative gas mmcf',\n",
    " 'NNAZ_4_Cumulative water mbbl', 'NNAZ_5_EUR_30yr_Actual_Oil_P20_MBO', 'NNAZ_5_EUR_30yr_Actual_Oil_P35_MBO',\n",
    " 'NNAZ_5_EUR_30yr_Actual_Oil_P50_MBO', 'NNAZ_5_EUR_30yr_Actual_Oil_P65_MBO', 'NNAZ_5_EUR_30yr_Actual_Oil_P80_MBO',\n",
    " 'NNAZ_5_EUR_30yr_Actual_Gas_P20_MMCF', 'NNAZ_5_EUR_30yr_Actual_Gas_P35_MMCF', 'NNAZ_5_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    " 'NNAZ_5_EUR_30yr_Actual_Gas_P65_MMCF', 'NNAZ_5_EUR_30yr_Actual_Gas_P80_MMCF', 'NNAZ_5_Cumulative oil mbo',\n",
    " 'NNAZ_5_Cumulative gas mmcf', 'NNAZ_5_Cumulative water mbbl', 'NNAZ_6_EUR_30yr_Actual_Oil_P20_MBO',\n",
    " 'NNAZ_6_EUR_30yr_Actual_Oil_P35_MBO', 'NNAZ_6_EUR_30yr_Actual_Oil_P50_MBO', 'NNAZ_6_EUR_30yr_Actual_Oil_P65_MBO',\n",
    " 'NNAZ_6_EUR_30yr_Actual_Oil_P80_MBO', 'NNAZ_6_EUR_30yr_Actual_Gas_P20_MMCF', 'NNAZ_6_EUR_30yr_Actual_Gas_P35_MMCF',\n",
    " 'NNAZ_6_EUR_30yr_Actual_Gas_P50_MMCF', 'NNAZ_6_EUR_30yr_Actual_Gas_P65_MMCF', 'NNAZ_6_EUR_30yr_Actual_Gas_P80_MMCF',\n",
    " 'NNAZ_6_Cumulative oil mbo', 'NNAZ_6_Cumulative gas mmcf', 'NNAZ_6_Cumulative water mbbl',\n",
    "                 'DEPTH_ABOVE_ZONE_1Q',\n",
    " # ,\n",
    " 'NNAZ_1_EUR_30yr_Actual_Oil_P20_MBO', 'NNAZ_1_EUR_30yr_Actual_Oil_P35_MBO',  'NNAZ_1_EUR_30yr_Actual_Oil_P65_MBO',\n",
    " 'NNAZ_1_EUR_30yr_Actual_Oil_P80_MBO', 'NNAZ_1_EUR_30yr_Actual_Gas_P20_MMCF', 'NNAZ_1_EUR_30yr_Actual_Gas_P35_MMCF',\n",
    " 'NNAZ_1_EUR_30yr_Actual_Gas_P65_MMCF', 'NNAZ_1_EUR_30yr_Actual_Gas_P80_MMCF', 'NNAZ_2_EUR_30yr_Actual_Oil_P20_MBO',\n",
    " 'NNAZ_2_EUR_30yr_Actual_Oil_P35_MBO', 'NNAZ_2_EUR_30yr_Actual_Oil_P65_MBO', 'NNAZ_2_EUR_30yr_Actual_Oil_P80_MBO',\n",
    " 'NNAZ_2_EUR_30yr_Actual_Gas_P20_MMCF', 'NNAZ_2_EUR_30yr_Actual_Gas_P35_MMCF', 'NNAZ_2_EUR_30yr_Actual_Gas_P65_MMCF',\n",
    " 'NNAZ_2_EUR_30yr_Actual_Gas_P80_MMCF', 'NNSZ_1_EUR_30yr_Actual_Oil_P20_MBO', 'NNSZ_1_EUR_30yr_Actual_Oil_P35_MBO',\n",
    " 'NNSZ_1_EUR_30yr_Actual_Oil_P65_MBO', 'NNSZ_1_EUR_30yr_Actual_Oil_P80_MBO', 'NNSZ_1_EUR_30yr_Actual_Gas_P20_MMCF',\n",
    " 'NNSZ_1_EUR_30yr_Actual_Gas_P35_MMCF', 'NNSZ_1_EUR_30yr_Actual_Gas_P65_MMCF', 'NNSZ_1_EUR_30yr_Actual_Gas_P80_MMCF',\n",
    " 'NNSZ_2_EUR_30yr_Actual_Oil_P20_MBO', 'NNSZ_2_EUR_30yr_Actual_Oil_P35_MBO', 'NNSZ_2_EUR_30yr_Actual_Oil_P65_MBO',\n",
    " 'NNSZ_2_EUR_30yr_Actual_Oil_P80_MBO', 'NNSZ_2_EUR_30yr_Actual_Gas_P20_MMCF', 'NNSZ_2_EUR_30yr_Actual_Gas_P35_MMCF',\n",
    " 'NNSZ_2_EUR_30yr_Actual_Gas_P65_MMCF', 'NNSZ_2_EUR_30yr_Actual_Gas_P80_MMCF',\n",
    "                 'NNAZ_3_TRUEDIST',\n",
    " 'NNAZ_3_HZDIST',\n",
    " 'NNAZ_3_VTDIST',\n",
    " 'NNAZ_4_TRUEDIST',\n",
    " 'NNAZ_4_HZDIST',\n",
    " 'NNAZ_4_VTDIST',\n",
    " 'NNAZ_5_TRUEDIST',\n",
    " 'NNAZ_5_HZDIST',\n",
    " 'NNAZ_5_VTDIST',\n",
    " 'NNAZ_6_TRUEDIST',\n",
    " 'NNAZ_6_HZDIST',\n",
    " 'NNAZ_6_VTDIST',\n",
    "'Oil_Params_P20_BuildupRate',\n",
    " 'Oil_Params_P20_MonthsInProd',\n",
    " 'Oil_Params_P20_InitialProd',\n",
    " 'Oil_Params_P20_DiCoefficient',\n",
    " 'Oil_Params_P20_BCoefficient',\n",
    " 'Oil_Params_P20_LimDeclineRate',\n",
    " 'Gas_Params_P20_BuildupRate',\n",
    " 'Gas_Params_P20_MonthsInProd',\n",
    " 'Gas_Params_P20_InitialProd',\n",
    " 'Gas_Params_P20_DiCoefficient',\n",
    " 'Gas_Params_P20_BCoefficient',\n",
    " 'Gas_Params_P20_LimDeclineRate',\n",
    " 'Oil_Params_P35_BuildupRate',\n",
    " 'Oil_Params_P35_MonthsInProd',\n",
    " 'Oil_Params_P35_InitialProd',\n",
    " 'Oil_Params_P35_DiCoefficient',\n",
    " 'Oil_Params_P35_BCoefficient',\n",
    " 'Oil_Params_P35_LimDeclineRate',\n",
    " 'Gas_Params_P35_BuildupRate',\n",
    " 'Gas_Params_P35_MonthsInProd',\n",
    " 'Gas_Params_P35_InitialProd',\n",
    " 'Gas_Params_P35_DiCoefficient',\n",
    " 'Gas_Params_P35_BCoefficient',\n",
    " 'Gas_Params_P35_LimDeclineRate',\n",
    "                  'Oil_Params_P65_BuildupRate',\n",
    " 'Oil_Params_P65_MonthsInProd',\n",
    " 'Oil_Params_P65_InitialProd',\n",
    " 'Oil_Params_P65_DiCoefficient',\n",
    " 'Oil_Params_P65_BCoefficient',\n",
    " 'Oil_Params_P65_LimDeclineRate',\n",
    " 'Gas_Params_P65_BuildupRate',\n",
    " 'Gas_Params_P65_MonthsInProd',\n",
    " 'Gas_Params_P65_InitialProd',\n",
    " 'Gas_Params_P65_DiCoefficient',\n",
    " 'Gas_Params_P65_BCoefficient',\n",
    " 'Gas_Params_P65_LimDeclineRate',\n",
    " 'Oil_Params_P80_BuildupRate',\n",
    " 'Oil_Params_P80_MonthsInProd',\n",
    " 'Oil_Params_P80_InitialProd',\n",
    " 'Oil_Params_P80_DiCoefficient',\n",
    " 'Oil_Params_P80_BCoefficient',\n",
    " 'Oil_Params_P80_LimDeclineRate',\n",
    " 'Gas_Params_P80_BuildupRate',\n",
    " 'Gas_Params_P80_MonthsInProd',\n",
    " 'Gas_Params_P80_InitialProd',\n",
    " 'Gas_Params_P80_DiCoefficient',\n",
    " 'Gas_Params_P80_BCoefficient',\n",
    " 'Gas_Params_P80_LimDeclineRate',\n",
    "                 'PERCENT_IN_ZONE',\n",
    "                  'Gas_Params_P50_LimDeclineRate',\n",
    "                  'Oil_Params_P50_LimDeclineRate',\n",
    "                  'Water_Params_P50_LimDeclineRate',\n",
    "                  'MIDPOINT_DEPTH',\n",
    "                   'NNSZ_2_EUR_30yr_Actual_Oil_P50_MBO',\n",
    " 'NNSZ_2_EUR_30yr_Actual_Oil_P50_MBO',\n",
    " 'NNSZ_2_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    " 'NNSZ_2_Cumulative oil mbo',\n",
    " 'NNSZ_2_Cumulative gas mmcf',\n",
    " 'NNSZ_2_Cumulative water mbbl',\n",
    " 'NNAZ_2_TRUEDIST',\n",
    " 'NNAZ_2_HZDIST',\n",
    " 'NNAZ_2_VTDIST',\n",
    "                  'MIDPOINT_DEPTH',\n",
    " 'NNAZ_2_EUR_30yr_Actual_Oil_P50_MBO',\n",
    " 'NNAZ_2_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    " 'NNAZ_2_Cumulative oil mbo',\n",
    " 'NNAZ_2_Cumulative gas mmcf',\n",
    " 'NNAZ_2_Cumulative water mbbl',\n",
    "                  'NNSZ_2_TRUEDIST',\n",
    " 'NNSZ_2_HZDIST',\n",
    " 'NNSZ_2_VTDIST'\n",
    "                  \n",
    "]\n",
    "# Dropping the columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if '_Method' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "#months_in_prod_columns = [col for col in df.columns if 'LimDeclineRate' in col]\n",
    "#df.drop(months_in_prod_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6d6bdcb-ae9a-4f96-a615-dd02b0cd39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_null_counts = df.count()\n",
    "\n",
    "# # Display columns with discrepancies in their count of non-null values\n",
    "# # This assumes the DataFrame has rows where some columns may be consistently non-null\n",
    "# max_count = non_null_counts.max()\n",
    "# discrepancy_columns = non_null_counts[non_null_counts < max_count]\n",
    "# discrepancy_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3f1ee63-1e6f-421b-bd97-e8d35703a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill missing values with zero\n",
    "# df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c331919b-e0bd-4605-aa95-7413dbf02e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_counts = df.count()\n",
    "\n",
    "# Display columns with discrepancies in their count of non-null values\n",
    "# This assumes the DataFrame has rows where some columns may be consistently non-null\n",
    "max_count = non_null_counts.max()\n",
    "discrepancy_columns = non_null_counts[non_null_counts < max_count]\n",
    "discrepancy_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e84878-adfc-469c-afee-d08917799480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fdfdeb9-9c1f-4a12-aaa3-dc14477e03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame shape: (13244, 40)\n"
     ]
    }
   ],
   "source": [
    "months_in_prod_columns = [col for col in df.columns if '_LAT' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if '_LON' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if 'COMPLETION_RELATIONSHIP' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if 'WELL_TRAJECTORY' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if 'PRIMARY_FORMATION' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "months_in_prod_columns = [col for col in df.columns if 'Typecurve' in col]\n",
    "df.drop(months_in_prod_columns, axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_orig = df\n",
    "\n",
    "# Print the shape of the DataFrame to confirm rows have been dropped\n",
    "print(f\"Updated DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a80523-e335-4a23-84bc-b38da8325320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combined_eur(df):\n",
    "    # Identify the relevant columns\n",
    "    oil_columns = [col for col in df.columns if col.endswith('_EUR_30yr_Actual_Oil_P50_MBO')]\n",
    "    gas_columns = [col for col in df.columns if col.endswith('_EUR_30yr_Actual_Gas_P50_MMCF')]\n",
    "    \n",
    "    # Loop through the columns to calculate the combined value\n",
    "    for oil_col in oil_columns:\n",
    "        # Derive the base name by removing the specific suffix\n",
    "        base_name = oil_col.replace('_EUR_30yr_Actual_Oil_P50_MBO', '')\n",
    "        \n",
    "        # Find the corresponding gas column\n",
    "        gas_col = base_name + '_EUR_30yr_Actual_Gas_P50_MMCF'\n",
    "        \n",
    "        if gas_col in df.columns:\n",
    "            # Calculate the combined EUR value\n",
    "            combined_col = base_name + '_EUR_Combined_P50_MBO'\n",
    "            df[combined_col] = df[oil_col] + df[gas_col] / 20\n",
    "            \n",
    "            # Drop the original oil and gas columns\n",
    "            df.drop([oil_col, gas_col], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fcc67a7-8894-4d73-9bda-42f8e1596a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_combined_eur(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e39a8632-1a81-452f-a3c4-5c565f68fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure BasinTC and FORMATION_CONDENSED are categorical\n",
    "df['BasinTC'] = df['BasinTC'].astype(str)\n",
    "df['FORMATION_CONDENSED'] = df['FORMATION_CONDENSED'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa54c0ee-7771-4fe5-b5ee-086297074f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns excluding 'BasinTC' and 'FORMATION_CONDENSED'\n",
    "categorical_columns = [col for col in df.select_dtypes(include=['object', 'category']).columns.tolist() if col not in ['BasinTC', 'FORMATION_CONDENSED']]\n",
    "# Prepare data for modeling\n",
    "y_headers = [col for col in df.columns if 'Params' in col]             # Define target headers dynamically if needed\n",
    "numerical_columns = [col for col in df.columns if col not in categorical_columns + ['BasinTC', 'FORMATION_CONDENSED', *y_headers]]\n",
    "# Update feature_columns excluding 'BasinTC' and 'FORMATION_CONDENSED'\n",
    "feature_columns = numerical_columns + categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2576420f-e403-4d14-a264-136435451258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oil_Params_P50_BuildupRate',\n",
       " 'Oil_Params_P50_MonthsInProd',\n",
       " 'Oil_Params_P50_InitialProd',\n",
       " 'Oil_Params_P50_DiCoefficient',\n",
       " 'Oil_Params_P50_BCoefficient',\n",
       " 'Gas_Params_P50_BuildupRate',\n",
       " 'Gas_Params_P50_MonthsInProd',\n",
       " 'Gas_Params_P50_InitialProd',\n",
       " 'Gas_Params_P50_DiCoefficient',\n",
       " 'Gas_Params_P50_BCoefficient',\n",
       " 'Water_Params_P50_BuildupRate',\n",
       " 'Water_Params_P50_MonthsInProd',\n",
       " 'Water_Params_P50_InitialProd',\n",
       " 'Water_Params_P50_DiCoefficient',\n",
       " 'Water_Params_P50_BCoefficient']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71ad792f-2b71-4f1f-ae90-73458a8f83ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HORIZONTIAL_WELL_LENGTH',\n",
       " 'EST_FORMATION_THICKNESS',\n",
       " 'AVERAGE_DEPTH_BELOW_TOP',\n",
       " 'NNAZ_1_TRUEDIST',\n",
       " 'NNAZ_1_HZDIST',\n",
       " 'NNAZ_1_VTDIST',\n",
       " 'NNSZ_1_TRUEDIST',\n",
       " 'NNSZ_1_HZDIST',\n",
       " 'NNSZ_1_VTDIST',\n",
       " 'AVERAGE_DEPTH_TO_NEXT_ZONE',\n",
       " 'TotalDepthTVD',\n",
       " 'ProppantPerFoot',\n",
       " 'FluidPerFoot_bblft',\n",
       " 'NNAZ_1_Cumulative oil mbo',\n",
       " 'NNAZ_1_Cumulative gas mmcf',\n",
       " 'NNAZ_1_Cumulative water mbbl',\n",
       " 'NNSZ_1_Cumulative oil mbo',\n",
       " 'NNSZ_1_Cumulative gas mmcf',\n",
       " 'NNSZ_1_Cumulative water mbbl',\n",
       " 'NNAZ_1_EUR_Combined_P50_MBO',\n",
       " 'NNSZ_1_EUR_Combined_P50_MBO']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fec15e46-65e9-40ae-8ccd-5caba1305b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4019b14-417f-407c-96e8-f9e2688a1a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min       2.9\n",
       "max    5280.0\n",
       "Name: NNAZ_1_HZDIST, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate min and max values\n",
    "min_max_values = df['NNAZ_1_HZDIST'].agg(['min', 'max']).transpose()\n",
    "min_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d790c6-13b4-48c4-8aef-9dde29313577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of unique categories and their names for each column\n",
    "# category_info = {col: {'count': df[col].nunique(), 'names': df[col].unique()} for col in categorical_columns}\n",
    "# # Print the number of categories grouped by the header name\n",
    "# for header, info in category_info.items():\n",
    "#     print(f\"{header}:\")\n",
    "#     print(f\"  Count of Categories: {info['count']}\")\n",
    "#     print(f\"  Categories: {info['names']}\\n\")\n",
    "# # Print the shape of the DataFrame to confirm rows have been dropped\n",
    "# print(f\"Updated DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b95037b-b267-471f-926d-8f9e5b3c2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to keep LabelEncoders for each column\n",
    "encoders = {}\n",
    "# Encode categorical columns and store the encoders\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))  # Convert and encode\n",
    "    encoders[col] = le  # Store the encoder for inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b488516f-2c0f-46a8-bf39-ee7aee0549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data\n",
    "def remove_outliers(df, numerical_columns, threshold=3):\n",
    "    if df is None:\n",
    "        raise ValueError(\"The input DataFrame is None. Please provide a valid DataFrame.\")\n",
    "    \n",
    "    # Check if numerical columns exist in the DataFrame\n",
    "    missing_columns = [col for col in numerical_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The input DataFrame is missing the following required columns: {missing_columns}\")\n",
    "    \n",
    "    # Calculate z-scores for the numerical columns\n",
    "    z_scores = np.abs(stats.zscore(df[numerical_columns]))\n",
    "    \n",
    "    # Apply the threshold to filter out rows with outliers\n",
    "    filter_mask = (z_scores < threshold).all(axis=1)\n",
    "    filtered_df = df[filter_mask]\n",
    "    \n",
    "    # Calculate the number of rows removed\n",
    "    rows_removed = len(df) - len(filtered_df)\n",
    "    \n",
    "    return filtered_df, rows_removed\n",
    "\n",
    "#  usage with visualization\n",
    "def visualize_outlier_removal(df, numerical_columns, pdf_filename='outlier_removal_plots.pdf'):\n",
    "    # Remove outliers and get the count of removed rows\n",
    "    df_cleaned, rows_removed = remove_outliers(df, numerical_columns)\n",
    "    \n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        # Plot before and after outlier removal side by side\n",
    "        for col in numerical_columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "            # Plot before outlier removal\n",
    "            sns.histplot(df[col], kde=True, ax=axes[0])\n",
    "            axes[0].set_title(f'Before Outlier Removal: {col}')\n",
    "\n",
    "            # Plot after outlier removal\n",
    "            sns.histplot(df_cleaned[col], kde=True, ax=axes[1])\n",
    "            axes[1].set_title(f'After Outlier Removal: {col}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)  # Save the current figure to the PDF\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"Total rows removed: {rows_removed}\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd2a0786-4e48-4985-b746-7dec63d2fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_copy=df\n",
    "#df = visualize_outlier_removal(df, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c27e8-a3cd-46de-9649-c445dcd939c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d948915f-6b51-4982-b7f6-43c3c2468f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot decline curves in semi-logarithmic scale\n",
    "# def plot_decline_curves(time_array, actual, title):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(time_array, actual, 'b-o', label='Actual')\n",
    "#     plt.yscale('log')\n",
    "#     plt.ylim(1, 100000)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Time (Months)')\n",
    "#     plt.ylabel('Production Rate (bbl/day)')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, which='both', linestyle='--')\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to generate production rates from the provided parameters\n",
    "# def generate_production_rates(df, headers, time_array, resource_type='Oil'):\n",
    "#     productions = []\n",
    "\n",
    "#     if resource_type == 'Oil':\n",
    "#         prefix = 'Oil_Params_P50_'\n",
    "#     elif resource_type == 'Gas':\n",
    "#         prefix = 'Gas_Params_P50_'\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid resource type. Must be 'Oil' or 'Gas'.\")\n",
    "\n",
    "#     for idx in range(len(df)):\n",
    "#         qi = df.iloc[idx][df.columns.get_loc(f'{prefix}InitialProd')]\n",
    "#         di = df.iloc[idx][df.columns.get_loc(f'{prefix}DiCoefficient')]\n",
    "#         b = df.iloc[idx][df.columns.get_loc(f'{prefix}BCoefficient')]\n",
    "#         IBU = df.iloc[idx][df.columns.get_loc(f'{prefix}BuildupRate')]\n",
    "#         MBU = df.iloc[idx][df.columns.get_loc(f'{prefix}MonthsInProd')]\n",
    "#         Dlim = df.iloc[idx][df.columns.get_loc(f'{prefix}LimDeclineRate')]\n",
    "        \n",
    "#         production = modified_hyperbolic(time_array, float(qi), float(di), float(b), float(Dlim), float(IBU), float(MBU))\n",
    "#         productions.append(production)\n",
    "\n",
    "#     return productions\n",
    "\n",
    "# # Function to detect spurious curves\n",
    "# def detect_spurious_curves(time_array, productions, threshold=0.5):\n",
    "#     spurious_indices = []\n",
    "#     for i, production in enumerate(productions):\n",
    "#         # Check for NaN values\n",
    "#         if any(np.isnan(production)):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#         # Check for continuously increasing production\n",
    "#         if all(np.diff(production) > 0):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "\n",
    "#         # Check for unusually long buildup period (> 1.2 years, 14.4 months)\n",
    "#         if np.argmax(production) > 14.4:\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#         # Check for extreme values\n",
    "#         if any(production <= 0) or any(production > 1e6):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#     return spurious_indices\n",
    "\n",
    "# # Function to interactively remove spurious curves\n",
    "# def remove_spurious_curves(df, headers, time_array, threshold=0.5, resource_type='Oil'):\n",
    "#     # Generate production rates\n",
    "#     productions = generate_production_rates(df, headers, time_array, resource_type)\n",
    "\n",
    "#     # Detect spurious curves\n",
    "#     spurious_indices = detect_spurious_curves(time_array, productions, threshold)\n",
    "\n",
    "#     selected_indices = []\n",
    "\n",
    "#     def plot_and_select():\n",
    "#         for idx in spurious_indices:\n",
    "#             plt.figure(figsize=(10, 6))\n",
    "#             plt.plot(time_array, productions[idx], 'b-o', label='Actual')\n",
    "#             plt.yscale('log')\n",
    "#             plt.ylim(1, 100000)\n",
    "#             plt.title(f'{resource_type} Curve {idx}')\n",
    "#             plt.xlabel('Time (Months)')\n",
    "#             plt.ylabel('Production Rate (bbl/day)')\n",
    "#             plt.legend()\n",
    "#             plt.grid(True, which='both', linestyle='--')\n",
    "#             plt.show()\n",
    "            \n",
    "#             response = input(f\"Do you want to remove {resource_type} Curve {idx}? (yes/no): \").strip().lower()\n",
    "#             if response == 'yes':\n",
    "#                 selected_indices.append(idx)\n",
    "    \n",
    "#     plot_and_select()\n",
    "#     return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d59c553c-3927-4528-8d05-9ef3ffb6c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = 5\n",
    "# time_array = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00ee3685-a4d6-4ac4-a7c9-b2b78294d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove spurious curves for oil\n",
    "# selected_indices_oil = remove_spurious_curves(df, headers, time_array, resource_type='Oil')\n",
    "# print(\"Selected spurious indices for oil:\", selected_indices_oil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50d05be3-1382-40ee-9495-e74954e69039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove spurious curves for gas\n",
    "# selected_indices_gas = remove_spurious_curves(df, headers, time_array, resource_type='Gas')yyyyyy\n",
    "# print(\"Selected spurious indices for gas:\", selected_indices_gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35fd196b-22c5-44fa-b428-6a86fc4518db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to detect spurious curves\n",
    "# def detect_spurious_curves(time_array, productions, threshold=0.5):\n",
    "#     spurious_indices = []\n",
    "#     for i, production in enumerate(productions):\n",
    "#         # Check for NaN values\n",
    "#         if any(np.isnan(production)):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#         # Check for continuously increasing production\n",
    "#         if all(np.diff(production) > 0):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "\n",
    "#         # Check for unusually long buildup period (> 2 years, 24 months)\n",
    "#         if np.argmax(production) > 24:\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#         # Check for extreme values\n",
    "#         if any(production <= 0) or any(production > 1e6):\n",
    "#             spurious_indices.append(i)\n",
    "#             continue\n",
    "        \n",
    "#     return spurious_indices\n",
    "\n",
    "# # Function to remove spurious curves automatically\n",
    "# def remove_spurious_curves(df, headers, time_array, threshold=0.5, resource_type='Oil'):\n",
    "#     # Generate production rates\n",
    "#     productions = generate_production_rates(df, headers, time_array, resource_type)\n",
    "\n",
    "#     # Detect spurious curves\n",
    "#     spurious_indices = detect_spurious_curves(time_array, productions, threshold)\n",
    "\n",
    "#     # Convert spurious_indices to the actual DataFrame index\n",
    "#     spurious_indices = df.index[spurious_indices]\n",
    "\n",
    "#     # Drop the spurious indices from the DataFrame\n",
    "#     df_cleaned = df.drop(spurious_indices).reset_index(drop=True)\n",
    "\n",
    "#     print(f\"Removed {len(spurious_indices)} spurious {resource_type.lower()} curves.\")\n",
    "#     return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d019910-91bb-4531-b86e-242188c0550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_inputs(qi, di, b, Dlim, IBU, MBU):\n",
    "    # A simple validation function where IBU and MBU can be zero\n",
    "    return all(x >= 0 for x in [IBU, MBU]) and all(x > 0 for x in [qi, di, b, Dlim])\n",
    "    \n",
    "def modified_hyperbolic(time_array, qi, di, b, Dlim, IBU, MBU, buildup_method='Linear', epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate the monthly production volume using the Modified Hyperbolic Decline Model with a buildup phase.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not validate_inputs(qi, di, b, Dlim, IBU, MBU):\n",
    "        #print(f\"Invalid inputs detected: qi={qi}, di={di}, b={b}, Dlim={Dlim}, IBU={IBU}, MBU={MBU}\")\n",
    "        return np.zeros_like(time_array)\n",
    "    \n",
    "    # Convert decline rates to nominal rates\n",
    "    try:\n",
    "        di_nominal = ((1 - 0.01 * di) ** (-b) - 1) / (12 * b)\n",
    "        Dlim_nominal = ((1 - 0.01 * Dlim) ** (-b) - 1) / (12 * b)\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        #print(f\"Invalid decline rates: di={di}, Dlim={Dlim}, b={b}. Error: {e}\")\n",
    "        return np.zeros_like(time_array)\n",
    "\n",
    "    # Calculate switch point\n",
    "    try:\n",
    "        qlim = qi * (Dlim_nominal / (di_nominal + epsilon)) ** (1.0 / b)\n",
    "        tlim = ((qi / qlim) ** b - 1.0) / (b * (di_nominal + epsilon))\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # print(f\"Invalid switch point calculations: qi={qi}, qlim={qlim}, di={di}, Dlim={Dlim}, b={b}. Error: {e}\")\n",
    "        return np.zeros_like(time_array)\n",
    "\n",
    "    # Handle the case when MBU (buildup period) is zero\n",
    "    if MBU == 0:\n",
    "        # If MBU is zero, skip the buildup phase and start with the hyperbolic decline directly\n",
    "        t_buildup = np.array([])\n",
    "        buildup_production = np.array([])\n",
    "        t_post_buildup = time_array\n",
    "        t_hyp = t_post_buildup[t_post_buildup < tlim]\n",
    "        t_exp = t_post_buildup[t_post_buildup >= tlim]\n",
    "    else:\n",
    "        # Separate time into buildup, hyperbolic, and exponential segments\n",
    "        t_buildup = time_array[time_array <= MBU]\n",
    "        t_post_buildup = time_array[time_array > MBU]\n",
    "        t_hyp = t_post_buildup[t_post_buildup < tlim]\n",
    "        t_exp = t_post_buildup[t_post_buildup >= tlim]\n",
    "\n",
    "        # Calculate production during buildup period\n",
    "        if buildup_method == 'Flat':\n",
    "            buildup_production = IBU + np.zeros_like(t_buildup)\n",
    "        elif buildup_method == 'Linear':\n",
    "            slope = (qi - IBU) / (MBU + epsilon)\n",
    "            buildup_production = IBU + slope * t_buildup\n",
    "        elif buildup_method == 'Exp':\n",
    "            slope = np.log(qi / (IBU + epsilon)) / (MBU + epsilon)\n",
    "            buildup_production = IBU * np.exp(slope * t_buildup)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown buildup method: {buildup_method}\")\n",
    "\n",
    "    # Calculate production during hyperbolic decline period\n",
    "    try:\n",
    "        q_model_hyp = qi * (1.0 + b * di_nominal * (t_hyp - MBU)) ** (-1.0 / b)\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # print(f\"Invalid hyperbolic decline calculation: qi={qi}, di={di}, b={b}. Error: {e}\")\n",
    "        q_model_hyp = np.zeros_like(t_hyp)\n",
    "\n",
    "    # Calculate production during exponential decline period\n",
    "    try:\n",
    "        q_model_exp = qlim * np.exp(-Dlim_nominal * (t_exp - tlim))\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # print(f\"Invalid exponential decline calculation: qlim={qlim}, Dlim={Dlim}, tlim={tlim}. Error: {e}\")\n",
    "        q_model_exp = np.zeros_like(t_exp)\n",
    "\n",
    "    # Smooth transition at tlim (Hyperbolic to Exponential)\n",
    "    if len(t_exp) > 0 and len(t_hyp) > 0:\n",
    "        time_fraction = (tlim - t_hyp[-1]) / (t_exp[0] - t_hyp[-1] + epsilon)\n",
    "        q_model_exp[0] = time_fraction * q_model_hyp[-1] + (1 - time_fraction) * q_model_exp[0]\n",
    "\n",
    "    # Combine production rates\n",
    "    production_post_buildup = np.concatenate((q_model_hyp, q_model_exp))\n",
    "    daily_production = np.concatenate((buildup_production, production_post_buildup))\n",
    "\n",
    "    # Calculate monthly production volumes by summing daily rates over each month\n",
    "    days_per_month = 30\n",
    "    monthly_volumes = daily_production * days_per_month\n",
    "\n",
    "    return daily_production, monthly_volumes\n",
    "\n",
    "# Function to generate production rates from the provided parameters\n",
    "def generate_production_rates(df, headers, time_array, resource_type='Oil'):\n",
    "    productions = []\n",
    "\n",
    "    if resource_type == 'Oil':\n",
    "        prefix = 'Oil_Params_P50_'\n",
    "    elif resource_type == 'Gas':\n",
    "        prefix = 'Gas_Params_P50_'\n",
    "    elif resource_type == 'Water':\n",
    "        prefix = 'Water_Params_P50_'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resource type. Must be 'Oil', 'Gas', or 'Water'.\")\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "        qi = df.iloc[idx][df.columns.get_loc(f'{prefix}InitialProd')]\n",
    "        di = df.iloc[idx][df.columns.get_loc(f'{prefix}DiCoefficient')]\n",
    "        b = df.iloc[idx][df.columns.get_loc(f'{prefix}BCoefficient')]\n",
    "        IBU = df.iloc[idx][df.columns.get_loc(f'{prefix}BuildupRate')]\n",
    "        MBU = df.iloc[idx][df.columns.get_loc(f'{prefix}MonthsInProd')]\n",
    "        Dlim = 7  # df.iloc[idx][df.columns.get_loc(f'{prefix}LimDeclineRate')]\n",
    "\n",
    "        # Check for negative values\n",
    "        if any(x < 0 for x in [qi, di, b, IBU, MBU, Dlim]):\n",
    "            productions.append(np.full_like(time_array, np.nan))  # Mark as spurious\n",
    "            continue  \n",
    "        production = modified_hyperbolic(time_array, float(qi), float(di), float(b), float(Dlim), float(IBU), float(MBU))[1]\n",
    "        #print(production)\n",
    "        productions.append(production)\n",
    "\n",
    "    return productions\n",
    "\n",
    "def detect_spurious_curves(time_array, productions, threshold=0.5):\n",
    "    spurious_indices = []\n",
    "    for i, production in enumerate(productions):\n",
    "        # Ensure production is a numpy array\n",
    "        production = np.atleast_1d(production)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if np.any(np.isnan(production)):\n",
    "            spurious_indices.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Ensure production has more than one element\n",
    "        if len(production) > 1:\n",
    "            # Check for continuously increasing production\n",
    "            if np.all(np.diff(production) > 0):\n",
    "                spurious_indices.append(i)\n",
    "                continue\n",
    "        else:\n",
    "            # If production has only one element, consider it spurious\n",
    "            spurious_indices.append(i)\n",
    "            continue\n",
    "\n",
    "        # Check for unusually long buildup period (> 2 years, 24 months)\n",
    "        if np.argmax(production) > 24:\n",
    "            spurious_indices.append(i)\n",
    "            continue\n",
    "        \n",
    "        # Check for extreme values\n",
    "        if np.any(production <= 0) or np.any(production > 1e20):\n",
    "            spurious_indices.append(i)\n",
    "            continue\n",
    "        \n",
    "    return spurious_indices\n",
    "\n",
    "\n",
    "# Function to remove spurious curves automatically\n",
    "def remove_spurious_curves(df, headers, time_array, threshold=0.5, resource_type='Oil'):\n",
    "    # Generate production rates\n",
    "    productions = generate_production_rates(df, headers, time_array, resource_type)\n",
    "\n",
    "    # Detect spurious curves\n",
    "    spurious_indices = detect_spurious_curves(time_array, productions, threshold)\n",
    "\n",
    "    # Convert spurious_indices to the actual DataFrame index\n",
    "    spurious_indices = df.index[spurious_indices]\n",
    "\n",
    "    # Drop the spurious indices from the DataFrame\n",
    "    df_cleaned = df.drop(spurious_indices).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Removed {len(spurious_indices)} spurious {resource_type.lower()} curves.\")\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2114bc8-596b-4048-ba89-a1fea74e9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\2037614077.py:100: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  qi = df.iloc[idx][df.columns.get_loc(f'{prefix}InitialProd')]\n",
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\2037614077.py:101: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  di = df.iloc[idx][df.columns.get_loc(f'{prefix}DiCoefficient')]\n",
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\2037614077.py:102: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  b = df.iloc[idx][df.columns.get_loc(f'{prefix}BCoefficient')]\n",
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\2037614077.py:103: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  IBU = df.iloc[idx][df.columns.get_loc(f'{prefix}BuildupRate')]\n",
      "C:\\Users\\Prakhar.Sarkar\\AppData\\Local\\Temp\\ipykernel_25152\\2037614077.py:104: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  MBU = df.iloc[idx][df.columns.get_loc(f'{prefix}MonthsInProd')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1730 spurious oil curves.\n",
      "Removed 431 spurious gas curves.\n",
      "Removed 31 spurious water curves.\n",
      "Original DataFrame length: 13244\n",
      "Cleaned DataFrame length: 11052\n"
     ]
    }
   ],
   "source": [
    "# Remove spurious curves for oil and gas\n",
    "years = 20\n",
    "time_array = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n",
    "\n",
    "headers = df.columns.tolist()\n",
    "# Remove spurious curves for oil and gas\n",
    "df_cleaned_oil = remove_spurious_curves(df, headers, time_array, resource_type='Oil')\n",
    "df_cleaned_gas = remove_spurious_curves(df_cleaned_oil, headers, time_array, resource_type='Gas')\n",
    "df_cleaned_water = remove_spurious_curves(df_cleaned_gas, headers, time_array, resource_type='Water')\n",
    "\n",
    "print(f\"Original DataFrame length: {len(df)}\")\n",
    "print(f\"Cleaned DataFrame length: {len(df_cleaned_water)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b47b84d1-d6ca-4633-80cb-2a927edfa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10abaac9-14c2-46ee-9942-01908357fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dropping the selected spurious curves from the DataFrame\n",
    "# spurious_indices_oil = selected_indices_oil()\n",
    "# spurious_indices_gas = selected_indices_gas()\n",
    "# df = df.drop(spurious_indices_oil + spurious_indices_gas).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ee18b3c-9649-4933-b630-f994987e8dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de2dce0d-65d1-4f7a-a671-fbd37651a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def prepare_data(df, numerical_columns, categorical_columns):\n",
    "#     df = df.copy()  # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "#     df.loc[:, categorical_columns] = df[categorical_columns].astype(np.int32)\n",
    "#     return df\n",
    "def prepare_data(df, numerical_columns, categorical_columns):\n",
    "    df = df.copy()\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    return df\n",
    "# Define function to filter data by basin and formation\n",
    "def filter_by_basin_and_formation(dfinput, basin, formation):\n",
    "    return dfinput[(dfinput['BasinTC'] == basin) & (dfinput['FORMATION_CONDENSED'] == formation)]\n",
    "def debug_shapes(train, val, test, y_headers):\n",
    "    print(f'Train X shape: {train.shape}, Train Y shape: {train[y_headers].shape}')\n",
    "    print(f'Val X shape: {val.shape}, Val Y shape: {val[y_headers].shape}')\n",
    "    print(f'Test X shape: {test.shape}, Test Y shape: {test[y_headers].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e9fcd48-d800-40d5-b2c8-ca6c120a3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling on train, validate and test sets\n",
    "#train_df = remove_outliers(train_df, numerical_columns)\n",
    "# Fit separate scalers for input features and output parameters\n",
    "# input_scaler = StandardScaler()\n",
    "# output_scaler = StandardScaler()\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply log transformation to BuildupRate and InitialProd columns before scaling\n",
    "log_transform_columns = [col for col in y_headers if 'BuildupRate' in col or 'InitialProd' in col]\n",
    "#log_transform_columns = [col for col in y_headers if 'MonthsInProd' in col or 'InitialProd' in col]\n",
    "#log_transform_columns = [col for col in y_headers if 'InitialProd' in col]\n",
    "#log_transform_columns = []\n",
    "\n",
    "# Replace negative values with zeros before log transformation\n",
    "train_df[log_transform_columns] = train_df[log_transform_columns].apply(lambda x: x.clip(lower=0))\n",
    "val_df[log_transform_columns] = val_df[log_transform_columns].apply(lambda x: x.clip(lower=0))\n",
    "test_df[log_transform_columns] = test_df[log_transform_columns].apply(lambda x: x.clip(lower=0))\n",
    "\n",
    "train_df[log_transform_columns] = np.log1p(train_df[log_transform_columns])\n",
    "val_df[log_transform_columns] = np.log1p(val_df[log_transform_columns])\n",
    "test_df[log_transform_columns] = np.log1p(test_df[log_transform_columns])\n",
    "\n",
    "input_scaler.fit(train_df[numerical_columns])\n",
    "output_scaler.fit(train_df[y_headers])\n",
    "\n",
    "# Prepare data by scaling numerical columns and encoding categorical columns\n",
    "train_df[numerical_columns] = input_scaler.transform(train_df[numerical_columns])\n",
    "train_df[y_headers] = output_scaler.transform(train_df[y_headers])\n",
    "val_df[numerical_columns] = input_scaler.transform(val_df[numerical_columns])\n",
    "val_df[y_headers] = output_scaler.transform(val_df[y_headers])\n",
    "test_df[numerical_columns] = input_scaler.transform(test_df[numerical_columns])\n",
    "test_df[y_headers] = output_scaler.transform(test_df[y_headers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adcda555-6884-401c-a59c-7b1df5ef7ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7736"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "006d0731-0fb7-450f-8725-f7b0c506ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df, train_df, val_df, test_df are defined and contain the necessary columns\n",
    "unique_combinations = df[['BasinTC', 'FORMATION_CONDENSED']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2db689d5-a441-4ba3-ba3a-47c41fda4d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BasinTC</th>\n",
       "      <th>FORMATION_CONDENSED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midland</td>\n",
       "      <td>LSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Midland</td>\n",
       "      <td>WCB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Midland</td>\n",
       "      <td>WCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Midland</td>\n",
       "      <td>JMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BasinTC FORMATION_CONDENSED\n",
       "0   Midland                 LSS\n",
       "4   Midland                 WCB\n",
       "5   Midland                 WCA\n",
       "26  Midland                 JMS"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9228e8e0-d16b-4100-8a1c-b727ddf4fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimePlottingCallback(Callback):\n",
    "    def __init__(self, combo_description):\n",
    "        super().__init__()\n",
    "        self.combo_description = combo_description\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Ensure we are appending the correct epoch number\n",
    "        self.epochs.append(len(self.epochs) + 1)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "        # Dynamic Plotting\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.epochs, self.losses, label='Training Loss', color='blue')\n",
    "        plt.plot(self.epochs, self.val_losses, label='Validation Loss', color='red')\n",
    "        plt.title(f'Training and Validation Loss for {self.combo_description}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xticks(range(1, max(self.epochs) + 1, max(1, len(self.epochs) // 10)))\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.pause(0.001)  # Pause to allow the plot to update\n",
    "        plt.close()  # Close the plot to prevent overlapping\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Final static plot after training\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.epochs, self.losses, label='Training Loss', color='blue')\n",
    "        plt.plot(self.epochs, self.val_losses, label='Validation Loss', color='red')\n",
    "        plt.title(f'Final Training and Validation Loss for {self.combo_description}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xticks(range(1, max(self.epochs) + 1, max(1, len(self.epochs) // 10)))\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efe11fa5-4445-4e69-8740-bc8437e8239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(history, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ml_performance(y_true, y_pred, title):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{title} - MSE: {mse}, MAE: {mae}, R²: {r2}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73f69b77-2d16-4384-b9e1-81f0652e47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MAPE as a custom loss function\n",
    "def mape_loss(y_true, y_pred):\n",
    "    epsilon = K.epsilon()  # Small constant to prevent division by zero\n",
    "    y_true = K.clip(y_true, epsilon, None)  # Avoid division by zero\n",
    "    return K.mean(K.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ec804d1-3c05-4ab3-b41d-0c4a5c4db1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(numerical_columns, categorical_columns, df, output_size, model_type='neural_network'):\n",
    "    if model_type == 'neural_network':\n",
    "        embedding_output_dim = 10  \n",
    "        dense_layer_sizes = [64,32]  # Further simplified model\n",
    "        dropout_rate = 0.2  # Increased dropout\n",
    "        regularization = l2(0.01)  # Reintroduced L2 regularization\n",
    "        activation = 'relu'\n",
    "        optimizer_choice = 'adam'\n",
    "        loss_function = 'mse'  # Switched to MAE\n",
    "\n",
    "        numerical_input = Input(shape=(len(numerical_columns),), name='num_input')\n",
    "        \n",
    "        if categorical_columns:\n",
    "            categorical_inputs = [Input(shape=(1,), name=f'cat_input_{i}') for i, _ in enumerate(categorical_columns)]\n",
    "            embeddings = [Embedding(input_dim=df[col].nunique() + 1, output_dim=embedding_output_dim, name=f'emb_{col}')(cat_input)\n",
    "                          for cat_input, col in zip(categorical_inputs, categorical_columns)]\n",
    "            flat_embeddings = [Flatten()(emb) for emb in embeddings]\n",
    "            merged = Concatenate()([numerical_input] + flat_embeddings)\n",
    "        else:\n",
    "            categorical_inputs = []\n",
    "            merged = numerical_input\n",
    "\n",
    "        x = merged\n",
    "        for size in dense_layer_sizes:\n",
    "            x = Dense(size, activation=activation, kernel_regularizer=regularization)(x)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "\n",
    "        output = Dense(output_size, activation='linear')(x)\n",
    "        model = Model(inputs=[numerical_input] + categorical_inputs, outputs=output)\n",
    "\n",
    "        opt = Adam(learning_rate=0.0001)  # Lowered learning rate\n",
    "\n",
    "        model.compile(optimizer=opt, loss=loss_function, metrics=[MeanSquaredError()])\n",
    "        #model.compile(optimizer=opt, loss=lambda y_true, y_pred: custom_production_loss(y_true, y_pred, time_array, output_scaler), metrics=[MeanSquaredError()])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    elif model_type in ['random_forest', 'decision_tree', 'xgboost', 'ridge', 'lasso', 'multioutput']:\n",
    "        if model_type == 'random_forest':\n",
    "            model = RandomForestRegressor(\n",
    "            n_jobs=-1,\n",
    "            n_estimators=100,  # Default value\n",
    "            max_depth=10,       # Default value\n",
    "            min_samples_split=2, # Default value\n",
    "            min_samples_leaf=1,  # Default value\n",
    "            bootstrap=True       # Default value\n",
    "        )\n",
    "        elif model_type == 'decision_tree':\n",
    "            model = DecisionTreeRegressor(\n",
    "            max_depth=20,         # Default value\n",
    "            min_samples_split=5,  # Default value\n",
    "            min_samples_leaf=2    # Default value\n",
    "        )\n",
    "        elif model_type == 'xgboost':\n",
    "            model = XGBRegressor(\n",
    "            tree_method='hist',\n",
    "            n_estimators=200,      # Default value\n",
    "            max_depth=7,           # Default value\n",
    "            learning_rate=0.01,    # Default value\n",
    "            subsample=0.8,         # Default value\n",
    "            colsample_bytree=0.8,  # Default value\n",
    "            reg_alpha=0.01,        # Default value\n",
    "            reg_lambda=1,          # Default value\n",
    "            gamma=0.1              # Default value\n",
    "        )\n",
    "        elif model_type == 'ridge':\n",
    "            model = MultiOutputRegressor(\n",
    "            Lasso(alpha=0.1, max_iter=10000)  # Default values\n",
    "        )\n",
    "        elif model_type == 'lasso':\n",
    "            model = MultiOutputRegressor(Lasso(alpha=kwargs.get('alpha', 0.1), max_iter=kwargs.get('max_iter', 10000)))\n",
    "        elif model_type == 'multioutput':\n",
    "            model = MultiOutputRegressor(\n",
    "            Lasso(alpha=0.1, max_iter=10000)  # Default values\n",
    "        )\n",
    "\n",
    "        numerical_transformer = Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=False))])\n",
    "        categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_columns), ('cat', categorical_transformer, categorical_columns)])\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "        \n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ddf935c-028e-4c0a-8a82-6fcac712a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositivePredictionCallback(Callback):\n",
    "#     def __init__(self, combo_train, combo_val, numerical_columns, categorical_columns, y_headers, output_scaler, time_array, initial_error_threshold=0.1, final_error_threshold=0.01, patience=10, apply_trends_after_epochs=100):\n",
    "#         super().__init__()\n",
    "#         self.combo_train = combo_train\n",
    "#         self.combo_val = combo_val\n",
    "#         self.numerical_columns = numerical_columns\n",
    "#         self.categorical_columns = categorical_columns\n",
    "#         self.y_headers = y_headers\n",
    "#         self.output_scaler = output_scaler\n",
    "#         self.patience = patience\n",
    "#         self.wait = 0\n",
    "#         self.best_weights = None\n",
    "#         self.best_loss = np.Inf\n",
    "#         self.initial_error_threshold = initial_error_threshold\n",
    "#         self.final_error_threshold = final_error_threshold\n",
    "#         self.epoch_count = 0\n",
    "#         self.initial_weights = None\n",
    "#         self.time = time_array\n",
    "#         self.apply_trends_after_epochs = apply_trends_after_epochs\n",
    "        \n",
    "#         # Define trends as a dictionary\n",
    "#         self.feature_trends = {\n",
    "#             'HORIZONTIAL_WELL_LENGTH': 'positive',\n",
    "#             'EST_FORMATION_THICKNESS': 'positive',\n",
    "#             'AVERAGE_DEPTH_BELOW_TOP': None,\n",
    "#             'NNAZ_1_TRUEDIST': 'positive',\n",
    "#             'NNAZ_1_HZDIST': 'positive',\n",
    "#             'NNAZ_1_VTDIST': 'positive',\n",
    "#             'NNSZ_1_TRUEDIST': 'positive',\n",
    "#             'NNSZ_1_HZDIST': 'positive',\n",
    "#             'NNSZ_1_VTDIST': 'positive',\n",
    "#             'AVERAGE_DEPTH_TO_NEXT_ZONE': None,\n",
    "#             'TotalDepthTVD': None,\n",
    "#             'ProppantPerFoot': 'positive',\n",
    "#             'FluidPerFoot_bblft': 'positive',\n",
    "#             'NNAZ_1_Cumulative oil mbo': 'negative',\n",
    "#             'NNAZ_1_Cumulative gas mmcf': 'negative',\n",
    "#             'NNAZ_1_Cumulative water mbbl': 'negative',\n",
    "#             'NNSZ_1_Cumulative oil mbo': 'negative',\n",
    "#             'NNSZ_1_Cumulative gas mmcf': 'negative',\n",
    "#             'NNSZ_1_Cumulative water mbbl': 'negative',\n",
    "#             'NNAZ_1_EUR_Combined_P50_MBO': None,\n",
    "#             'NNSZ_1_EUR_Combined_P50_MBO': None\n",
    "#         }\n",
    "\n",
    "#     def apply_trends(self, predictions_denorm, combo_data):\n",
    "#         print(\"Before applying trends:\", predictions_denorm)\n",
    "#         for feature in self.numerical_columns:\n",
    "#             trend = self.feature_trends.get(feature, None)  # Default to None if not mentioned in trends\n",
    "#             if trend is not None:\n",
    "#                 feature_values = combo_data[feature].values\n",
    "#                 for idx in range(predictions_denorm.shape[0]):\n",
    "#                     if trend == 'positive' and feature_values[idx] < 0:\n",
    "#                         predictions_denorm[idx] = np.maximum(0, predictions_denorm[idx])  # Element-wise max\n",
    "#                     elif trend == 'negative' and feature_values[idx] > 0:\n",
    "#                         predictions_denorm[idx] = np.minimum(0, predictions_denorm[idx])  # Element-wise min\n",
    "#         print(\"After applying trends:\", predictions_denorm)\n",
    "#         return predictions_denorm\n",
    "\n",
    "#     def validate_productions(self, predictions_denorm):\n",
    "#         def check_validity(qi, di, b, IBU, MBU):\n",
    "#             if tf.reduce_any(tf.less_equal([qi, di, b, IBU, MBU], 0)):\n",
    "#                 return False\n",
    "#             production = modified_hyperbolic(self.time, qi, di, b, 7.0, IBU, MBU)\n",
    "#             return not (tf.reduce_any(tf.math.is_nan(production)) or tf.reduce_any(tf.less(production, 0)))\n",
    "\n",
    "#         def check_validity(qi, di, b, IBU, MBU):\n",
    "#             valid_conditions = tf.reduce_all(tf.greater([qi, di, b, IBU, MBU], 0))\n",
    "#             production = tf.cond(valid_conditions, \n",
    "#                                  lambda: modified_hyperbolic(self.time, qi, di, b, 7.0, IBU, MBU),\n",
    "#                                  lambda: tf.constant(float('nan'), shape=self.time.shape))  # return NaNs if invalid\n",
    "#             return tf.logical_and(valid_conditions, tf.reduce_all(tf.greater(production, 0)) & tf.reduce_all(tf.math.is_finite(production)))\n",
    "\n",
    "#         validity_checks = tf.map_fn(\n",
    "#             lambda idx: check_validity(\n",
    "#                 predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_InitialProd')],\n",
    "#                 predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_DiCoefficient')],\n",
    "#                 predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BCoefficient')],\n",
    "#                 predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BuildupRate')],\n",
    "#                 predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "#             ),\n",
    "#             tf.range(tf.shape(predictions_denorm)[0]),\n",
    "#             dtype=tf.bool\n",
    "#         )\n",
    "#         return tf.reduce_all(validity_checks)\n",
    "\n",
    "#     def on_train_begin(self, logs=None):\n",
    "#         self.initial_weights = self.model.get_weights()\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self.epoch_count += 1\n",
    "#         if epoch % 10 == 0:  # Check every 10 epochs\n",
    "#             predictions = self.model.predict(\n",
    "#                 x=[self.combo_train[self.numerical_columns].values] + [self.combo_train[col].astype(int).values.reshape(-1, 1) for col in self.categorical_columns]\n",
    "#             )\n",
    "#             predictions_denorm = self.output_scaler.inverse_transform(predictions)\n",
    "            \n",
    "#             # Apply trends only after warmup_epochs\n",
    "#             if self.epoch_count > self.apply_trends_after_epochs:\n",
    "#                 print(f\"Epoch {epoch+1}: Applying trends...\")\n",
    "#                 predictions_denorm = self.apply_trends(predictions_denorm, self.combo_train)\n",
    "\n",
    "#             # Validate predictions\n",
    "#             if not self.validate_productions(predictions_denorm):\n",
    "#                 print(f\"Epoch {epoch+1}: Predictions invalid, checking patience.\")\n",
    "#                 self.wait += 1\n",
    "#                 if self.wait >= self.patience:\n",
    "#                     if self.best_weights is not None:\n",
    "#                         print(f\"Epoch {epoch+1}: Invalid predictions detected. Restoring best weights.\")\n",
    "#                         self.model.set_weights(self.best_weights)\n",
    "#                     else:\n",
    "#                         print(f\"Epoch {epoch+1}: No valid weights to restore. Restoring initial weights.\")\n",
    "#                         self.model.set_weights(self.initial_weights)\n",
    "#                     self.wait = 0\n",
    "#             else:\n",
    "#                 self.wait = 0\n",
    "#                 if logs['val_loss'] < self.best_loss:\n",
    "#                     self.best_loss = logs['val_loss']\n",
    "#                     self.best_weights = self.model.get_weights()\n",
    "#                     print(f\"Epoch {epoch+1}: New best weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95a35419-dcc3-402c-9081-9dbe8b7f6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositivePredictionCallback(Callback):\n",
    "    def __init__(self, combo_train, combo_val, numerical_columns, categorical_columns, y_headers, output_scaler, time_array, initial_error_threshold=0.1, final_error_threshold=0.01, patience=10, apply_trends_after_epochs=100):\n",
    "        super().__init__()\n",
    "        self.combo_train = combo_train\n",
    "        self.combo_val = combo_val\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.y_headers = y_headers\n",
    "        self.output_scaler = output_scaler\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best_loss = np.Inf\n",
    "        self.initial_error_threshold = initial_error_threshold\n",
    "        self.final_error_threshold = final_error_threshold\n",
    "        self.epoch_count = 0\n",
    "        self.initial_weights = None\n",
    "        self.time = time_array\n",
    "        self.apply_trends_after_epochs = apply_trends_after_epochs\n",
    "\n",
    "        # Define trends as a dictionary\n",
    "        self.feature_trends = {\n",
    "            'HORIZONTIAL_WELL_LENGTH': 'positive',\n",
    "            'EST_FORMATION_THICKNESS': 'positive',\n",
    "            'AVERAGE_DEPTH_BELOW_TOP': None,\n",
    "            'NNAZ_1_TRUEDIST': 'positive',\n",
    "            'NNAZ_1_HZDIST': 'positive',\n",
    "            'NNAZ_1_VTDIST': 'positive',\n",
    "            'NNSZ_1_TRUEDIST': 'positive',\n",
    "            'NNSZ_1_HZDIST': 'positive',\n",
    "            'NNSZ_1_VTDIST': 'positive',\n",
    "            'AVERAGE_DEPTH_TO_NEXT_ZONE': None,\n",
    "            'TotalDepthTVD': None,\n",
    "            'ProppantPerFoot': 'positive',\n",
    "            'FluidPerFoot_bblft': 'positive',\n",
    "            'NNAZ_1_Cumulative oil mbo': 'negative',\n",
    "            'NNAZ_1_Cumulative gas mmcf': 'negative',\n",
    "            'NNAZ_1_Cumulative water mbbl': 'negative',\n",
    "            'NNSZ_1_Cumulative oil mbo': 'negative',\n",
    "            'NNSZ_1_Cumulative gas mmcf': 'negative',\n",
    "            'NNSZ_1_Cumulative water mbbl': 'negative',\n",
    "            'NNAZ_1_EUR_Combined_P50_MBO': None,\n",
    "            'NNSZ_1_EUR_Combined_P50_MBO': None\n",
    "        }\n",
    "\n",
    "    def apply_trends(self, predictions_denorm, combo_data):\n",
    "        #print(\"Before applying trends:\", predictions_denorm)\n",
    "        print(\"Before applying trends:\")\n",
    "\n",
    "        for idx in range(predictions_denorm.shape[0]):\n",
    "            # Extract the predicted parameters for the current row\n",
    "            qi = predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "            di = predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "            b = predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "            IBU = predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "            MBU = predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "\n",
    "            # Calculate the initial production\n",
    "            initial_production = modified_hyperbolic(self.time, qi, di, b, 7.0, IBU, MBU)[1]\n",
    "\n",
    "            # Apply trends by checking if the production follows the expected trend based on feature values\n",
    "            for feature in self.numerical_columns:\n",
    "                trend = self.feature_trends.get(feature, None)\n",
    "                if trend is not None:\n",
    "                    feature_value = combo_data[feature].values[idx]\n",
    "\n",
    "                    if trend == 'positive' and feature_value > 0:\n",
    "                        hypothetical_qi = qi * 1.1  # Slight increase in qi for positive trend\n",
    "                        hypothetical_production = modified_hyperbolic(self.time, hypothetical_qi, di, b, 7.0, IBU, MBU)[1]\n",
    "                        if np.sum(hypothetical_production) > np.sum(initial_production):\n",
    "                            #print(f\"Feature {feature} at row {idx} should increase production but does not. Adjusting...\")\n",
    "                            predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_InitialProd')] *= 1.1\n",
    "\n",
    "                    elif trend == 'negative' and feature_value < 0:\n",
    "                        hypothetical_qi = qi * 0.9  # Slight decrease in qi for negative trend\n",
    "                        hypothetical_production = modified_hyperbolic(self.time, hypothetical_qi, di, b, 7.0, IBU, MBU)[1]\n",
    "                        if np.sum(hypothetical_production) < np.sum(initial_production):\n",
    "                            #print(f\"Feature {feature} at row {idx} should decrease production but does not. Adjusting...\")\n",
    "                            predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_InitialProd')] *= 0.9\n",
    "\n",
    "        #print(\"After applying trends:\", predictions_denorm)\n",
    "        print(\"After applying trends:\")\n",
    "        return predictions_denorm\n",
    "\n",
    "    def validate_productions(self, predictions_denorm):\n",
    "        def check_validity(qi, di, b, IBU, MBU):\n",
    "            if tf.reduce_any(tf.less_equal([qi, di, b], 0)) or tf.reduce_any(tf.less([IBU, MBU], 0)):\n",
    "                return False\n",
    "            production = modified_hyperbolic(self.time, qi, di, b, 7.0, IBU, MBU)[1]\n",
    "            return not (tf.reduce_any(tf.math.is_nan(production)) or tf.reduce_any(tf.less(production, 0)))\n",
    "\n",
    "        validity_checks = tf.map_fn(\n",
    "            lambda idx: check_validity(\n",
    "                predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_InitialProd')],\n",
    "                predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_DiCoefficient')],\n",
    "                predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BCoefficient')],\n",
    "                predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_BuildupRate')],\n",
    "                predictions_denorm[idx][self.y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "            ),\n",
    "            tf.range(tf.shape(predictions_denorm)[0]),\n",
    "            dtype=tf.bool\n",
    "        )\n",
    "        return tf.reduce_all(validity_checks)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.initial_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_count += 1\n",
    "        if epoch % 10 == 0:  # Check every 10 epochs\n",
    "            predictions = self.model.predict(\n",
    "                x=[self.combo_train[self.numerical_columns].values] + [self.combo_train[col].astype(int).values.reshape(-1, 1) for col in self.categorical_columns]\n",
    "            )\n",
    "            predictions_denorm = self.output_scaler.inverse_transform(predictions)\n",
    "\n",
    "            # Apply trends only after warmup_epochs\n",
    "            if self.epoch_count > self.apply_trends_after_epochs:\n",
    "                print(f\"Epoch {epoch+1}: Applying trends...\")\n",
    "                predictions_denorm = self.apply_trends(predictions_denorm, self.combo_train)\n",
    "\n",
    "            # Validate predictions\n",
    "            if not self.validate_productions(predictions_denorm):\n",
    "                print(f\"Epoch {epoch+1}: Predictions invalid, checking patience.\")\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    if self.best_weights is not None:\n",
    "                        #print(f\"Epoch {epoch+1}: Invalid predictions detected. Restoring best weights.\")\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "                    else:\n",
    "                        #print(f\"Epoch {epoch+1}: No valid weights to restore. Restoring initial weights.\")\n",
    "                        self.model.set_weights(self.initial_weights)\n",
    "                    self.wait = 0\n",
    "            else:\n",
    "                self.wait = 0\n",
    "                if logs['val_loss'] < self.best_loss:\n",
    "                    self.best_loss = logs['val_loss']\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    print(f\"Epoch {epoch+1}: New best weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1371fd57-bcc7-4d92-b256-2c6e75c24846",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_production_loss(y_true, y_pred, time_array, output_scaler, y_headers):\n",
    "    scale_ = tf.convert_to_tensor(output_scaler.scale_, dtype=tf.float32)\n",
    "    min_ = tf.convert_to_tensor(output_scaler.min_, dtype=tf.float32)\n",
    "\n",
    "    # Denormalize the predictions\n",
    "    y_pred_denorm = y_pred * scale_ + min_\n",
    "\n",
    "    def compute_batch_loss(y_true_row, y_pred_row):\n",
    "        # Extract predicted parameters for each phase\n",
    "        qi_oil = y_pred_row[y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "        di_oil = y_pred_row[y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "        b_oil = y_pred_row[y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "        IBU_oil = y_pred_row[y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "        MBU_oil = y_pred_row[y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "\n",
    "        qi_gas = y_pred_row[y_headers.index('Gas_Params_P50_InitialProd')]\n",
    "        di_gas = y_pred_row[y_headers.index('Gas_Params_P50_DiCoefficient')]\n",
    "        b_gas = y_pred_row[y_headers.index('Gas_Params_P50_BCoefficient')]\n",
    "        IBU_gas = y_pred_row[y_headers.index('Gas_Params_P50_BuildupRate')]\n",
    "        MBU_gas = y_pred_row[y_headers.index('Gas_Params_P50_MonthsInProd')]\n",
    "\n",
    "        qi_water = y_pred_row[y_headers.index('Water_Params_P50_InitialProd')]\n",
    "        di_water = y_pred_row[y_headers.index('Water_Params_P50_DiCoefficient')]\n",
    "        b_water = y_pred_row[y_headers.index('Water_Params_P50_BCoefficient')]\n",
    "        IBU_water = y_pred_row[y_headers.index('Water_Params_P50_BuildupRate')]\n",
    "        MBU_water = y_pred_row[y_headers.index('Water_Params_P50_MonthsInProd')]\n",
    "\n",
    "        Dlim = tf.constant(7.0, dtype=tf.float32)\n",
    "\n",
    "        # Generate predicted production for each phase using modified hyperbolic\n",
    "        pred_oil_production = modified_hyperbolic_tf(time_array, qi_oil, di_oil, b_oil, Dlim, IBU_oil, MBU_oil)\n",
    "        pred_gas_production = modified_hyperbolic_tf(time_array, qi_gas, di_gas, b_gas, Dlim, IBU_gas, MBU_gas)\n",
    "        pred_water_production = modified_hyperbolic_tf(time_array, qi_water, di_water, b_water, Dlim, IBU_water, MBU_water)\n",
    "\n",
    "        # Concatenate the productions for comparison\n",
    "        pred_production = tf.concat([pred_oil_production, pred_gas_production, pred_water_production], axis=0)\n",
    "\n",
    "        # Debug the shapes before computing the loss\n",
    "        tf.print(\"Shapes of y_true and pred_production:\", y_true_row.shape, pred_production.shape)\n",
    "\n",
    "        # Check for compatibility of shapes before subtracting\n",
    "        if y_true_row.shape != pred_production.shape:\n",
    "            tf.print(\"Shape mismatch detected! Returning high loss to force re-evaluation.\")\n",
    "            return tf.constant(1e6, dtype=tf.float32)  # Return a high loss if shapes are incompatible\n",
    "\n",
    "        # Calculate the loss as the MSE between true and predicted production\n",
    "        batch_loss = tf.reduce_mean(tf.square(y_true_row - pred_production))\n",
    "        return batch_loss\n",
    "\n",
    "    # Apply the computation across the batch\n",
    "    total_loss = tf.map_fn(lambda x: compute_batch_loss(x[0], x[1]), (y_true, y_pred_denorm), fn_output_signature=tf.float32)\n",
    "\n",
    "    # Return the mean loss across the batch\n",
    "    return tf.reduce_mean(total_loss)\n",
    "\n",
    "@tf.function\n",
    "def validate_inputs_tf(qi, di, b, Dlim, IBU, MBU):\n",
    "    # A TensorFlow validation function where IBU and MBU can be zero\n",
    "    valid_ibu_mbu = tf.reduce_all(tf.greater_equal([IBU, MBU], 0))\n",
    "    valid_params = tf.reduce_all(tf.greater([qi, di, b, Dlim], 0))\n",
    "    return tf.logical_and(valid_ibu_mbu, valid_params)\n",
    "\n",
    "@tf.function\n",
    "def modified_hyperbolic_tf(time_array, qi, di, b, Dlim, IBU, MBU, buildup_method='Linear', epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate the monthly production volume using the Modified Hyperbolic Decline Model with a buildup phase.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not validate_inputs_tf(qi, di, b, Dlim, IBU, MBU):\n",
    "        # Return zeros if inputs are invalid\n",
    "        return tf.zeros_like(time_array, dtype=tf.float32)\n",
    "\n",
    "    # Convert decline rates to nominal rates\n",
    "    try:\n",
    "        di_nominal = ((1 - 0.01 * di) ** (-b) - 1) / (12 * b)\n",
    "        Dlim_nominal = ((1 - 0.01 * Dlim) ** (-b) - 1) / (12 * b)\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # Return zeros if there's an error in calculation\n",
    "        return tf.zeros_like(time_array, dtype=tf.float32)\n",
    "\n",
    "    # Calculate switch point\n",
    "    try:\n",
    "        qlim = qi * (Dlim_nominal / (di_nominal + epsilon)) ** (1.0 / b)\n",
    "        tlim = ((qi / qlim) ** b - 1.0) / (b * (di_nominal + epsilon))\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # Return zeros if there's an error in calculation\n",
    "        return tf.zeros_like(time_array, dtype=tf.float32)\n",
    "\n",
    "    # Handle the case when MBU (buildup period) is zero\n",
    "    if MBU == 0:\n",
    "        # If MBU is zero, skip the buildup phase and start with the hyperbolic decline directly\n",
    "        t_buildup = tf.constant([], dtype=tf.float32)\n",
    "        buildup_production = tf.constant([], dtype=tf.float32)\n",
    "        t_post_buildup = time_array\n",
    "        t_hyp = t_post_buildup[t_post_buildup < tlim]\n",
    "        t_exp = t_post_buildup[t_post_buildup >= tlim]\n",
    "    else:\n",
    "        # Separate time into buildup, hyperbolic, and exponential segments\n",
    "        t_buildup = time_array[time_array <= MBU]\n",
    "        t_post_buildup = time_array[time_array > MBU]\n",
    "        t_hyp = t_post_buildup[t_post_buildup < tlim]\n",
    "        t_exp = t_post_buildup[t_post_buildup >= tlim]\n",
    "\n",
    "        # Calculate production during buildup period\n",
    "        if buildup_method == 'Flat':\n",
    "            buildup_production = IBU + tf.zeros_like(t_buildup, dtype=tf.float32)\n",
    "        elif buildup_method == 'Linear':\n",
    "            slope = (qi - IBU) / (MBU + epsilon)\n",
    "            buildup_production = IBU + slope * t_buildup\n",
    "        elif buildup_method == 'Exp':\n",
    "            slope = tf.math.log(qi / (IBU + epsilon)) / (MBU + epsilon)\n",
    "            buildup_production = IBU * tf.exp(slope * t_buildup)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown buildup method: {buildup_method}\")\n",
    "\n",
    "    # Calculate production during hyperbolic decline period\n",
    "    try:\n",
    "        q_model_hyp = qi * (1.0 + b * di_nominal * (t_hyp - MBU)) ** (-1.0 / b)\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # Return zeros if there's an error in calculation\n",
    "        q_model_hyp = tf.zeros_like(t_hyp, dtype=tf.float32)\n",
    "\n",
    "    # Calculate production during exponential decline period\n",
    "    try:\n",
    "        q_model_exp = qlim * tf.exp(-Dlim_nominal * (t_exp - tlim))\n",
    "    except (ZeroDivisionError, OverflowError, ValueError) as e:\n",
    "        # Return zeros if there's an error in calculation\n",
    "        q_model_exp = tf.zeros_like(t_exp, dtype=tf.float32)\n",
    "\n",
    "    # Smooth transition at tlim (Hyperbolic to Exponential)\n",
    "    if tf.size(t_exp) > 0 and tf.size(t_hyp) > 0:\n",
    "        time_fraction = (tlim - t_hyp[-1]) / (t_exp[0] - t_hyp[-1] + epsilon)\n",
    "        q_model_exp = tf.tensor_scatter_nd_update(q_model_exp, [[0]], [time_fraction * q_model_hyp[-1] + (1 - time_fraction) * q_model_exp[0]])\n",
    "\n",
    "    # Combine production rates\n",
    "    production_post_buildup = tf.concat([q_model_hyp, q_model_exp], axis=0)\n",
    "    daily_production = tf.concat([buildup_production, production_post_buildup], axis=0)\n",
    "\n",
    "    # Calculate monthly production volumes by summing daily rates over each month\n",
    "    days_per_month = 30\n",
    "    monthly_volumes = daily_production * days_per_month\n",
    "\n",
    "    return monthly_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eadd44db-74e7-478c-9bad-3b6b83de9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_xgboost_training(model, preprocessor, combo_train, combo_val, numerical_columns, categorical_columns, y_headers):\n",
    "    combo_train_val = pd.concat([combo_train, combo_val])\n",
    "    \n",
    "    preprocessor.fit(combo_train_val[numerical_columns + categorical_columns])\n",
    "    combo_train_transformed = preprocessor.transform(combo_train[numerical_columns + categorical_columns])\n",
    "    combo_val_transformed = preprocessor.transform(combo_val[numerical_columns + categorical_columns])\n",
    "\n",
    "    # Initial training\n",
    "    model.fit(combo_train_transformed, combo_train[y_headers].values, \n",
    "              eval_set=[(combo_val_transformed, combo_val[y_headers].values)], \n",
    "              early_stopping_rounds=10, \n",
    "              verbose=True)\n",
    "    \n",
    "    # Custom validation loop\n",
    "    for round in range(10, model.get_booster().best_iteration + 1, 10):\n",
    "        predictions = model.predict(combo_val_transformed, iteration_range=(0, round))\n",
    "        \n",
    "        # Apply custom validation (e.g., using modified_hyperbolic)\n",
    "        valid = validate_productionsxgb(predictions, combo_val, y_headers)\n",
    "        \n",
    "        if not valid:\n",
    "            print(f\"Invalid predictions detected at round {round}. Stopping training.\")\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validate_productionsxgb(predictions, combo_val, y_headers):\n",
    "    valid = True\n",
    "    for idx in range(predictions.shape[0]):\n",
    "        qi = predictions[idx][y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "        di = predictions[idx][y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "        b = predictions[idx][y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "        IBU = predictions[idx][y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "        MBU = predictions[idx][y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "        Dlim = 7  # Example fixed value or predicted value\n",
    "\n",
    "        # Ensure all predictions are positive\n",
    "        if any(x <= 0 for x in [qi, di, b, IBU, MBU, Dlim]):\n",
    "            valid = False\n",
    "            break\n",
    "\n",
    "        # Generate production using modified_hyperbolic\n",
    "        production = modified_hyperbolic(np.arange(1, 121), qi, di, b, Dlim, IBU, MBU)[1]\n",
    "\n",
    "        # Check if production contains NaN or negative values\n",
    "        if np.isnan(production).any() or (production < 0).any():\n",
    "            valid = False\n",
    "            break\n",
    "\n",
    "    return valid\n",
    "    \n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(model, X, y, cv=5):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    val_scores_mean = -val_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training error\")\n",
    "    plt.plot(train_sizes, val_scores_mean, label=\"Validation error\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def plot_residuals(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    residuals = y - predictions\n",
    "    plt.figure()\n",
    "    plt.scatter(predictions, residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a88ab21-3fab-445d-9c2a-e13bd3948c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_model(combo_train, combo_val, combo_test, numerical_columns, categorical_columns, y_headers, output_size, model_type, df, task_times):\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Starting {model_type} training\")\n",
    "\n",
    "#     if model_type == 'neural_network':\n",
    "#         model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#         real_time_plotter = RealTimePlottingCallback(combo_description=model_type)\n",
    "        \n",
    "#         # Instantiate PositivePredictionCallback without passing max_allowed_error\n",
    "#         positive_pred_callback = PositivePredictionCallback(\n",
    "#                                 combo_train=combo_train,\n",
    "#                                 combo_val=combo_val,\n",
    "#                                 numerical_columns=numerical_columns,\n",
    "#                                 categorical_columns=categorical_columns,\n",
    "#                                 y_headers=y_headers,\n",
    "#                                 output_scaler=output_scaler,\n",
    "#                                 time=np.arange(1, 121)  # Assuming you are predicting over 10 years on a monthly basis\n",
    "#                                 )\n",
    "        \n",
    "#         history = model.fit(\n",
    "#             x=[combo_train[numerical_columns].values] + [combo_train[col].astype(int).values.reshape(-1, 1) for col in categorical_columns], \n",
    "#             y=combo_train[y_headers].values, \n",
    "#             validation_data=(\n",
    "#                 [combo_val[numerical_columns].values] + [combo_val[col].astype(int).values.reshape(-1, 1) for col in categorical_columns],\n",
    "#                 combo_val[y_headers].values\n",
    "#             ),\n",
    "#             epochs=1000, \n",
    "#             batch_size=10, \n",
    "#             callbacks=[real_time_plotter,\n",
    "#                        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "#                        #ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6),\n",
    "#                        positive_pred_callback\n",
    "#                       ],            \n",
    "#             verbose=1\n",
    "#         )\n",
    "#     else:\n",
    "#         combo_train_val = pd.concat([combo_train, combo_val])\n",
    "#         model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#         if model_type == 'xgboost':\n",
    "#             preprocessor = model.named_steps['preprocessor']\n",
    "#             xgb_model = model.named_steps['model']\n",
    "#             preprocessor.fit(combo_train_val[numerical_columns + categorical_columns])\n",
    "#             combo_train_val_transformed = preprocessor.transform(combo_train_val[numerical_columns + categorical_columns])\n",
    "#             combo_val_transformed = preprocessor.transform(combo_val[numerical_columns + categorical_columns])\n",
    "#             xgb_model.fit(combo_train_val_transformed, combo_train_val[y_headers].values, eval_set=[(combo_val_transformed, combo_val[y_headers].values)], early_stopping_rounds=10, verbose=False)\n",
    "#         else:\n",
    "#             model.fit(combo_train_val[numerical_columns + categorical_columns], combo_train_val[y_headers].values)\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     duration = end_time - start_time\n",
    "#     task_key = f\"{model_type}\"\n",
    "#     task_times[task_key] = duration\n",
    "#     print(f\"Training completed for {task_key} in {duration:.2f} seconds\")\n",
    "#     return model\n",
    "# Define your training function and ensure compatibility\n",
    "# def train_and_evaluate_model(combo_train, combo_val, combo_test, numerical_columns, categorical_columns, y_headers, output_size, model_type, df, task_times):\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Starting {model_type} training\")\n",
    "\n",
    "#     if model_type == 'neural_network':\n",
    "#         model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#         real_time_plotter = RealTimePlottingCallback(combo_description=model_type)\n",
    "        \n",
    "#         positive_pred_callback = PositivePredictionCallback(\n",
    "#             combo_train=combo_train,\n",
    "#             combo_val=combo_val,\n",
    "#             numerical_columns=numerical_columns,\n",
    "#             categorical_columns=categorical_columns,\n",
    "#             y_headers=y_headers,\n",
    "#             output_scaler=output_scaler,\n",
    "#             time_array=np.arange(1, 121)  # Assuming 10 years of monthly data\n",
    "#         )\n",
    "\n",
    "#         opt = Adam(learning_rate=0.00005, clipvalue=1.0)\n",
    "\n",
    "#         model.compile(optimizer=opt, loss=lambda y_true, y_pred: custom_production_loss(y_true, y_pred, time_array, output_scaler, y_headers), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        \n",
    "#         checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "        \n",
    "\n",
    "#         history = model.fit(\n",
    "#             x=[combo_train[numerical_columns].values] + [combo_train[col].astype(int).values.reshape(-1, 1) for col in categorical_columns], \n",
    "#             y=combo_train[y_headers].values, \n",
    "#             validation_data=(\n",
    "#                 [combo_val[numerical_columns].values] + [combo_val[col].astype(int).values.reshape(-1, 1) for col in categorical_columns],\n",
    "#                 combo_val[y_headers].values\n",
    "#             ),\n",
    "#             epochs=1000, \n",
    "#             batch_size=16,  # Increased batch size\n",
    "#             callbacks=[\n",
    "#                 real_time_plotter,\n",
    "#                 EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "#                 ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n",
    "#                 checkpoint,\n",
    "#                 positive_pred_callback\n",
    "#             ],\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         combo_train_val = pd.concat([combo_train, combo_val])\n",
    "#         model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#         if model_type == 'xgboost':\n",
    "#             preprocessor = model.named_steps['preprocessor']\n",
    "#             xgb_model = model.named_steps['model']\n",
    "#             trained_model = custom_xgboost_training(\n",
    "#                 xgb_model, \n",
    "#                 preprocessor, \n",
    "#                 combo_train, \n",
    "#                 combo_val, \n",
    "#                 numerical_columns, \n",
    "#                 categorical_columns, \n",
    "#                 y_headers\n",
    "#             )\n",
    "#         else:\n",
    "#             model.fit(combo_train_val[numerical_columns + categorical_columns], combo_train_val[y_headers].values)\n",
    "\n",
    "#             predictions = model.predict(combo_val[numerical_columns + categorical_columns])\n",
    "#             valid = validate_productions(predictions, combo_val, y_headers)\n",
    "#             if not valid:\n",
    "#                 print(\"Invalid predictions detected, adjusting model and retraining.\")\n",
    "#                 model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#                 model.fit(combo_train_val[numerical_columns + categorical_columns], combo_train_val[y_headers].values)\n",
    "        \n",
    "#         trained_model = model\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     duration = end_time - start_time\n",
    "#     task_key = f\"{model_type}\"\n",
    "#     task_times[task_key] = duration\n",
    "#     print(f\"Training completed for {task_key} in {duration:.2f} seconds\")\n",
    "#     return trained_model if model_type == 'xgboost' else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "046b6a11-4119-4817-9f49-6b381aea9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_model(combo_train, combo_val, combo_test, numerical_columns, categorical_columns, y_headers, output_size, model_type, df, task_times):\n",
    "#     start_time = time.time()\n",
    "#     print(f\"Starting {model_type} training\")\n",
    "\n",
    "#     total_epochs = 500\n",
    "#     warmup_epochs = 20\n",
    "\n",
    "#     if model_type == 'neural_network':\n",
    "#         model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "#         real_time_plotter = RealTimePlottingCallback(combo_description=model_type)\n",
    "\n",
    "#         positive_pred_callback = PositivePredictionCallback(\n",
    "#             combo_train=combo_train,\n",
    "#             combo_val=combo_val,\n",
    "#             numerical_columns=numerical_columns,\n",
    "#             categorical_columns=categorical_columns,\n",
    "#             y_headers=y_headers,\n",
    "#             output_scaler=output_scaler,\n",
    "#             time_array=np.arange(1, 121),  # Assuming 10 years of monthly data\n",
    "#             apply_trends_after_epochs=warmup_epochs  # New parameter to control trend application\n",
    "#         )\n",
    "\n",
    "#         opt = Adam(learning_rate=0.00005, clipvalue=1.0)\n",
    "\n",
    "#         checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "#         mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "#         def combined_loss(y_true, y_pred, epoch, warmup_epochs, mse_loss_fn):\n",
    "#             # Implement the combined loss logic if needed, or just return MSE loss\n",
    "#             return mse_loss_fn(y_true, y_pred)\n",
    "\n",
    "#         def named_combined_loss(y_true, y_pred):\n",
    "#             return combined_loss(y_true, y_pred, epoch, warmup_epochs, mse_loss_fn)\n",
    "\n",
    "#         # Lists to store loss values for plotting later\n",
    "#         train_losses = []\n",
    "#         val_losses = []\n",
    "\n",
    "#         for epoch in range(total_epochs):\n",
    "#             print(f\"Epoch {epoch + 1}/{total_epochs}\")\n",
    "#             model.compile(optimizer=opt, loss=named_combined_loss, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "#             history = model.fit(\n",
    "#                 x=[combo_train[numerical_columns].values] + [combo_train[col].astype(int).values.reshape(-1, 1) for col in categorical_columns], \n",
    "#                 y=combo_train[y_headers].values, \n",
    "#                 validation_data=(\n",
    "#                     [combo_val[numerical_columns].values] + [combo_val[col].astype(int).values.reshape(-1, 1) for col in categorical_columns],\n",
    "#                     combo_val[y_headers].values\n",
    "#                 ),\n",
    "#                 epochs=1,\n",
    "#                 batch_size=16,\n",
    "#                 callbacks=[\n",
    "#                     #real_time_plotter,\n",
    "#                     EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "#                     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "#                     checkpoint,\n",
    "#                     positive_pred_callback  # Modified callback\n",
    "#                 ],\n",
    "#                 verbose=0\n",
    "#             )\n",
    "#             # Append losses to lists\n",
    "#             train_losses.append(history.history['loss'][0])\n",
    "#             val_losses.append(history.history['val_loss'][0])\n",
    "\n",
    "#         # Plot the losses vs. epochs at the end of training\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(range(1, total_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "#         plt.plot(range(1, total_epochs + 1), val_losses, label='Validation Loss', color='red')\n",
    "#         plt.title(f'Training and Validation Loss vs Epochs for {model_type}')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "def train_and_evaluate_model(combo_train, combo_val, combo_test, numerical_columns, categorical_columns, y_headers, output_size, model_type, df, task_times):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting {model_type} training\")\n",
    "\n",
    "    total_epochs = 200\n",
    "    warmup_epochs = 20\n",
    "    use_custom_loss = False  # Set this to True to use the custom loss function after the warmup period\n",
    "\n",
    "    if model_type == 'neural_network':\n",
    "        model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "        real_time_plotter = RealTimePlottingCallback(combo_description=model_type)\n",
    "\n",
    "        positive_pred_callback = PositivePredictionCallback(\n",
    "            combo_train=combo_train,\n",
    "            combo_val=combo_val,\n",
    "            numerical_columns=numerical_columns,\n",
    "            categorical_columns=categorical_columns,\n",
    "            y_headers=y_headers,\n",
    "            output_scaler=output_scaler,\n",
    "            time_array=np.arange(1, 121),  # Assuming 10 years of monthly data\n",
    "            apply_trends_after_epochs=warmup_epochs  # New parameter to control trend application\n",
    "        )\n",
    "\n",
    "        opt = Adam(learning_rate=0.001, clipvalue=1.0)\n",
    "\n",
    "        checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "        mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        def combined_loss(y_true, y_pred):\n",
    "            if epoch >= warmup_epochs and use_custom_loss:\n",
    "                return custom_production_loss(y_true, y_pred, tf.convert_to_tensor(np.arange(1, 121), dtype=tf.float32), output_scaler, y_headers)\n",
    "            else:\n",
    "                return mse_loss_fn(y_true, y_pred)\n",
    "\n",
    "        # Option to toggle custom loss after warmup\n",
    "        def named_combined_loss(y_true, y_pred):\n",
    "            return combined_loss(y_true, y_pred)\n",
    "\n",
    "        # Lists to store loss values for plotting later\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(total_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{total_epochs}\")\n",
    "            \n",
    "            # Compile the model for the current epoch\n",
    "            model.compile(optimizer=opt, loss=named_combined_loss, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "            history = model.fit(\n",
    "                x=[combo_train[numerical_columns].values] + [combo_train[col].astype(int).values.reshape(-1, 1) for col in categorical_columns], \n",
    "                y=combo_train[y_headers].values, \n",
    "                validation_data=(\n",
    "                    [combo_val[numerical_columns].values] + [combo_val[col].astype(int).values.reshape(-1, 1) for col in categorical_columns],\n",
    "                    combo_val[y_headers].values\n",
    "                ),\n",
    "                epochs=1,\n",
    "                batch_size=16,\n",
    "                callbacks=[\n",
    "                    # real_time_plotter,  # Comment/uncomment to use real-time plotting\n",
    "                    EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "                    checkpoint,\n",
    "                    positive_pred_callback  # Modified callback\n",
    "                ],\n",
    "                verbose=0\n",
    "            )\n",
    "            # Append losses to lists\n",
    "            train_losses.append(history.history['loss'][0])\n",
    "            val_losses.append(history.history['val_loss'][0])\n",
    "\n",
    "        # Plot the losses vs. epochs at the end of training\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, total_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "        plt.plot(range(1, total_epochs + 1), val_losses, label='Validation Loss', color='red')\n",
    "        plt.title(f'Training and Validation Loss vs Epochs for {model_type}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        combo_train_val = pd.concat([combo_train, combo_val])\n",
    "        model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "        if model_type == 'xgboost':\n",
    "            preprocessor = model.named_steps['preprocessor']\n",
    "            xgb_model = model.named_steps['model']\n",
    "            trained_model = custom_xgboost_training(\n",
    "                xgb_model, \n",
    "                preprocessor, \n",
    "                combo_train, \n",
    "                combo_val, \n",
    "                numerical_columns, \n",
    "                categorical_columns, \n",
    "                y_headers\n",
    "            )\n",
    "        else:\n",
    "            model.fit(combo_train_val[numerical_columns + categorical_columns], combo_train_val[y_headers].values)\n",
    "\n",
    "            predictions = model.predict(combo_val[numerical_columns + categorical_columns])\n",
    "            valid = validate_productions(predictions, combo_val, y_headers)\n",
    "            if not valid:\n",
    "                print(\"Invalid predictions detected, adjusting model and retraining.\")\n",
    "                model = build_model(numerical_columns, categorical_columns, df, output_size, model_type=model_type)\n",
    "                model.fit(combo_train_val[numerical_columns + categorical_columns], combo_train_val[y_headers].values)\n",
    "        \n",
    "        trained_model = model\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    task_key = f\"{model_type}\"\n",
    "    task_times[task_key] = duration\n",
    "    print(f\"Training completed for {task_key} in {duration:.2f} seconds\")\n",
    "    return trained_model if model_type == 'xgboost' else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8150dd47-702e-4f52-a849-bf9851848b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training(specific_combinations, train_df, val_df, test_df, numerical_columns, categorical_columns, y_headers, output_size, df, task_times):\n",
    "    all_models = {}\n",
    "    for combo in tqdm(specific_combinations.iterrows(), total=specific_combinations.shape[0]):\n",
    "        combo_data = combo[1]\n",
    "        basin = combo_data['BasinTC']\n",
    "        formation = combo_data['FORMATION_CONDENSED']\n",
    "        combo_train = train_df[(train_df['BasinTC'] == basin) & (train_df['FORMATION_CONDENSED'] == formation)]\n",
    "        combo_val = val_df[(val_df['BasinTC'] == basin) & (val_df['FORMATION_CONDENSED'] == formation)]\n",
    "        combo_test = test_df[(test_df['BasinTC'] == basin) & (test_df['FORMATION_CONDENSED'] == formation)]\n",
    "        \n",
    "        for ml_config in ml_configurations:\n",
    "            model = train_and_evaluate_model(combo_train, combo_val, combo_test, numerical_columns, categorical_columns, y_headers, output_size, ml_config['model_type'], df, task_times)\n",
    "            all_models[(basin, formation, ml_config['model_type'])] = model\n",
    "    \n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5524eae2-cc71-4939-8356-b465a34c238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f76d0bd-7a7d-4f7f-a62e-a526c9d041e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6642f6-10fa-474b-baf0-8c4c405ced9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting neural_network training\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.25380, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Predictions invalid, checking patience.\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.25380 to 0.09106, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.09106 to 0.04875, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.04875 to 0.03793, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03793 to 0.03474, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03474 to 0.03377, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03377 to 0.03344, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03344 to 0.03327, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03327\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03327\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03327\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03327 to 0.03325, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03325\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03325\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03325 to 0.03320, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03320 to 0.03318, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03318 to 0.03315, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03315 to 0.03315, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03315\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03315 to 0.03314, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03314\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03314 to 0.03313, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03313 to 0.03313, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03313 to 0.03313, saving model to best_model.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03313\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [32:13<1:36:40, 1933.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for neural_network in 1933.54 seconds\n",
      "Starting neural_network training\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.11986, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.11986 to 0.04248, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.04248 to 0.03481, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03481 to 0.03351, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03351 to 0.03335, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03335 to 0.03320, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03320\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03320 to 0.03319, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03319\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03319 to 0.03319, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 1: val_loss improved from 0.03319 to 0.03318, saving model to best_model.keras\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 1: New best weights saved.\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03318\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03318\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03318\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.03318\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1: Applying trends...\n",
      "Before applying trends:\n",
      "After applying trends:\n"
     ]
    }
   ],
   "source": [
    "# Assuming df, train_df, val_df, test_df are defined and contain the necessary columns\n",
    "# unique_combinations = df[['BasinTC', 'FORMATION_CONDENSED']].drop_duplicates()\n",
    "#configurations = [\n",
    "#    {'embedding_output_dim': 20, 'dense_layer_sizes': [256, 128, 64, 32], 'dropout_rate': 0.3, 'regularization': l2(0.01), 'activation': 'relu', 'optimizer': 'adam', 'loss_function': 'mse'},\n",
    "# {'embedding_output_dim': 20, 'dense_layer_sizes': [256, 128], 'dropout_rate': 0.3, 'regularization': l2(0.01), 'activation': 'relu', 'optimizer': 'adam', 'loss_function': 'mse'}\n",
    "\n",
    "  #    {'embedding_output_dim': 15, 'dense_layer_sizes': [64, 32], 'dropout_rate': 0.2, 'regularization': l1(0.01), 'activation': 'relu', 'optimizer': 'rmsprop', 'loss_function': 'mse'}\n",
    "#]\n",
    "ml_configurations = [\n",
    "    {'model_type': 'neural_network'},\n",
    "    # {'model_type': 'random_forest'},\n",
    "    # {'model_type': 'decision_tree'},\n",
    "    # {'model_type': 'xgboost'},\n",
    "    # {'model_type': 'ridge'},\n",
    "    # {'model_type': 'lasso'},\n",
    "    # {'model_type': 'multioutput'}\n",
    "]\n",
    "# Assuming df, train_df, val_df, test_df are defined and contain the necessary columns\n",
    "specific_combinations = df[\n",
    "    (df['BasinTC'] == 'Midland') & \n",
    "   (df['FORMATION_CONDENSED'].isin(['LSS', 'WCA', 'WCB', 'JMS']))\n",
    "   #(df['FORMATION_CONDENSED'].isin(['WCA']))\n",
    "].drop_duplicates(subset=['BasinTC', 'FORMATION_CONDENSED'])\n",
    "output_size = len(train_df[y_headers].columns)  # Ensure output_size is defined\n",
    "\n",
    "# Dictionary to store task times\n",
    "task_times = {}\n",
    "\n",
    "\n",
    "models = execute_training(specific_combinations, train_df, val_df, test_df, numerical_columns, categorical_columns, y_headers, output_size, df, task_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca610f0e-df77-48d7-89ec-77815fd15faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('train.csv', index=False)\n",
    "# val_df.to_csv('val.csv', index=False)\n",
    "# test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e57544-afe2-4ca1-b58d-c88deccab811",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e0931-6839-425e-94a3-7a66d33d1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "#shap_values_dict = compute_and_log_shap_values(models, test_df, numerical_columns, categorical_columns, shap_sample_size=100)\n",
    "# Print out the task times to identify slow tasks\n",
    "print(\"Task times (in seconds):\")\n",
    "for task, duration in task_times.items():\n",
    "    print(f\"{task}: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535421fe-b7ce-4390-9070-eb45f12e4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the variables\n",
    "# basin = 'Midland'\n",
    "# #formation = 'WCA'\n",
    "# formation = ['LSS', 'WCA', 'WCB', 'WCD']\n",
    "# config_str = \"{'embedding_output_dim': 20, 'dense_layer_sizes': [256, 128, 64, 32], 'dropout_rate': 0.3, 'regularization': <keras.src.regularizers.regularizers.L2 object at 0x0000024A7D101640>, 'activation': 'relu', 'optimizer': 'adam', 'loss_function': 'mse'}\"\n",
    "# feature_names = numerical_columns + categorical_columns  # Ensure this list contains all your feature names\n",
    "# output_pdf = r\"C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\shap_summary_plot.pdf\"\n",
    "\n",
    "# # Plot the SHAP values\n",
    "# plot_shap_values(shap_values_dict, y_headers, feature_names, output_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4cd96-6567-4c16-a0dc-ef752beb869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize numerical data\n",
    "def denormalize_data_input(data, scaler):\n",
    "    return scaler.inverse_transform(data)\n",
    "# Function to denormalize data and revert log transformation\n",
    "def denormalize_data_output(data, scaler, log_transform_columns):\n",
    "    denormalized_data = scaler.inverse_transform(data)\n",
    "    denormalized_df = pd.DataFrame(denormalized_data, columns=scaler.get_feature_names_out())\n",
    "    denormalized_df[log_transform_columns] = np.expm1(denormalized_df[log_transform_columns])\n",
    "    if any('BuildupRate' in col for col in log_transform_columns):\n",
    "        denormalized_df[log_transform_columns] = denormalized_df[log_transform_columns].replace(0, 0)\n",
    "    return denormalized_df\n",
    "# Function to decode categorical data\n",
    "def decode_categorical(data, encoders, column_names):\n",
    "    decoded_data = {}\n",
    "    for i, col in enumerate(column_names):\n",
    "        le = encoders[col]\n",
    "        decoded_data[col] = le.inverse_transform(data[:, i].astype(int))\n",
    "    return decoded_data\n",
    "# Denormalize and decode the training DataFrame\n",
    "def denormalize_and_decode(train_df, numerical_columns, categorical_columns, y_headers, input_scaler, output_scaler, encoders):\n",
    "    # Denormalize numerical features\n",
    "    train_df[numerical_columns] = denormalize_data_input(train_df[numerical_columns], input_scaler)\n",
    "    #log_transform_columns = [col for col in y_headers if 'BuildupRate' in col or 'InitialProd' in col]\n",
    "    log_transform_columns = [col for col in y_headers if 'InitialProd' in col]\n",
    "    # Denormalize target features\n",
    "    train_df[y_headers] = denormalize_data_output(train_df[y_headers], output_scaler, log_transform_columns)\n",
    "    # Decode categorical features\n",
    "    decoded_categorical = decode_categorical(train_df, encoders, categorical_columns)\n",
    "    for col in categorical_columns:\n",
    "        train_df[col] = decoded_categorical[col]\n",
    "    return train_df\n",
    "# Denormalize and decode the input DataFrame\n",
    "def denormalize_and_decode_inputs(input_df, numerical_columns, categorical_columns, input_scaler, encoders):\n",
    "    # Denormalize numerical features\n",
    "    input_df[numerical_columns] = denormalize_data_input(input_df[numerical_columns], input_scaler)\n",
    "    \n",
    "    # Decode categorical features\n",
    "    decoded_categorical = decode_categorical(input_df[categorical_columns].values, encoders, categorical_columns)\n",
    "    for col in categorical_columns:\n",
    "        input_df[col] = decoded_categorical[col]\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645ed88-f367-4c18-a786-f4d5ac038d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def hyperbolic_decline(t, qi, di, b):\n",
    "    return qi / ((1 + b * di * t) ** (1/b))\n",
    "\n",
    "def plot_decline_curves(time, actual, predicted, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(time, actual, 'b-o', label='Actual')\n",
    "    plt.plot(time, predicted, 'r--x', label='Predicted')\n",
    "    plt.yscale('log')\n",
    "    #plt.ylim(1, 100000)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (Years)')\n",
    "    plt.ylabel('Production Monthly Volumes (bbls/months)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "def generate_production_rates_for_comp(y_pred_denormalized, y_true_denormalized, headers, time, resource_type='Oil'):\n",
    "    predicted_productions = []\n",
    "    actual_productions = []\n",
    "\n",
    "    if resource_type == 'Oil':\n",
    "        prefix = 'Oil_Params_P50_'\n",
    "    elif resource_type == 'Gas':\n",
    "        prefix = 'Gas_Params_P50_'\n",
    "    elif resource_type == 'Water':\n",
    "        prefix = 'Water_Params_P50_'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resource type. Must be 'Oil', 'Gas', or 'Water'.\")\n",
    "\n",
    "    for idx in range(len(y_pred_denormalized)):\n",
    "        try:\n",
    "            qi_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd')]\n",
    "            di_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient')]\n",
    "            b_pred  = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient')]\n",
    "            IBU_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate')]\n",
    "            MBU_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd')]\n",
    "            Dlim_pred = 7#y_pred_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate')]\n",
    "            \n",
    "            qi_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd')]\n",
    "            di_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient')]\n",
    "            b_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient')]\n",
    "            IBU_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate')]\n",
    "            MBU_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd')]\n",
    "            Dlim_true = 7# y_true_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate')]\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if not validate_inputs(qi_pred, di_pred, b_pred, Dlim_pred, IBU_pred, MBU_pred):\n",
    "                print(f\"Skipping invalid prediction inputs at index {idx}: {qi_pred}, {di_pred}, {b_pred}, {Dlim_pred}, {IBU_pred}, {MBU_pred}\")\n",
    "                predicted_productions.append(np.zeros_like(time))\n",
    "                actual_productions.append(modified_hyperbolic(time, qi_true, di_true, b_true, Dlim_true, IBU_true, MBU_true))\n",
    "                continue\n",
    "\n",
    "            predicted_production = modified_hyperbolic(time, qi_pred, di_pred, b_pred, Dlim_pred, IBU_pred, MBU_pred)[1]\n",
    "            actual_production = modified_hyperbolic(time, qi_true, di_true, b_true, Dlim_true, IBU_true, MBU_true)[1]\n",
    "            \n",
    "            if len(predicted_production) == len(time) and len(actual_production) == len(time):\n",
    "                predicted_productions.append(predicted_production)\n",
    "                actual_productions.append(actual_production)\n",
    "            else:\n",
    "                print(f\"Production length mismatch: predicted {len(predicted_production)}, actual {len(actual_production)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating production rates: {e}\")\n",
    "            predicted_productions.append(np.zeros_like(time))\n",
    "            actual_productions.append(np.zeros_like(time))\n",
    "\n",
    "    return predicted_productions, actual_productions\n",
    "\n",
    "def calculate_errors(predicted_productions, actual_productions):\n",
    "    errors = {\n",
    "        'MSE': [],\n",
    "        'MAE': [],\n",
    "        'sMAPE': []\n",
    "    }\n",
    "\n",
    "    for pred, actual in zip(predicted_productions, actual_productions):\n",
    "        if len(pred) != len(actual):\n",
    "            print(f\"Length mismatch: predicted {len(pred)}, actual {len(actual)}\")\n",
    "            continue\n",
    "\n",
    "        if np.any(np.isnan(pred)) or np.any(np.isnan(actual)):\n",
    "            print(f\"NaN values found in predictions or actuals. Skipping this entry.\")\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(actual, pred)\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        smape = np.mean(np.abs(pred - actual) / ((np.abs(actual) + np.abs(pred)) / 2)) * 100\n",
    "        \n",
    "        errors['MSE'].append(mse)\n",
    "        errors['MAE'].append(mae)\n",
    "        errors['sMAPE'].append(smape)\n",
    "\n",
    "    return errors\n",
    "# Define a function to determine model type\n",
    "def determine_model_type(config_str):\n",
    "    if 'regularization' in config_str or 'dense_layer_sizes' in config_str:\n",
    "        return 'Neural Network'\n",
    "    else:\n",
    "        return config_str.split(' ')[0]\n",
    "def calculate_scalar_errors(y_pred_denormalized, y_true_denormalized, headers, resource_type='Oil'):\n",
    "    if resource_type == 'Oil':\n",
    "        prefix = 'Oil_Params_P50_'\n",
    "    elif resource_type == 'Gas':\n",
    "        prefix = 'Gas_Params_P50_'\n",
    "    elif resource_type == 'Water':\n",
    "        prefix = 'Water_Params_P50_'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resource type. Must be 'Oil', 'Gas', or 'Water'.\")\n",
    "\n",
    "    scalar_errors = {\n",
    "        'InitialProd_MAE': [],\n",
    "        'DiCoefficient_MAE': [],\n",
    "        'BCoefficient_MAE': [],\n",
    "        'BuildupRate_MAE': [],\n",
    "        'MonthsInProd_MAE': [],\n",
    "        'LimDeclineRate_MAE': [],\n",
    "        'InitialProd_RMSE': [],\n",
    "        'DiCoefficient_RMSE': [],\n",
    "        'BCoefficient_RMSE': [],\n",
    "        'BuildupRate_RMSE': [],\n",
    "        'MonthsInProd_RMSE': [],\n",
    "        'LimDeclineRate_RMSE': []\n",
    "    }\n",
    "\n",
    "    for idx in range(len(y_pred_denormalized)):\n",
    "        try:\n",
    "            qi_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd')]\n",
    "            di_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient')]\n",
    "            b_pred  = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient')]\n",
    "            IBU_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate')]\n",
    "            MBU_pred = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd')]\n",
    "            Dlim_pred =7# y_pred_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate')]\n",
    "            \n",
    "            qi_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd')]\n",
    "            di_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient')]\n",
    "            b_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient')]\n",
    "            IBU_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate')]\n",
    "            MBU_true = y_true_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd')]\n",
    "            Dlim_true = 7#y_true_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate')]\n",
    "\n",
    "            scalar_errors['InitialProd_MAE'].append(np.abs(qi_pred - qi_true))\n",
    "            scalar_errors['DiCoefficient_MAE'].append(np.abs(di_pred - di_true))\n",
    "            scalar_errors['BCoefficient_MAE'].append(np.abs(b_pred - b_true))\n",
    "            scalar_errors['BuildupRate_MAE'].append(np.abs(IBU_pred - IBU_true))\n",
    "            scalar_errors['MonthsInProd_MAE'].append(np.abs(MBU_pred - MBU_true))\n",
    "            scalar_errors['LimDeclineRate_MAE'].append(np.abs(Dlim_pred - Dlim_true))\n",
    "\n",
    "            scalar_errors['InitialProd_RMSE'].append((qi_pred - qi_true) ** 2)\n",
    "            scalar_errors['DiCoefficient_RMSE'].append((di_pred - di_true) ** 2)\n",
    "            scalar_errors['BCoefficient_RMSE'].append((b_pred - b_true) ** 2)\n",
    "            scalar_errors['BuildupRate_RMSE'].append((IBU_pred - IBU_true) ** 2)\n",
    "            scalar_errors['MonthsInProd_RMSE'].append((MBU_pred - MBU_true) ** 2)\n",
    "            scalar_errors['LimDeclineRate_RMSE'].append((Dlim_pred - Dlim_true) ** 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating scalar errors: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate mean values\n",
    "    for key in scalar_errors.keys():\n",
    "        scalar_errors[key] = np.mean(scalar_errors[key]) if len(scalar_errors[key]) > 0 else np.nan\n",
    "\n",
    "    # Convert RMSE values\n",
    "    for key in list(scalar_errors.keys()):\n",
    "        if key.endswith('_RMSE'):\n",
    "            scalar_errors[key] = np.sqrt(scalar_errors[key])\n",
    "\n",
    "    return scalar_errors\n",
    "    \n",
    "def identify_best_worst_matches(errors, y_true_denormalized, y_pred_denormalized, error_metric='MSE'):\n",
    "    # Ensure the errors dictionary contains the chosen error metric\n",
    "    if error_metric not in errors:\n",
    "        raise ValueError(f\"Error metric '{error_metric}' not found in errors.\")\n",
    "    \n",
    "    # Use the selected error metric to find the best and worst matches\n",
    "    best_match_idx = np.argmin(errors[error_metric])\n",
    "    worst_match_idx = np.argmax(errors[error_metric])\n",
    "\n",
    "    best_match_true = y_true_denormalized.iloc[best_match_idx]\n",
    "    best_match_pred = y_pred_denormalized.iloc[best_match_idx]\n",
    "\n",
    "    worst_match_true = y_true_denormalized.iloc[worst_match_idx]\n",
    "    worst_match_pred = y_pred_denormalized.iloc[worst_match_idx]\n",
    "\n",
    "    return best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx\n",
    "\n",
    "# Function to print performance metrics in a readable format\n",
    "def print_performance_metrics(basin, formation, config_str, model_type, mse, mae, smape, scalar_errors):\n",
    "    metrics = (\n",
    "        f\"Performance for Basin: {basin}\\n\"\n",
    "        f\"Formation: {formation}\\n\"\n",
    "        f\"Config: {model_type}\\n\"\n",
    "        f\"MSE: {mse:.4f}\\n\"\n",
    "        f\"MAE: {mae:.4f}\\n\"\n",
    "        f\"sMAPE: {smape:.2f}%\\n\"\n",
    "        \"Scalar Errors:\\n\"\n",
    "        f\"InitialProd_MAE: {scalar_errors['InitialProd_MAE']:.4f}\\n\"\n",
    "        f\"DiCoefficient_MAE: {scalar_errors['DiCoefficient_MAE']:.4f}\\n\"\n",
    "        f\"BCoefficient_MAE: {scalar_errors['BCoefficient_MAE']:.4f}\\n\"\n",
    "        f\"BuildupRate_MAE: {scalar_errors['BuildupRate_MAE']:.4f}\\n\"\n",
    "        f\"MonthsInProd_MAE: {scalar_errors['MonthsInProd_MAE']:.4f}\\n\"\n",
    "        f\"LimDeclineRate_MAE: {scalar_errors['LimDeclineRate_MAE']:.4f}\\n\"\n",
    "        f\"InitialProd_RMSE: {scalar_errors['InitialProd_RMSE']:.4f}\\n\"\n",
    "        f\"DiCoefficient_RMSE: {scalar_errors['DiCoefficient_RMSE']:.4f}\\n\"\n",
    "        f\"BCoefficient_RMSE: {scalar_errors['BCoefficient_RMSE']:.4f}\\n\"\n",
    "        f\"BuildupRate_RMSE: {scalar_errors['BuildupRate_RMSE']:.4f}\\n\"\n",
    "        f\"MonthsInProd_RMSE: {scalar_errors['MonthsInProd_RMSE']:.4f}\\n\"\n",
    "        f\"LimDeclineRate_RMSE: {scalar_errors['LimDeclineRate_RMSE']:.4f}\\n\"\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "# Function to save plots and performance metrics in a PDF file\n",
    "def save_plots_to_pdf(plot_data, performance_data, output_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    \n",
    "    for i, (basin, formation, config_str, time, best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx) in enumerate(plot_data):\n",
    "        model_type = determine_model_type(config_str)\n",
    "        \n",
    "        # Add a page for each model\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        \n",
    "        # Add the performance metrics\n",
    "        metrics = performance_data[(basin, formation, config_str)]\n",
    "        metrics_text = print_performance_metrics(\n",
    "            basin, formation, config_str, model_type, \n",
    "            metrics['MSE'], metrics['MAE'], metrics['sMAPE'], metrics\n",
    "        )\n",
    "        # Add bold, underlined, and red text\n",
    "        pdf.set_text_color(255, 0, 0)  # Red color\n",
    "        pdf.set_font(\"Arial\", style='BU', size=12)  # Bold and Underlined\n",
    "        pdf.multi_cell(0, 10, f\"Performance for Basin: {basin}\\nFormation: {formation}\\nConfig: {model_type}\\n\")\n",
    "\n",
    "        # Reset to regular font for the rest of the text\n",
    "        pdf.set_text_color(0, 0, 0)  # Black color\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        pdf.multi_cell(0, 10, metrics_text)\n",
    "\n",
    "        # Ensure lengths match\n",
    "        min_length = min(len(time), len(best_match_true), len(best_match_pred), len(worst_match_true), len(worst_match_pred))\n",
    "        time = time[:min_length]\n",
    "        best_match_true = best_match_true[:min_length]\n",
    "        best_match_pred = best_match_pred[:min_length]\n",
    "        worst_match_true = worst_match_true[:min_length]\n",
    "        worst_match_pred = worst_match_pred[:min_length]\n",
    "\n",
    "        # Save the plots as images\n",
    "        best_plot_path = f\"best_plot_{i}.png\"\n",
    "        worst_plot_path = f\"worst_plot_{i}.png\"\n",
    "        \n",
    "        # Generate and save best plot\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "        ax.plot(time, best_match_true, 'b-o', label='Actual')\n",
    "        ax.plot(time, best_match_pred, 'r--x', label='Predicted')\n",
    "        ax.set_yscale('log')\n",
    "        #ax.set_ylim(1, 100000)\n",
    "        ax.set_title(f'Best Match - {model_type}', fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Time (Months)')\n",
    "        ax.set_ylabel('Production Rate (bbl/day)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(best_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Generate and save worst plot\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "        ax.plot(time, worst_match_true, 'b-o', label='Actual')\n",
    "        ax.plot(time, worst_match_pred, 'r--x', label='Predicted')\n",
    "        ax.set_yscale('log')\n",
    "        #ax.set_ylim(1, 100000)\n",
    "        ax.set_title(f'Worst Match - {model_type}', fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Time (Months)')\n",
    "        ax.set_ylabel('Production Rate (bbl/day)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(worst_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Debug statements to check if images exist\n",
    "        if os.path.exists(best_plot_path):\n",
    "            print(f\"Best plot saved successfully at {best_plot_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to save best plot at {best_plot_path}\")\n",
    "\n",
    "        if os.path.exists(worst_plot_path):\n",
    "            print(f\"Worst plot saved successfully at {worst_plot_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to save worst plot at {worst_plot_path}\")\n",
    "\n",
    "        # Add images to PDF\n",
    "        pdf.image(best_plot_path, x=10, y=pdf.get_y(), w=90)\n",
    "        pdf.image(worst_plot_path, x=110, y=pdf.get_y(), w=90)\n",
    "\n",
    "        # Remove the temporary plot images\n",
    "        os.remove(best_plot_path)\n",
    "        os.remove(worst_plot_path)\n",
    "    \n",
    "    pdf.output(output_path)\n",
    "    print(f\"PDF saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b14338-c722-4e4f-97cc-c41bde096be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "#@tf.function(reduce_retracing=True)\n",
    "def predict_with_model(model, numerical_data, categorical_data):\n",
    "    if isinstance(model, Model):  # Keras model\n",
    "        return model.predict([numerical_data] + categorical_data)\n",
    "    else:  # Sklearn model\n",
    "        import pandas as pd\n",
    "        data_combined = np.hstack([numerical_data] + categorical_data)\n",
    "        combined_df = pd.DataFrame(data_combined, columns=numerical_columns + categorical_columns)\n",
    "        return model.predict(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b513030-d5af-4b0c-a0c4-03f7f74be343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "years = 20\n",
    "time = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n",
    "time_array=time\n",
    "\n",
    "# Use the function to create and save the PDF file\n",
    "output_pdf_path = f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/model_performance_withHL.pdf'\n",
    "\n",
    "evaluation_results = {}\n",
    "best_performing_models = {}\n",
    "plot_data = []\n",
    "performance_data = {}  # Store the performance metrics for each model configuration\n",
    "\n",
    "for (basin, formation, config_str), model in models.items():\n",
    "    # Filter the test data for the specific basin and formation\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    \n",
    "    # Extract numerical and categorical data from the filtered test set\n",
    "    numerical_data = combo_test[numerical_columns].values\n",
    "    categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "    \n",
    "    # Prepare the target values (y_true)\n",
    "    y_true = combo_test[y_headers].values  # Use entire y_headers as output\n",
    "\n",
    "    # Predict outputs using the model\n",
    "    y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "\n",
    "    # Denormalize predictions and actual values\n",
    "    y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "    y_true_denormalized = denormalize_data_output(y_true, output_scaler, log_transform_columns)\n",
    "\n",
    "    # Check for NaN or infinite values\n",
    "    if np.any(np.isnan(y_pred_denormalized)) or np.any(np.isinf(y_pred_denormalized)):\n",
    "        print(f\"NaN or infinite values found in predictions for {basin} - {formation} with config {config_str}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate production rates\n",
    "    predicted_productions, actual_productions = generate_production_rates_for_comp(y_pred_denormalized, y_true_denormalized, y_headers, time)\n",
    "    #print(predicted_productions)\n",
    "\n",
    "    # Calculate errors\n",
    "    errors = calculate_errors(predicted_productions, actual_productions)\n",
    "    scalar_errors = calculate_scalar_errors(y_pred_denormalized, y_true_denormalized, y_headers)\n",
    "\n",
    "    # Identify best and worst matches\n",
    "    best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx = identify_best_worst_matches(errors, y_true_denormalized, y_pred_denormalized)\n",
    "\n",
    "    # Plot the best match\n",
    "    best_actual_production = actual_productions[best_match_idx]\n",
    "    best_predicted_production = predicted_productions[best_match_idx]\n",
    "    plot_decline_curves(time_array, best_actual_production, best_predicted_production, f'Best Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "    # Plot the worst match\n",
    "    worst_actual_production = actual_productions[worst_match_idx]\n",
    "    worst_predicted_production = predicted_productions[worst_match_idx]\n",
    "    plot_decline_curves(time_array, worst_actual_production, worst_predicted_production, f'Worst Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "        # Save performance metrics\n",
    "    if y_true_denormalized.shape == y_pred_denormalized.shape:\n",
    "        mse = mean_squared_error(y_true_denormalized, y_pred_denormalized)\n",
    "        mae = mean_absolute_error(y_true_denormalized, y_pred_denormalized)\n",
    "        smape = np.mean(np.abs(y_pred_denormalized - y_true_denormalized) / ((np.abs(y_true_denormalized) + np.abs(y_pred_denormalized)) / 2)) * 100\n",
    "        performance_data[(basin, formation, config_str)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors}\n",
    "        print(f\"Performance for Basin: {basin}, Formation: {formation}, Config: {config_str} - MSE: {mse:.4f}, MAE: {mae:.4f}, sMAPE: {smape:.2f}% - Scalar Errors: {scalar_errors}\")\n",
    "        \n",
    "        # Update best performing model\n",
    "        if (basin, formation) not in best_performing_models or mse < best_performing_models[(basin, formation)]['MSE']:\n",
    "            best_performing_models[(basin, formation)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors, 'config_str': config_str, 'model': model}\n",
    "    else:\n",
    "        print(f\"Shape mismatch between y_true {y_true_denormalized.shape} and y_pred {y_pred_denormalized.shape}, check data preparation steps.\")\n",
    "\n",
    "    plot_data.append((basin, formation, config_str, time, best_actual_production, best_predicted_production, worst_actual_production, worst_predicted_production, best_match_idx, worst_match_idx))\n",
    "\n",
    "# Print the best performing model for each basin and formation\n",
    "for (basin, formation), best_model_info in best_performing_models.items():\n",
    "    model_type = determine_model_type(best_model_info['config_str'])\n",
    "    print(f\"Best Performing Model for Basin: {basin}, Formation: {formation} - Config: {model_type} - MSE: {best_model_info['MSE']:.4f}, MAE: {best_model_info['MAE']:.4f}, sMAPE: {best_model_info['sMAPE']:.2f}% - Scalar Errors: {best_model_info}\")\n",
    "\n",
    "# Save plots and performance metrics in a PDF file\n",
    "save_plots_to_pdf(plot_data, performance_data, output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02028fd6-b7c6-4f24-accf-351ec519f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust the length of time and data arrays\n",
    "def adjust_length(time, data):\n",
    "    if len(time) > len(data):\n",
    "        time = time[:len(data)]\n",
    "    else:\n",
    "        data = data[:len(time)]\n",
    "    return time, data\n",
    "\n",
    "# Function to create and save individual plots\n",
    "def create_individual_plots(data):\n",
    "    for i, (basin, formation, config_str, time, best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx) in enumerate(data):\n",
    "        if 'regularization' in config_str or 'dense_layer_sizes' in config_str:\n",
    "            model_type = 'Neural Network'\n",
    "        else:\n",
    "            model_type = config_str.split(' ')[0]\n",
    "        \n",
    "        # Check for valid data before plotting\n",
    "        if len(best_match_true) == 0 or len(best_match_pred) == 0 or len(worst_match_true) == 0 or len(worst_match_pred) == 0:\n",
    "            continue\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "        \n",
    "        # Adjust lengths of time and data\n",
    "        time_best_true, best_match_true = adjust_length(time, best_match_true)\n",
    "        time_best_pred, best_match_pred = adjust_length(time, best_match_pred)\n",
    "        time_worst_true, worst_match_true = adjust_length(time, worst_match_true)\n",
    "        time_worst_pred, worst_match_pred = adjust_length(time, worst_match_pred)\n",
    "        \n",
    "        # Plot best match\n",
    "        axs[0].plot(time_best_true, best_match_true, 'b-o', label='Actual')\n",
    "        axs[0].plot(time_best_pred, best_match_pred, 'r--x', label='Predicted')\n",
    "        axs[0].set_yscale('log')\n",
    "        #axs[0].set_ylim(1, 100000)\n",
    "        axs[0].set_title(f'{basin}-{formation} ({model_type})', fontsize=16, fontweight='bold')\n",
    "        axs[0].set_xlabel('Time (Months)')\n",
    "        axs[0].set_ylabel('Production Rate (bbl/day)')\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True, which='both', linestyle='--')\n",
    "        \n",
    "        # Plot worst match\n",
    "        axs[1].plot(time_worst_true, worst_match_true, 'b-o', label='Actual')\n",
    "        axs[1].plot(time_worst_pred, worst_match_pred, 'r--x', label='Predicted')\n",
    "        axs[1].set_yscale('log')\n",
    "        #axs[1].set_ylim(1, 100000)\n",
    "        axs[1].set_title(f'{basin}-{formation} ({model_type})', fontsize=16, fontweight='bold')\n",
    "        axs[1].set_xlabel('Time (Months)')\n",
    "        axs[1].set_ylabel('Production Rate (bbl/day)')\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True, which='both', linestyle='--')\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/plot_{i}_withHL.jpeg')\n",
    "        plt.close()\n",
    "\n",
    "# Function to create and save grid plots with adjusted lengths\n",
    "def create_grid_plot(data):\n",
    "    formations = set(item[1] for item in data)\n",
    "    \n",
    "    for formation in formations:\n",
    "        formation_data = [item for item in data if item[1] == formation]\n",
    "        valid_data = [item for item in formation_data if len(item[4]) > 0 or len(item[5]) > 0 or len(item[6]) > 0 or len(item[7]) > 0]\n",
    "        \n",
    "        if not valid_data:\n",
    "            continue\n",
    "        \n",
    "        num_methods = len(valid_data)\n",
    "        num_cols = num_methods\n",
    "        num_rows = 2\n",
    "        \n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(6 * num_cols, 12))\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "        # Ensure axs is always a 2D array\n",
    "        if num_cols == 1:\n",
    "            axs = np.array([axs]).T\n",
    "\n",
    "        for i, (basin, formation, config_str, time, best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx) in enumerate(valid_data):\n",
    "            if 'regularization' in config_str or 'dense_layer_sizes' in config_str:\n",
    "                model_type = 'Neural Network'\n",
    "            else:\n",
    "                model_type = config_str.split(' ')[0]\n",
    "            \n",
    "            col = i\n",
    "            \n",
    "            # Adjust lengths of time and data\n",
    "            time_best_true, best_match_true = adjust_length(time, best_match_true)\n",
    "            time_best_pred, best_match_pred = adjust_length(time, best_match_pred)\n",
    "            time_worst_true, worst_match_true = adjust_length(time, worst_match_true)\n",
    "            time_worst_pred, worst_match_pred = adjust_length(time, worst_match_pred)\n",
    "            \n",
    "            # Plot best match\n",
    "            if len(best_match_true) > 0 and len(best_match_pred) > 0:\n",
    "                axs[0, col].plot(time_best_true, best_match_true, 'b-o', label='Actual')\n",
    "                axs[0, col].plot(time_best_pred, best_match_pred, 'r--x', label='Predicted')\n",
    "                axs[0, col].set_yscale('log')\n",
    "                #axs[0, col].set_ylim(1, 100000)\n",
    "                axs[0, col].set_title(f'Best Match - {model_type}', fontsize=16, fontweight='bold')\n",
    "                axs[0, col].set_xlabel('Time (Months)')\n",
    "                axs[0, col].set_ylabel('Production Rate (bbl/day)')\n",
    "                axs[0, col].legend()\n",
    "                axs[0, col].grid(True, which='both', linestyle='--')\n",
    "            # else:\n",
    "            #     axs[0, col].axis('off')\n",
    "            \n",
    "            # Plot worst match on the next row\n",
    "            if len(worst_match_true) > 0 and len(worst_match_pred) > 0:\n",
    "                axs[1, col].plot(time_worst_true, worst_match_true, 'b-o', label='Actual')\n",
    "                axs[1, col].plot(time_worst_pred, worst_match_pred, 'r--x', label='Predicted')\n",
    "                axs[1, col].set_yscale('log')\n",
    "                #axs[1, col].set_ylim(1, 100000)\n",
    "                axs[1, col].set_title(f'Worst Match - {model_type}', fontsize=16, fontweight='bold')\n",
    "                axs[1, col].set_xlabel('Time (Months)')\n",
    "                axs[1, col].set_ylabel('Production Rate (bbl/day)')\n",
    "                axs[1, col].legend()\n",
    "                axs[1, col].grid(True, which='both', linestyle='--')\n",
    "            # else:\n",
    "            #     axs[1, col].axis('off')\n",
    "        \n",
    "        # # Turn off any unused subplots\n",
    "        # for j in range(i + 1, num_rows * num_cols):\n",
    "        #     row = j // num_cols\n",
    "        #     col = j % num_cols\n",
    "        #     axs[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/grid_plot_{formation}_withHL.jpeg')\n",
    "        plt.close()\n",
    "\n",
    "# Create and save individual plots\n",
    "create_individual_plots(plot_data) \n",
    "# Create and save grid plots\n",
    "create_grid_plot(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122852d1-43ba-4a31-87e1-6903ed0c79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "years = 20\n",
    "time = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n",
    "\n",
    "# Use the function to create and save the PDF file\n",
    "output_pdf_path = f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/model_performance_gas_withHL.pdf'\n",
    "\n",
    "evaluation_results = {}\n",
    "best_performing_models = {}\n",
    "plot_data_gas = []\n",
    "performance_data = {}  # Store the performance metrics for each model configuration\n",
    "\n",
    "for (basin, formation, config_str), model in models.items():\n",
    "    # Filter the test data for the specific basin and formation\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    \n",
    "    # Extract numerical and categorical data from the filtered test set\n",
    "    numerical_data = combo_test[numerical_columns].values\n",
    "    categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "    \n",
    "    # Prepare the target values (y_true)\n",
    "    y_true = combo_test[y_headers].values  # Use entire y_headers as output\n",
    "\n",
    "    # Predict outputs using the model\n",
    "    y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "\n",
    "    # Denormalize predictions and actual values\n",
    "    y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "    y_true_denormalized = denormalize_data_output(y_true, output_scaler, log_transform_columns)\n",
    "\n",
    "    # Check for NaN or infinite values\n",
    "    if np.any(np.isnan(y_pred_denormalized)) or np.any(np.isinf(y_pred_denormalized)):\n",
    "        print(f\"NaN or infinite values found in predictions for {basin} - {formation} with config {config_str}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate production rates\n",
    "    predicted_productions, actual_productions = generate_production_rates_for_comp(y_pred_denormalized, y_true_denormalized, y_headers, time,resource_type='Gas')\n",
    "\n",
    "    # Calculate errors\n",
    "    errors = calculate_errors(predicted_productions, actual_productions)\n",
    "    scalar_errors = calculate_scalar_errors(y_pred_denormalized, y_true_denormalized, y_headers,resource_type='Gas')\n",
    "\n",
    "    # Identify best and worst matches\n",
    "    best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx = identify_best_worst_matches(errors, y_true_denormalized, y_pred_denormalized)\n",
    "\n",
    "    # Plot the best match\n",
    "    best_actual_production = actual_productions[best_match_idx]\n",
    "    best_predicted_production = predicted_productions[best_match_idx]\n",
    "    plot_decline_curves(time_array, best_actual_production, best_predicted_production, f'Best Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "    # Plot the worst match\n",
    "    worst_actual_production = actual_productions[worst_match_idx]\n",
    "    worst_predicted_production = predicted_productions[worst_match_idx]\n",
    "    plot_decline_curves(time_array, worst_actual_production, worst_predicted_production, f'Worst Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "        # Save performance metrics\n",
    "    if y_true_denormalized.shape == y_pred_denormalized.shape:\n",
    "        mse = mean_squared_error(y_true_denormalized, y_pred_denormalized)\n",
    "        mae = mean_absolute_error(y_true_denormalized, y_pred_denormalized)\n",
    "        smape = np.mean(np.abs(y_pred_denormalized - y_true_denormalized) / ((np.abs(y_true_denormalized) + np.abs(y_pred_denormalized)) / 2)) * 100\n",
    "        performance_data[(basin, formation, config_str)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors}\n",
    "        print(f\"Performance for Basin: {basin}, Formation: {formation}, Config: {config_str} - MSE: {mse:.4f}, MAE: {mae:.4f}, sMAPE: {smape:.2f}% - Scalar Errors: {scalar_errors}\")\n",
    "        \n",
    "        # Update best performing model\n",
    "        if (basin, formation) not in best_performing_models or mse < best_performing_models[(basin, formation)]['MSE']:\n",
    "            best_performing_models[(basin, formation)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors, 'config_str': config_str, 'model': model}\n",
    "    else:\n",
    "        print(f\"Shape mismatch between y_true {y_true_denormalized.shape} and y_pred {y_pred_denormalized.shape}, check data preparation steps.\")\n",
    "\n",
    "    plot_data_gas.append((basin, formation, config_str, time, best_actual_production, best_predicted_production, worst_actual_production, worst_predicted_production, best_match_idx, worst_match_idx))\n",
    "\n",
    "# Print the best performing model for each basin and formation\n",
    "for (basin, formation), best_model_info in best_performing_models.items():\n",
    "    model_type = determine_model_type(best_model_info['config_str'])\n",
    "    print(f\"Best Performing Model for Basin: {basin}, Formation: {formation} - Config: {model_type} - MSE: {best_model_info['MSE']:.4f}, MAE: {best_model_info['MAE']:.4f}, sMAPE: {best_model_info['sMAPE']:.2f}% - Scalar Errors: {best_model_info}\")\n",
    "\n",
    "# Save plots and performance metrics in a PDF file\n",
    "save_plots_to_pdf(plot_data_gas, performance_data, output_pdf_path)\n",
    "# Create and save individual plots\n",
    "create_individual_plots(plot_data_gas) \n",
    "# Create and save grid plots\n",
    "create_grid_plot(plot_data_gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1e3f2-7680-4f4f-bff8-19547f437301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "years = 20\n",
    "time = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n",
    "\n",
    "# Use the function to create and save the PDF file\n",
    "output_pdf_path = f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/model_performance_water_withHL.pdf'\n",
    "\n",
    "evaluation_results = {}\n",
    "best_performing_models = {}\n",
    "plot_data_water = []\n",
    "performance_data = {}  # Store the performance metrics for each model configuration\n",
    "\n",
    "for (basin, formation, config_str), model in models.items():\n",
    "    # Filter the test data for the specific basin and formation\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    \n",
    "    # Extract numerical and categorical data from the filtered test set\n",
    "    numerical_data = combo_test[numerical_columns].values\n",
    "    categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "    \n",
    "    # Prepare the target values (y_true)\n",
    "    y_true = combo_test[y_headers].values  # Use entire y_headers as output\n",
    "\n",
    "    # Predict outputs using the model\n",
    "    y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "\n",
    "    # Denormalize predictions and actual values\n",
    "    y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "    y_true_denormalized = denormalize_data_output(y_true, output_scaler, log_transform_columns)\n",
    "\n",
    "    # Check for NaN or infinite values\n",
    "    if np.any(np.isnan(y_pred_denormalized)) or np.any(np.isinf(y_pred_denormalized)):\n",
    "        print(f\"NaN or infinite values found in predictions for {basin} - {formation} with config {config_str}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate production rates\n",
    "    predicted_productions, actual_productions = generate_production_rates_for_comp(y_pred_denormalized, y_true_denormalized, y_headers, time,resource_type='Water')\n",
    "\n",
    "    # Calculate errors\n",
    "    errors = calculate_errors(predicted_productions, actual_productions)\n",
    "    scalar_errors = calculate_scalar_errors(y_pred_denormalized, y_true_denormalized, y_headers,resource_type='Water')\n",
    "\n",
    "    # Identify best and worst matches\n",
    "    best_match_true, best_match_pred, worst_match_true, worst_match_pred, best_match_idx, worst_match_idx = identify_best_worst_matches(errors, y_true_denormalized, y_pred_denormalized)\n",
    "\n",
    "    # Plot the best match\n",
    "    best_actual_production = actual_productions[best_match_idx]\n",
    "    best_predicted_production = predicted_productions[best_match_idx]\n",
    "    plot_decline_curves(time_array, best_actual_production, best_predicted_production, f'Best Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "    # Plot the worst match\n",
    "    worst_actual_production = actual_productions[worst_match_idx]\n",
    "    worst_predicted_production = predicted_productions[worst_match_idx]\n",
    "    plot_decline_curves(time_array, worst_actual_production, worst_predicted_production, f'Worst Match Hyperbolic Decline Curve for {basin} - {formation}')\n",
    "\n",
    "        # Save performance metrics\n",
    "    if y_true_denormalized.shape == y_pred_denormalized.shape:\n",
    "        mse = mean_squared_error(y_true_denormalized, y_pred_denormalized)\n",
    "        mae = mean_absolute_error(y_true_denormalized, y_pred_denormalized)\n",
    "        smape = np.mean(np.abs(y_pred_denormalized - y_true_denormalized) / ((np.abs(y_true_denormalized) + np.abs(y_pred_denormalized)) / 2)) * 100\n",
    "        performance_data[(basin, formation, config_str)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors}\n",
    "        print(f\"Performance for Basin: {basin}, Formation: {formation}, Config: {config_str} - MSE: {mse:.4f}, MAE: {mae:.4f}, sMAPE: {smape:.2f}% - Scalar Errors: {scalar_errors}\")\n",
    "        \n",
    "        # Update best performing model\n",
    "        if (basin, formation) not in best_performing_models or mse < best_performing_models[(basin, formation)]['MSE']:\n",
    "            best_performing_models[(basin, formation)] = {'MSE': mse, 'MAE': mae, 'sMAPE': smape, **scalar_errors, 'config_str': config_str, 'model': model}\n",
    "    else:\n",
    "        print(f\"Shape mismatch between y_true {y_true_denormalized.shape} and y_pred {y_pred_denormalized.shape}, check data preparation steps.\")\n",
    "\n",
    "    plot_data_water.append((basin, formation, config_str, time, best_actual_production, best_predicted_production, worst_actual_production, worst_predicted_production, best_match_idx, worst_match_idx))\n",
    "\n",
    "# Print the best performing model for each basin and formation\n",
    "for (basin, formation), best_model_info in best_performing_models.items():\n",
    "    model_type = determine_model_type(best_model_info['config_str'])\n",
    "    print(f\"Best Performing Model for Basin: {basin}, Formation: {formation} - Config: {model_type} - MSE: {best_model_info['MSE']:.4f}, MAE: {best_model_info['MAE']:.4f}, sMAPE: {best_model_info['sMAPE']:.2f}% - Scalar Errors: {best_model_info}\")\n",
    "\n",
    "# Save plots and performance metrics in a PDF file\n",
    "save_plots_to_pdf(plot_data_water, performance_data, output_pdf_path)\n",
    "# Create and save individual plots\n",
    "create_individual_plots(plot_data_water) \n",
    "# Create and save grid plots\n",
    "create_grid_plot(plot_data_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecb2a1-9176-4e8c-9c8b-059266af5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(numerical_data, categorical_data):\n",
    "    if categorical_data:  # Check if categorical_data is not empty\n",
    "        categorical_data_combined = np.hstack([cat for cat in categorical_data])\n",
    "        combined_data = np.hstack((numerical_data, categorical_data_combined))\n",
    "    else:\n",
    "        combined_data = numerical_data\n",
    "    return combined_data\n",
    "\n",
    "def sensitivity_analysis(cov_matrix, model, numerical_columns, categorical_columns, combo_val, time, y_headers, y_true_denormalized, y_pred_denormalized, output_scaler, log_transform_columns):\n",
    "    combined_val_data = combine_data(combo_val[numerical_columns].values, \n",
    "                                     [combo_val[col].astype(int).values.reshape(-1, 1) for col in categorical_columns])\n",
    "    sensitivities = []\n",
    "    feature_names = numerical_columns + categorical_columns\n",
    "\n",
    "    # Debugging: Print the shape of combined_val_data\n",
    "    print(f\"Shape of combined_val_data: {combined_val_data.shape}\")\n",
    "\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        perturbed_data = combined_val_data.copy()\n",
    "        perturbed_data[:, i] += np.sqrt(cov_matrix[i, i])  # Perturb by one standard deviation\n",
    "\n",
    "        # Predict using the model\n",
    "        perturbed_numerical_data = perturbed_data[:, :len(numerical_columns)]\n",
    "        perturbed_categorical_data = [perturbed_data[:, len(numerical_columns) + j].reshape(-1, 1) for j in range(len(categorical_columns))]\n",
    "        y_pred_perturbed = predict_with_model(model, perturbed_numerical_data, perturbed_categorical_data)\n",
    "\n",
    "        # Denormalize the predictions\n",
    "        y_pred_perturbed_denormalized = denormalize_data_output(y_pred_perturbed, output_scaler, log_transform_columns)\n",
    "        \n",
    "        # Generate production rates\n",
    "        predicted_productions_perturbed, _ = generate_production_rates_for_comp(y_pred_perturbed_denormalized, y_true_denormalized, y_headers, time)\n",
    "\n",
    "        # Calculate mean sensitivity (change in production rate)\n",
    "        mean_sensitivity = np.mean(np.abs(np.array(predicted_productions_perturbed) - np.array(predicted_productions)), axis=0)\n",
    "        sensitivities.append(mean_sensitivity)\n",
    "\n",
    "    return sensitivities, feature_names\n",
    "\n",
    "def plot_sensitivity_analysis(sensitivities, feature_names):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_names, sensitivities)\n",
    "    plt.xlabel('Mean Sensitivity')\n",
    "    plt.ylabel('Input Features')\n",
    "    plt.title('Sensitivity Analysis of Predicted Production Rates vs Input Features')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfb6d9-eb9d-4324-83f2-ff985aabfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save individual plots\n",
    "output_dir = 'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/covarianceplots_withHL/plots'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67a84a-4a4c-4747-967a-edcb4c8296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_by_combination = {}\n",
    "\n",
    "for (basin, formation, config_str), model in models.items():\n",
    "    # Check if config_str indicates a neural network configuration\n",
    "    if 'embedding_output_dim' in config_str or 'dense_layer_sizes' in config_str:\n",
    "        model_type = 'Neural Network'\n",
    "    else:\n",
    "        model_type = config_str.split(' ')[0]  # Use the first part of the config_str as the model type\n",
    "\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    numerical_data = combo_test[numerical_columns].values\n",
    "    categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "    y_true = combo_test[y_headers].values\n",
    "\n",
    "    y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "    y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "    y_true_denormalized = denormalize_data_output(y_true, output_scaler, log_transform_columns)\n",
    "\n",
    "    years = 5\n",
    "    time_array = np.linspace(1, 12 * years, 12 * years)  # 5 Years\n",
    "\n",
    "    # Generate production rates for each sample\n",
    "    predicted_productions_list = []\n",
    "    for i in range(y_pred_denormalized.shape[0]):\n",
    "        qi_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "        di_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "        b_pred  = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "        IBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "        MBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "        Dlim_pred = 7#y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_LimDeclineRate')]\n",
    "        predicted_production = modified_hyperbolic(time_array, qi_pred, di_pred, b_pred, Dlim_pred, IBU_pred, MBU_pred)[1]\n",
    "        predicted_productions_list.append(predicted_production)\n",
    "\n",
    "    predicted_productions_flat = np.array(predicted_productions_list).flatten()\n",
    "\n",
    "    # Check if categorical columns are empty\n",
    "    if categorical_columns:\n",
    "        categorical_dummies = pd.get_dummies(combo_test[categorical_columns], drop_first=True)\n",
    "        all_features = pd.concat([combo_test[numerical_columns], categorical_dummies], axis=1)\n",
    "    else:\n",
    "        all_features = combo_test[numerical_columns]\n",
    "\n",
    "    # Repeat features to match the length of flattened predicted productions\n",
    "    repeated_features = np.repeat(all_features.values, len(time_array), axis=0)\n",
    "\n",
    "    # Ensure the repeated_features array matches the size of the predicted production rates\n",
    "    if len(repeated_features) != len(predicted_productions_flat):\n",
    "        raise ValueError(f\"Feature DataFrame and predicted productions have different lengths: {len(repeated_features)} vs {len(predicted_productions_flat)}\")\n",
    "\n",
    "    # Calculate covariance\n",
    "    cov_matrix = np.cov(repeated_features.T, predicted_productions_flat)\n",
    "    feature_names = all_features.columns\n",
    "    sensitivity = cov_matrix[:-1, -1]\n",
    "\n",
    "    # Create a separate figure for each combination\n",
    "    plt.figure(figsize=(14, 20))\n",
    "    sns.barplot(x=sensitivity, y=feature_names)\n",
    "    plt.title(f'{basin}-{formation} ({model_type})', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Covariance', fontsize=12, fontname='Arial')\n",
    "    plt.ylabel('Input Features', fontsize=14, fontweight='bold', fontname='Arial')\n",
    "    plt.xticks(fontsize=10, fontname='Arial')\n",
    "    plt.yticks(fontsize=10, fontname='Arial')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = os.path.join(output_dir, f'{basin}_{formation}_{model_type}.jpeg')\n",
    "    plt.savefig(plot_filename, format='jpeg',dpi=(500))\n",
    "    plt.close()\n",
    "    \n",
    "    # Store plot filename for grid organization\n",
    "    if (basin, formation) not in plots_by_combination:\n",
    "        plots_by_combination[(basin, formation)] = []\n",
    "    plots_by_combination[(basin, formation)].append(plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e98d2-fa34-4cfa-9e44-df8b186fe7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save individual plots\n",
    "output_dir = 'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/EURcovarianceplots_withHL'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a62434-0d56-47bc-bee2-423002ca8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_production_and_EUR(qi, di, b, Dlim, IBU, MBU, years=30, buildup_method='Linear'):\n",
    "    months = np.linspace(0, years * 12, years * 12 + 1)\n",
    "    production = modified_hyperbolic(months, qi, di, b, Dlim, IBU, MBU, buildup_method)[0]\n",
    "    cumulative_production = np.cumsum(production)\n",
    "    EUR = cumulative_production[-1]\n",
    "    return cumulative_production, EUR\n",
    "\n",
    "def generate_EUR_predictions(models, test_df, numerical_columns, categorical_columns, output_scaler, log_transform_columns):\n",
    "    EUR_predictions = {}\n",
    "    for (basin, formation, config_str), model in models.items():\n",
    "        combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "        numerical_data = combo_test[numerical_columns].values\n",
    "        categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "        y_true = combo_test[y_headers].values\n",
    "\n",
    "        y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "        y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "\n",
    "        EUR_list = []\n",
    "        for i in range(y_pred_denormalized.shape[0]):\n",
    "            qi_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "            di_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "            b_pred  = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "            IBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "            MBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "            Dlim_pred =7# y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_LimDeclineRate')]\n",
    "            _, EUR = cumulative_production_and_EUR(qi_pred, di_pred, b_pred, Dlim_pred, IBU_pred, MBU_pred)\n",
    "            EUR_list.append(EUR)\n",
    "\n",
    "        EUR_predictions[(basin, formation, config_str)] = np.array(EUR_list)\n",
    "    return EUR_predictions\n",
    "\n",
    "def denormalize_features(combo_test, numerical_columns, categorical_columns, input_scaler, encoders):\n",
    "    denormalized_numerical = denormalize_data_input(combo_test[numerical_columns].values, input_scaler)\n",
    "    decoded_categorical = decode_categorical(combo_test[categorical_columns].values, encoders, categorical_columns)\n",
    "    \n",
    "    combo_test_denorm = combo_test.copy()\n",
    "    combo_test_denorm[numerical_columns] = denormalized_numerical\n",
    "    for col in categorical_columns:\n",
    "        combo_test_denorm[col] = decoded_categorical[col]\n",
    "    return combo_test_denorm\n",
    "\n",
    "def plot_feature_vs_EUR(combo_test, feature, EUR_values, output_dir, basin, formation, model_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Check if the feature is categorical\n",
    "    if combo_test[feature].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        feature_values = le.fit_transform(combo_test[feature].values).reshape(-1, 1)\n",
    "        feature_label = f'{feature} (encoded)'\n",
    "    else:\n",
    "        feature_values = combo_test[feature].values.reshape(-1, 1)\n",
    "        feature_label = feature\n",
    "    \n",
    "    # Remove outliers based on Z-score\n",
    "    feature_zscores = zscore(feature_values)\n",
    "    EUR_zscores = zscore(EUR_values)\n",
    "    mask = (np.abs(feature_zscores) < 3).ravel() & (np.abs(EUR_zscores) < 3)\n",
    "    \n",
    "    filtered_feature_values = feature_values[mask].reshape(-1, 1)\n",
    "    filtered_EUR_values = EUR_values[mask]\n",
    "    \n",
    "    if len(filtered_feature_values) > 0 and len(filtered_EUR_values) > 0:\n",
    "        plt.scatter(filtered_feature_values, filtered_EUR_values, alpha=0.5)\n",
    "        \n",
    "        # Adding a trend line\n",
    "        model = LinearRegression()\n",
    "        model.fit(filtered_feature_values, filtered_EUR_values)\n",
    "        trendline = model.predict(filtered_feature_values)\n",
    "        plt.plot(filtered_feature_values, trendline, color='red', linewidth=2, label='Trend line')\n",
    "\n",
    "        # Calculating slope and R²\n",
    "        slope = model.coef_[0]\n",
    "        r2 = model.score(filtered_feature_values, filtered_EUR_values)\n",
    "\n",
    "        # Adding text annotation for slope and R²\n",
    "        plt.annotate(f'Slope: {slope:.2f}\\nR²: {r2:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "                     horizontalalignment='left', verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7))\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No data points available after filtering', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.title(f'Feature: {feature_label} vs. EUR\\n{basin}-{formation} ({model_type})', fontsize=16)\n",
    "    plt.xlabel(f'{feature_label}', fontsize=12)\n",
    "    plt.ylabel('EUR', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Adding whitespace below the plot for footnotes\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Adding footnotes as text below the plot\n",
    "    footnote = f'{feature_label}: {\"Positive\" if np.cov(filtered_feature_values.ravel(), filtered_EUR_values)[0, 1] > 0 else \"Negative\"} covariance with EUR.' if len(filtered_feature_values) > 0 else 'Insufficient data to determine covariance.'\n",
    "    plt.figtext(0.5, -0.01, footnote, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "    plot_filename = os.path.join(output_dir, f'{feature}_vs_EUR.jpeg')\n",
    "    plt.savefig(plot_filename, format='jpeg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return plot_filename\n",
    "\n",
    "def calculate_covariance(combo_test, EUR_values, feature):\n",
    "    feature_values = combo_test[feature].values\n",
    "    \n",
    "    # Convert categorical features to numerical values\n",
    "    if combo_test[feature].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        feature_values = le.fit_transform(feature_values)\n",
    "    \n",
    "    # List of columns where zero values should be excluded\n",
    "    zero_data_exclusion_columns = [\n",
    "        'NNAZ_1_EUR_30yr_Actual_Oil_P50_MBO',\n",
    "        'NNAZ_1_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    "        'NNAZ_1_Cumulative oil mbo',\n",
    "        'NNAZ_1_Cumulative gas mmcf',\n",
    "        'NNAZ_1_Cumulative water mbbl',\n",
    "        'NNAZ_2_EUR_30yr_Actual_Oil_P50_MBO',\n",
    "        'NNAZ_2_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    "        'NNAZ_2_Cumulative oil mbo',\n",
    "        'NNAZ_2_Cumulative gas mmcf',\n",
    "        'NNAZ_2_Cumulative water mbbl',\n",
    "        'NNSZ_1_EUR_30yr_Actual_Oil_P50_MBO',\n",
    "        'NNSZ_1_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    "        'NNSZ_1_Cumulative oil mbo',\n",
    "        'NNSZ_1_Cumulative gas mmcf',\n",
    "        'NNSZ_1_Cumulative water mbbl',\n",
    "        'NNSZ_2_EUR_30yr_Actual_Oil_P50_MBO',\n",
    "        'NNSZ_2_EUR_30yr_Actual_Gas_P50_MMCF',\n",
    "        'NNSZ_2_Cumulative oil mbo',\n",
    "        'NNSZ_2_Cumulative gas mmcf',\n",
    "        'NNSZ_2_Cumulative water mbbl'\n",
    "    ]\n",
    "\n",
    "    # Exclude zero data points\n",
    "    if feature in zero_data_exclusion_columns:\n",
    "        non_zero_indices = np.nonzero(feature_values)[0]\n",
    "        feature_values = feature_values[non_zero_indices]\n",
    "        EUR_values = EUR_values[non_zero_indices]\n",
    "\n",
    "    # Exclude 5280 values for columns ending with '_HZDIST'\n",
    "    if feature.endswith('_HZDIST'):\n",
    "        non_5280_indices = np.where(feature_values != 5280)[0]\n",
    "        feature_values = feature_values[non_5280_indices]\n",
    "        EUR_values = EUR_values[non_5280_indices]\n",
    "\n",
    "    # Calculate covariance only if there are enough data points\n",
    "    if len(feature_values) > 1:\n",
    "        covariance = np.cov(feature_values, EUR_values)[0, 1]\n",
    "    else:\n",
    "        covariance = np.nan\n",
    "\n",
    "    return covariance\n",
    "\n",
    "\n",
    "def stitch_plots_to_pdf(plot_files, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    images_per_row = 3\n",
    "    image_width = 60  # Adjust the width according to your needs\n",
    "    image_height = 60  # Adjust the height according to your needs\n",
    "    x_offset = 10\n",
    "    y_offset = 10\n",
    "\n",
    "    for index, plot_file in enumerate(plot_files):\n",
    "        if index % images_per_row == 0:\n",
    "            pdf.add_page()\n",
    "            y_offset = 10  # Reset y offset for new page\n",
    "\n",
    "        x_position = x_offset + (index % images_per_row) * image_width\n",
    "\n",
    "        pdf.image(plot_file, x_position, y_offset, image_width, image_height)\n",
    "\n",
    "        if (index + 1) % images_per_row == 0:\n",
    "            y_offset += image_height\n",
    "\n",
    "    pdf.output(output_pdf_path, \"F\")\n",
    "\n",
    "def stitch_feature_vs_EUR_plots_to_pdf(plot_files, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    images_per_row = 2\n",
    "    image_width = 90  # Adjust the width according to your needs\n",
    "    image_height = 90  # Adjust the height according to your needs\n",
    "    x_offset = 10\n",
    "    y_offset = 10\n",
    "\n",
    "    for index, plot_file in enumerate(plot_files):\n",
    "        if index % images_per_row == 0:\n",
    "            pdf.add_page()\n",
    "            y_offset = 10  # Reset y offset for new page\n",
    "\n",
    "        x_position = x_offset + (index % images_per_row) * (image_width + x_offset)\n",
    "\n",
    "        pdf.image(plot_file, x_position, y_offset, image_width, image_height)\n",
    "\n",
    "        if (index + 1) % images_per_row == 0:\n",
    "            y_offset += image_height + y_offset\n",
    "\n",
    "    pdf.output(output_pdf_path, \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aa904-8f7e-478a-89dc-780cbef1957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_by_combination = {}\n",
    "\n",
    "EUR_predictions = generate_EUR_predictions(models, test_df, numerical_columns, categorical_columns, output_scaler, log_transform_columns)\n",
    "\n",
    "for (basin, formation, config_str), EUR_pred in EUR_predictions.items():\n",
    "    if 'embedding_output_dim' in config_str or 'dense_layer_sizes' in config_str:\n",
    "        model_type = 'Neural Network'\n",
    "    else:\n",
    "        model_type = config_str.split(' ')[0]\n",
    "\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    combo_test_denorm = denormalize_features(combo_test, numerical_columns, categorical_columns, input_scaler, encoders)\n",
    "\n",
    "    basin_formation_dir = os.path.join(output_dir, f'{basin}_{formation}')\n",
    "    method_dir = os.path.join(basin_formation_dir, model_type)\n",
    "    os.makedirs(method_dir, exist_ok=True)\n",
    "\n",
    "    plot_filenames = []\n",
    "    covariances = {}\n",
    "    for feature in numerical_columns + categorical_columns:\n",
    "        plot_filename = plot_feature_vs_EUR(combo_test_denorm, feature, EUR_pred, method_dir, basin, formation, model_type)\n",
    "        plot_filenames.append(plot_filename)\n",
    "        covariance = calculate_covariance(combo_test_denorm, EUR_pred, feature)\n",
    "        covariances[feature] = covariance\n",
    "\n",
    "    sorted_features = sorted(covariances.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "    features, covariances = zip(*sorted_features)\n",
    "\n",
    "    plt.figure(figsize=(14, 20))\n",
    "    sns.barplot(x=covariances, y=features)\n",
    "    plt.title(f'{basin}-{formation} ({model_type})', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Covariance with EUR', fontsize=12, fontname='Arial')\n",
    "    plt.ylabel('Input Features', fontsize=14, fontweight='bold', fontname='Arial')\n",
    "    plt.xticks(fontsize=10, fontname='Arial')\n",
    "    plt.yticks(fontsize=10, fontname='Arial')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adding critical comments as footnotes\n",
    "    footnotes = \"\\n\".join([f\"{feature}: {'Positive' if cov > 0 else 'Negative'} covariance with EUR.\"\n",
    "                           for feature, cov in sorted_features])\n",
    "    plt.figtext(0.5, -0.4, footnotes, wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "    cov_plot_filename = os.path.join(method_dir, 'covariance_plot.jpeg')\n",
    "    plt.savefig(cov_plot_filename, format='jpeg', dpi=500, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    if (basin, formation) not in plots_by_combination:\n",
    "        plots_by_combination[(basin, formation)] = []\n",
    "    plots_by_combination[(basin, formation)].append(cov_plot_filename)\n",
    "\n",
    "# Stitch the plots into PDFs for each combination\n",
    "for (basin, formation), plot_files in plots_by_combination.items():\n",
    "    pdf_output_path = os.path.join(output_dir, f'{basin}_{formation}_EUR_plots.pdf')\n",
    "    stitch_plots_to_pdf(plot_files, pdf_output_path)\n",
    "\n",
    "    pdf_output_path_feature_vs_EUR = os.path.join(output_dir, f'{basin}_{formation}_feature_vs_EUR_plots.pdf')\n",
    "    stitch_feature_vs_EUR_plots_to_pdf(plot_files, pdf_output_path_feature_vs_EUR)\n",
    "\n",
    "print(\"Plots generated and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59474d59-efee-456b-bba5-78cbd4b7d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_P50 = 'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/EURP50covarianceplots_withHL'\n",
    "output_dir_Sectional = 'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/EURSectionalcovarianceplots_withHL'\n",
    "os.makedirs(output_dir_P50, exist_ok=True)\n",
    "os.makedirs(output_dir_Sectional, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395381e6-5ad5-4677-9633-54dc3487121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_EUR_new(combo_test, feature, EUR_values, output_dir, basin, formation, model_type, EUR_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if combo_test[feature].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        feature_values = le.fit_transform(combo_test[feature].values).reshape(-1, 1)\n",
    "        feature_label = f'{feature} (encoded)'\n",
    "    else:\n",
    "        feature_values = combo_test[feature].values.reshape(-1, 1)\n",
    "        feature_label = feature\n",
    "    \n",
    "    plt.scatter(feature_values, EUR_values, alpha=0.5)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(feature_values, EUR_values)\n",
    "    trendline = model.predict(feature_values)\n",
    "    plt.plot(feature_values, trendline, color='red', linewidth=2, label='Trend line')\n",
    "\n",
    "    slope = model.coef_[0]\n",
    "    r2 = model.score(feature_values, EUR_values)\n",
    "\n",
    "    plt.annotate(f'Slope: {slope:.2f}\\nR²: {r2:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "                 horizontalalignment='left', verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    plt.title(f'Feature: {feature_label} vs. {EUR_type}\\n{basin}-{formation} ({model_type})', fontsize=16)\n",
    "    plt.xlabel(f'{feature_label}', fontsize=12)\n",
    "    plt.ylabel(EUR_type, fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    footnote = f'{feature_label}: {\"Positive\" if np.cov(feature_values.ravel(), EUR_values)[0, 1] > 0 else \"Negative\"} covariance with {EUR_type}.'\n",
    "    plt.figtext(0.5, -0.01, footnote, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "    plot_filename = os.path.join(output_dir, f'{feature}_vs_{EUR_type}.jpeg')\n",
    "    plt.savefig(plot_filename, format='jpeg', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return plot_filename\n",
    "   \n",
    "\n",
    "def calculate_covariance_new(combo_test, EUR_values, feature):\n",
    "    feature_values = combo_test[feature].values\n",
    "    if combo_test[feature].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        feature_values = le.fit_transform(feature_values)\n",
    "    \n",
    "    if len(feature_values) > 1:\n",
    "        covariance = np.cov(feature_values, EUR_values)[0, 1]\n",
    "    else:\n",
    "        covariance = np.nan\n",
    "    return covariance\n",
    "\n",
    "def stitch_plots_to_pdf_new(plot_files, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    images_per_row = 3\n",
    "    image_width = 60\n",
    "    image_height = 60\n",
    "    x_offset = 10\n",
    "    y_offset = 10\n",
    "\n",
    "    for index, plot_file in enumerate(plot_files):\n",
    "        if index % images_per_row == 0:\n",
    "            pdf.add_page()\n",
    "            y_offset = 10\n",
    "\n",
    "        x_position = x_offset + (index % images_per_row) * image_width\n",
    "\n",
    "        pdf.image(plot_file, x_position, y_offset, image_width, image_height)\n",
    "\n",
    "        if (index + 1) % images_per_row == 0:\n",
    "            y_offset += image_height\n",
    "\n",
    "    pdf.output(output_pdf_path, \"F\")\n",
    "\n",
    "def stitch_feature_vs_EUR_plots_to_pdf_new(plot_files, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    images_per_row = 2\n",
    "    image_width = 90\n",
    "    image_height = 90\n",
    "    x_offset = 10\n",
    "    y_offset = 10\n",
    "\n",
    "    for index, plot_file in enumerate(plot_files):\n",
    "        if index % images_per_row == 0:\n",
    "            pdf.add_page()\n",
    "            y_offset = 10\n",
    "\n",
    "        x_position = x_offset + (index % images_per_row) * (image_width + x_offset)\n",
    "\n",
    "        pdf.image(plot_file, x_position, y_offset, image_width, image_height)\n",
    "\n",
    "        if (index + 1) % images_per_row == 0:\n",
    "            y_offset += image_height + y_offset\n",
    "\n",
    "    pdf.output(output_pdf_path, \"F\")\n",
    "\n",
    "def generate_plots_and_covariance_new(combo_test, numerical_columns, categorical_columns, output_dir, basin, formation, model_type, EUR_Combined_P50_values, EUR_sectional_P50_values):\n",
    "    EUR_types = {\n",
    "        'EUR_Combined_P50': {\n",
    "            'values': EUR_Combined_P50_values, \n",
    "            'output_dir': output_dir_P50\n",
    "        },\n",
    "        'EUR_sectional_P50': {\n",
    "            'values': EUR_sectional_P50_values, \n",
    "            'output_dir': output_dir_Sectional\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for EUR_type, data in EUR_types.items():\n",
    "        EUR_values = data['values']\n",
    "        output_dir = data['output_dir']\n",
    "\n",
    "        method_dir = os.path.join(output_dir, f'{basin}_{formation}', model_type, EUR_type)\n",
    "        os.makedirs(method_dir, exist_ok=True)\n",
    "\n",
    "        plot_filenames = []\n",
    "        covariances = {}\n",
    "        for feature in numerical_columns + categorical_columns:\n",
    "            plot_filename = plot_feature_vs_EUR_new(combo_test, feature, EUR_values, method_dir, basin, formation, model_type, EUR_type)\n",
    "            plot_filenames.append(plot_filename)\n",
    "            covariance = calculate_covariance_new(combo_test, EUR_values, feature)\n",
    "            covariances[feature] = covariance\n",
    "\n",
    "        sorted_features = sorted(covariances.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "        features, covariances = zip(*sorted_features)\n",
    "\n",
    "        plt.figure(figsize=(14, 20))\n",
    "        sns.barplot(x=covariances, y=features)\n",
    "        plt.title(f'{basin}-{formation} ({model_type}) - {EUR_type}', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(f'Covariance with {EUR_type}', fontsize=12, fontname='Arial')\n",
    "        plt.ylabel('Input Features', fontsize=14, fontweight='bold', fontname='Arial')\n",
    "        plt.xticks(fontsize=10, fontname='Arial')\n",
    "        plt.yticks(fontsize=10, fontname='Arial')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        footnotes = \"\\n\".join([f\"{feature}: {'Positive' if cov > 0 else 'Negative'} covariance with {EUR_type}.\"\n",
    "                               for feature, cov in sorted_features])\n",
    "        \n",
    "        num_lines = len(footnotes.split('\\n'))\n",
    "        footnote_position = -0.01 - (num_lines * 0.02)\n",
    "        \n",
    "        plt.figtext(0.5, footnote_position, footnotes, wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "        cov_plot_filename = os.path.join(method_dir, 'covariance_plot.jpeg')\n",
    "        plt.savefig(cov_plot_filename, format='jpeg', dpi=500, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        stitch_plots_to_pdf_new(plot_filenames, os.path.join(method_dir, f'all_plots_{EUR_type}.pdf'))\n",
    "        stitch_feature_vs_EUR_plots_to_pdf_new(plot_filenames, os.path.join(method_dir, f'feature_vs_{EUR_type}_plots.pdf'))\n",
    "\n",
    "        if (basin, formation) not in plots_by_combination:\n",
    "            plots_by_combination[(basin, formation)] = []\n",
    "        plots_by_combination[(basin, formation)].append(cov_plot_filename)\n",
    "\n",
    "def generate_EUR_predictions_combined_sectional(models, test_df, numerical_columns, categorical_columns, output_scaler, log_transform_columns):\n",
    "    EUR_predictions = {}\n",
    "    EUR_sectional_predictions = {}\n",
    "    for (basin, formation, config_str), model in models.items():\n",
    "        combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "        numerical_data = combo_test[numerical_columns].values\n",
    "        categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "        y_true = combo_test[y_headers].values\n",
    "\n",
    "        y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "        y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "\n",
    "        EUR_list = []\n",
    "        EUR_sectional_list = []\n",
    "        for i in range(y_pred_denormalized.shape[0]):\n",
    "            oil_qi_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_InitialProd')]\n",
    "            oil_di_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_DiCoefficient')]\n",
    "            oil_b_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BCoefficient')]\n",
    "            oil_IBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_BuildupRate')]\n",
    "            oil_MBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_MonthsInProd')]\n",
    "            oil_Dlim_pred = 7#y_pred_denormalized.iloc[i, y_headers.index('Oil_Params_P50_LimDeclineRate')]\n",
    "            _, oil_EUR = cumulative_production_and_EUR(oil_qi_pred, oil_di_pred, oil_b_pred, oil_Dlim_pred, oil_IBU_pred, oil_MBU_pred)\n",
    "\n",
    "            gas_qi_pred = y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_InitialProd')]\n",
    "            gas_di_pred = y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_DiCoefficient')]\n",
    "            gas_b_pred = y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_BCoefficient')]\n",
    "            gas_IBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_BuildupRate')]\n",
    "            gas_MBU_pred = y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_MonthsInProd')]\n",
    "            gas_Dlim_pred = 7#y_pred_denormalized.iloc[i, y_headers.index('Gas_Params_P50_LimDeclineRate')]\n",
    "            _, gas_EUR = cumulative_production_and_EUR(gas_qi_pred, gas_di_pred, gas_b_pred, gas_Dlim_pred, gas_IBU_pred, gas_MBU_pred)\n",
    "\n",
    "            EUR_Combined_P50 = oil_EUR + gas_EUR / 20\n",
    "            \n",
    "            # Ensure NNSZ_1_HZDIST and NNSZ_2_HZDIST are scaled correctly\n",
    "            denorm_NNSZ_1_HZDIST = None\n",
    "            denorm_NNSZ_2_HZDIST = None\n",
    "            # Check if 'NNSZ_1_HZDIST' is in the numerical columns and calculate if present\n",
    "            if 'NNSZ_1_HZDIST' in numerical_columns:\n",
    "                denorm_NNSZ_1_HZDIST = input_scaler.inverse_transform(combo_test[numerical_columns])[i, numerical_columns.index('NNSZ_1_HZDIST')]\n",
    "            # Check if 'NNSZ_2_HZDIST' is in the numerical columns and calculate if present\n",
    "            if 'NNSZ_2_HZDIST' in numerical_columns:\n",
    "                denorm_NNSZ_2_HZDIST = input_scaler.inverse_transform(combo_test[numerical_columns])[i, numerical_columns.index('NNSZ_2_HZDIST')]\n",
    "            # Determine the minimum value considering the possibility that one of the distances might be missing\n",
    "            if denorm_NNSZ_1_HZDIST is not None and denorm_NNSZ_2_HZDIST is not None:\n",
    "                min_NNSZ_HZDIST = min(denorm_NNSZ_1_HZDIST, denorm_NNSZ_2_HZDIST)\n",
    "            elif denorm_NNSZ_1_HZDIST is not None:\n",
    "                min_NNSZ_HZDIST = denorm_NNSZ_1_HZDIST\n",
    "            elif denorm_NNSZ_2_HZDIST is not None:\n",
    "                min_NNSZ_HZDIST = denorm_NNSZ_2_HZDIST\n",
    "            else:\n",
    "                min_NNSZ_HZDIST = None  # Handle the case where neither value is present\n",
    "\n",
    "            # Only perform the EUR_sectional_P50 calculation if a valid min_NNSZ_HZDIST is found\n",
    "            if min_NNSZ_HZDIST is not None:\n",
    "                EUR_sectional_P50 = EUR_Combined_P50 * 5280 / min_NNSZ_HZDIST\n",
    "\n",
    "            EUR_list.append(EUR_Combined_P50)\n",
    "            EUR_sectional_list.append(EUR_sectional_P50)\n",
    "\n",
    "        EUR_predictions[(basin, formation, config_str)] = np.array(EUR_list)\n",
    "        EUR_sectional_predictions[(basin, formation, config_str)] = np.array(EUR_sectional_list)\n",
    "    \n",
    "    return EUR_predictions, EUR_sectional_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f95a2-e4bd-4ef9-a88f-1363ace69df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a170655-416f-437f-9de0-feff739b3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_by_combination = {}\n",
    "\n",
    "EUR_predictions, EUR_sectional_predictions = generate_EUR_predictions_combined_sectional(models, test_df, numerical_columns, categorical_columns, output_scaler, log_transform_columns)\n",
    "\n",
    "for (basin, formation, config_str), EUR_pred in EUR_predictions.items():\n",
    "    # EUR_pred corresponds to EUR_Combined_P50\n",
    "    EUR_sectional_pred = EUR_sectional_predictions[(basin, formation, config_str)]\n",
    "    # EUR_sectional_pred corresponds to EUR_sectional_P50\n",
    "    if 'embedding_output_dim' in config_str or 'dense_layer_sizes' in config_str:\n",
    "        model_type = 'Neural Network'\n",
    "    else:\n",
    "        model_type = config_str.split(' ')[0]\n",
    "\n",
    "    combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "    combo_test_denorm = denormalize_features(combo_test, numerical_columns, categorical_columns, input_scaler, encoders)\n",
    "\n",
    "    generate_plots_and_covariance_new(combo_test_denorm, numerical_columns, categorical_columns, output_dir_P50, basin, formation, model_type, EUR_pred, EUR_sectional_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bd67e-b6c4-4f0d-a7a9-864af72e0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the limit for maximum image pixels\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# Load the saved plots and stitch them together into a grid for each combination\n",
    "for (basin, formation), plot_filenames in plots_by_combination.items():\n",
    "    images = [Image.open(filename) for filename in plot_filenames]\n",
    "\n",
    "    # Determine the number of rows and columns for the grid\n",
    "    num_plots = len(images)\n",
    "    num_cols = 4  # Set the number of columns for the grid\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    # Get the dimensions of the images\n",
    "    image_width, image_height = images[0].size\n",
    "\n",
    "    # Create a new blank image for the grid\n",
    "    grid_image = Image.new('RGB', (num_cols * image_width, num_rows * image_height))\n",
    "\n",
    "    # Paste each plot image into the grid\n",
    "    for index, image in enumerate(images):\n",
    "        row = index // num_cols\n",
    "        col = index % num_cols\n",
    "        grid_image.paste(image, (col * image_width, row * image_height))\n",
    "\n",
    "    # Save the final grid image for the combination\n",
    "    grid_image.save(f'C:/Users/Prakhar.Sarkar/OneDrive - SRP Management Services/Documents/_For_Prakhar/covarianceplots/{basin}_{formation}_grid_image_withHL.jpeg', format='jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7b04b-3b28-4d12-b42e-08f1aa04f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a652e-19ba-4395-8766-9e72c0f629a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_performing_index(models, test_df, numerical_columns, categorical_columns, y_headers, output_scaler, log_transform_columns):\n",
    "#     best_performing_indices = {}\n",
    "#     for (basin, formation, config_str), model in models.items():\n",
    "#         combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "#         numerical_data = combo_test[numerical_columns].values\n",
    "#         categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "#         y_true = combo_test[y_headers].values\n",
    "        \n",
    "#         y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "#         y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "#         y_true_denormalized = denormalize_data_output(y_true, output_scaler, log_transform_columns)\n",
    "\n",
    "#         mse = mean_squared_error(y_true_denormalized, y_pred_denormalized, multioutput='raw_values')\n",
    "        \n",
    "#         if mse.size > 0:\n",
    "#             best_match_idx = mse.mean(axis=0).argmin()\n",
    "#             if (basin, formation) not in best_performing_indices or mse.mean() < best_performing_indices[(basin, formation)]['mse']:\n",
    "#                 best_performing_indices[(basin, formation)] = {\n",
    "#                     'index': best_match_idx,\n",
    "#                     'mse': mse.mean()\n",
    "#                 }\n",
    "#     return best_performing_indices\n",
    "\n",
    "# def identify_best_performing_rows(models, test_df, numerical_columns, categorical_columns, y_headers, output_scaler, log_transform_columns, best_performing_indices):\n",
    "#     best_performing_rows = {}\n",
    "#     for (basin, formation, config_str), model in models.items():\n",
    "#         combo_test = filter_by_basin_and_formation(test_df, basin, formation)\n",
    "#         best_match_idx = best_performing_indices[(basin, formation)]['index']\n",
    "#         best_match_row = combo_test.iloc[best_match_idx]\n",
    "        \n",
    "#         best_performing_rows[(basin, formation, config_str)] = {\n",
    "#             'model': model,\n",
    "#             'best_match_row': best_match_row\n",
    "#         }\n",
    "#     return best_performing_rows\n",
    "\n",
    "\n",
    "# def generate_sensitivity_data(best_performing_row, model, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array):\n",
    "#     variations = {\n",
    "#         'HORIZONTIAL_WELL_LENGTH': np.arange(8000, 11500, 500),\n",
    "#         'ProppantPerFoot': np.arange(1000, 2200, 200),\n",
    "#         'FluidPerFoot_bblft': np.arange(40, 70, 10)\n",
    "#     }\n",
    "    \n",
    "#     results = {key: [] for key in variations.keys()}\n",
    "#     avg_differences = {key: 0 for key in variations.keys()}\n",
    "    \n",
    "#     # Convert the best_performing_row to a DataFrame\n",
    "#     best_performing_row_df = pd.DataFrame([best_performing_row])\n",
    "    \n",
    "#     # Debug: print the best performing row DataFrame\n",
    "#     #print(\"Best performing row DataFrame:\\n\", best_performing_row_df)\n",
    "    \n",
    "#     # Predict the base production rate\n",
    "#     base_categorical_data = [best_performing_row_df[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "#     y_base_pred = predict_with_model(model, best_performing_row_df[numerical_columns].values, base_categorical_data)\n",
    "#     y_base_denormalized = denormalize_data_output(y_base_pred, output_scaler, log_transform_columns)\n",
    "    \n",
    "#     # Debug: print the base denormalized predictions and y_headers\n",
    "#     #print(\"Base denormalized predictions:\\n\", y_base_denormalized)\n",
    "#     #print(\"y_headers:\\n\", y_headers)\n",
    "#     #print(\"Columns in y_base_denormalized:\\n\", y_base_denormalized.columns)\n",
    "    \n",
    "#     # Access columns by name\n",
    "#     base_production_rates = modified_hyperbolic(time_array, \n",
    "#                                                 y_base_denormalized['Oil_Params_P50_InitialProd'][0],\n",
    "#                                                 y_base_denormalized['Oil_Params_P50_DiCoefficient'][0],\n",
    "#                                                 y_base_denormalized['Oil_Params_P50_BCoefficient'][0],\n",
    "#                                                 7,#y_base_denormalized['Oil_Params_P50_LimDeclineRate'][0],\n",
    "#                                                 y_base_denormalized['Oil_Params_P50_BuildupRate'][0],\n",
    "#                                                 y_base_denormalized['Oil_Params_P50_MonthsInProd'][0])\n",
    "    \n",
    "#     for param, values in variations.items():\n",
    "#         differences = []\n",
    "#         for value in values:\n",
    "#             # Create a copy of the row with the modified value\n",
    "#             modified_row_df = best_performing_row_df.copy()\n",
    "#             modified_row_df = denormalize_and_decode_inputs(modified_row_df, numerical_columns, categorical_columns, input_scaler, encoders)\n",
    "#             # Set the modified value in the row\n",
    "#             modified_row_df[param] = value\n",
    "            \n",
    "#             # Scale the entire row\n",
    "#             scaled_row_df = input_scaler.transform(modified_row_df[numerical_columns])\n",
    "            \n",
    "#             # Debug: print the modified DataFrame with the scaled value\n",
    "#             #print(f\"Modified {param} to {value} (scaled):\\n\", scaled_row_df)\n",
    "            \n",
    "#             # Prepare the categorical data\n",
    "#             categorical_data = [modified_row_df[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "            \n",
    "#             # Predict using the model\n",
    "#             y_pred = predict_with_model(model, scaled_row_df, categorical_data)\n",
    "#             y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "            \n",
    "#             # Debug: print the denormalized predictions\n",
    "#             #print(f\"Denormalized predictions for {param}={value}:\\n\", y_pred_denormalized)\n",
    "            \n",
    "#             # Convert to NumPy array for indexing\n",
    "#             y_pred_denormalized = np.array(y_pred_denormalized)\n",
    "\n",
    "#             production_rates = modified_hyperbolic(time_array, \n",
    "#                                                    y_pred_denormalized[0, y_headers.index('Oil_Params_P50_InitialProd')],\n",
    "#                                                    y_pred_denormalized[0, y_headers.index('Oil_Params_P50_DiCoefficient')],\n",
    "#                                                    y_pred_denormalized[0, y_headers.index('Oil_Params_P50_BCoefficient')],\n",
    "#                                                    7,#y_pred_denormalized[0, y_headers.index('Oil_Params_P50_LimDeclineRate')],\n",
    "#                                                    y_pred_denormalized[0, y_headers.index('Oil_Params_P50_BuildupRate')],\n",
    "#                                                    y_pred_denormalized[0, y_headers.index('Oil_Params_P50_MonthsInProd')])\n",
    "#             results[param].append((value, production_rates))\n",
    "            \n",
    "#             # Calculate the difference from the base production rate\n",
    "#             difference = np.mean(np.abs(production_rates - base_production_rates))\n",
    "#             differences.append(difference)\n",
    "        \n",
    "#         avg_differences[param] = np.mean(differences)\n",
    "#         print(f\"Average difference for {param}: {avg_differences[param]}\")\n",
    "    \n",
    "#     return results, avg_differences\n",
    "\n",
    "# def plot_sensitivity_analysis(results, time_array, param, ax, method):\n",
    "#     for value, production in results[param]:\n",
    "#         ax.plot(time_array, production, label=f'{param}={value}')\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_xlabel('Time (Months)', fontsize=12, fontname='Arial')\n",
    "#     ax.set_ylabel('Production Rate (bbl/day)', fontsize=12, fontname='Arial')\n",
    "#     wrapped_title = \"\\n\".join(wrap(f'Sensitivity Analysis for {param} ({method})', 30))\n",
    "#     ax.set_ylim(1, 100000)\n",
    "#     ax.set_title(wrapped_title, fontsize=14, fontweight='bold', fontname='Arial')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, which='both', linestyle='--')\n",
    "\n",
    "# def save_combined_grid_plot(best_performing_rows, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array, output_dir):\n",
    "#     plots_by_combination = {}\n",
    "    \n",
    "#     for (basin, formation, config_str), info in best_performing_rows.items():\n",
    "#         model = info['model']\n",
    "#         best_match_row = info['best_match_row']\n",
    "#         results, avg_differences = generate_sensitivity_data(best_match_row, model, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array)\n",
    "        \n",
    "#         method = 'Neural Network' if 'embedding_output_dim' in config_str else config_str.split(' ')[0]\n",
    "        \n",
    "#         if (basin, formation) not in plots_by_combination:\n",
    "#             plots_by_combination[(basin, formation)] = {'methods': [], 'results': []}\n",
    "        \n",
    "#         plots_by_combination[(basin, formation)]['methods'].append(method)\n",
    "#         plots_by_combination[(basin, formation)]['results'].append(results)\n",
    "\n",
    "#     for (basin, formation), data in plots_by_combination.items():\n",
    "#         methods = data['methods']\n",
    "#         results_list = data['results']\n",
    "        \n",
    "#         fig, axs = plt.subplots(len(methods), 3, figsize=(20, 6 * len(methods)))\n",
    "#         fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "        \n",
    "#         for i, (method, results) in enumerate(zip(methods, results_list)):\n",
    "#             for j, param in enumerate(results.keys()):\n",
    "#                 plot_sensitivity_analysis(results, time_array, param, axs[i, j], method)\n",
    "        \n",
    "#         overall_title = f'Sensitivity Analysis for {basin} - {formation}'\n",
    "#         fig.suptitle(overall_title, fontsize=16, fontweight='bold', fontname='Arial')\n",
    "\n",
    "#         basin_formation_dir = os.path.join(output_dir, f\"{basin}_{formation}\")\n",
    "#         ensure_directory_exists(basin_formation_dir)\n",
    "        \n",
    "#         plot_filename = os.path.join(basin_formation_dir, f'sensitivity_analysis_combined.png')\n",
    "#         plt.savefig(plot_filename, format='png', dpi=300)\n",
    "#         plt.close()\n",
    "        \n",
    "# def save_grid_plot(best_performing_rows, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array, output_dir):\n",
    "#     for (basin, formation, config_str), info in best_performing_rows.items():\n",
    "#         model = info['model']\n",
    "#         best_match_row = info['best_match_row']\n",
    "#         results, avg_differences = generate_sensitivity_data(best_match_row, model, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array)\n",
    "        \n",
    "#         method = 'Neural Network' if 'embedding_output_dim' in config_str else config_str.split(' ')[0]\n",
    "        \n",
    "#         fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "#         fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "        \n",
    "#         for j, param in enumerate(results.keys()):\n",
    "#             plot_sensitivity_analysis(results, time_array, param, axs[j], method)\n",
    "        \n",
    "#         overall_title = f'Sensitivity Analysis for {basin} - {formation} ({method})'\n",
    "#         fig.suptitle(overall_title, fontsize=16, fontweight='bold', fontname='Arial')\n",
    "\n",
    "#         # Create the directory for saving plots if it does not exist\n",
    "#         basin_formation_dir = os.path.join(output_dir, f\"{basin}_{formation}\")\n",
    "#         ensure_directory_exists(basin_formation_dir)\n",
    "        \n",
    "#         # Save the plot\n",
    "#         plot_filename = os.path.join(basin_formation_dir, f'{method}_sensitivity_analysis.png')\n",
    "#         plt.savefig(plot_filename, format='png', dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "\n",
    "# output_dir = 'sensitivity_analysis_plots'\n",
    "# time_array = np.linspace(0, 60, 61)  # time array for 60 months\n",
    "# best_performing_indices = find_best_performing_index(models, test_df, numerical_columns, categorical_columns, y_headers, output_scaler, log_transform_columns)\n",
    "# best_performing_rows = identify_best_performing_rows(models, test_df, numerical_columns, categorical_columns, y_headers, output_scaler, log_transform_columns, best_performing_indices)\n",
    "# save_combined_grid_plot(best_performing_rows, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array, output_dir)\n",
    "# save_grid_plot(best_performing_rows, numerical_columns, categorical_columns, input_scaler, output_scaler, log_transform_columns, y_headers, time_array, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b251577-c6d2-4ea9-b76a-1436ae3f1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Defining a dictionary to store all outputs\n",
    "outputs = {\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "    'models': models,\n",
    "    'input_scaler':input_scaler,\n",
    "   # 'shap_values_dict': shap_values_dict,\n",
    "    'evaluation_results': evaluation_results,\n",
    "    'best_performing_models': best_performing_models,\n",
    "    'output_scaler':output_scaler,\n",
    "    'task_times': task_times,\n",
    "    'log_transform_columns':log_transform_columns,\n",
    "    'y_headers': y_headers,\n",
    "    'numerical_columns': numerical_columns,\n",
    "    'categorical_columns': categorical_columns\n",
    "}\n",
    "\n",
    "# Saving the dictionary to a pickle file\n",
    "output_file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\outputs_NN_only_withHL.pkl'\n",
    "with open(output_file_path, 'wb') as file:\n",
    "    pickle.dump(outputs, file)\n",
    "\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f152c7c-a330-47e3-87e2-cb6227bef339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Type Curve Scaling\n",
    "file_path = r'C:\\Users\\Prakhar.Sarkar\\OneDrive - SRP Management Services\\Documents\\_For_Prakhar\\TCTest.xlsx'\n",
    "\n",
    "# Load the  file with the specified dtype\n",
    "Testing = pd.read_excel(file_path, dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704e8e7-e091-478a-8f99-13c3c25bc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'AVG_' prefix from the headers\n",
    "Testing = Testing.rename(columns={col: col.replace('AVG_', '') for col in Testing.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde76a8-18f5-4083-9bb7-11253bf5ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing['FluidPerFoot_bblft'] = Testing['FluidPerFoot'] / 42\n",
    "# Drop the original columns\n",
    "Testing.drop(columns=['FluidPerFoot'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad7f9f-6594-4bf2-ad5b-f961e2ccaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List updated headers\n",
    "updated_headers = Testing.columns.tolist()\n",
    "print(\"Updated Headers:\")\n",
    "print(updated_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a12606-d9af-41ac-80cd-4f938b700e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be parsed\n",
    "param_columns = [\n",
    "    'Oil_DCA_Parameters', 'Gas_DCA_Parameters', 'Water_DCA_Parameters'\n",
    "]\n",
    "\n",
    "# Apply the functions in sequence\n",
    "Testing, new_columns = split_parameters(Testing, param_columns)\n",
    "# Display the results\n",
    "print(new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3000ba-9deb-4aea-8cc1-e32f8bb5c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {\n",
    "    'Oil_DCA_Parameters_Method': 'Oil_Params_P50_Method',\n",
    "    'Oil_DCA_Parameters_BuildupRate': 'Oil_Params_P50_BuildupRate',\n",
    "    'Oil_DCA_Parameters_MonthsInProd': 'Oil_Params_P50_MonthsInProd',\n",
    "    'Oil_DCA_Parameters_InitialProd': 'Oil_Params_P50_InitialProd',\n",
    "    'Oil_DCA_Parameters_DiCoefficient': 'Oil_Params_P50_DiCoefficient',\n",
    "    'Oil_DCA_Parameters_BCoefficient': 'Oil_Params_P50_BCoefficient',\n",
    "    'Oil_DCA_Parameters_LimDeclineRate': 'Oil_Params_P50_LimDeclineRate',\n",
    "    'Gas_DCA_Parameters_Method': 'Gas_Params_P50_Method',\n",
    "    'Gas_DCA_Parameters_BuildupRate': 'Gas_Params_P50_BuildupRate',\n",
    "    'Gas_DCA_Parameters_MonthsInProd': 'Gas_Params_P50_MonthsInProd',\n",
    "    'Gas_DCA_Parameters_InitialProd': 'Gas_Params_P50_InitialProd',\n",
    "    'Gas_DCA_Parameters_DiCoefficient': 'Gas_Params_P50_DiCoefficient',\n",
    "    'Gas_DCA_Parameters_BCoefficient': 'Gas_Params_P50_BCoefficient',\n",
    "    'Gas_DCA_Parameters_LimDeclineRate': 'Gas_Params_P50_LimDeclineRate',\n",
    "    'Water_DCA_Parameters_Method': 'Water_Params_P50_Method',\n",
    "    'Water_DCA_Parameters_BuildupRate': 'Water_Params_P50_BuildupRate',\n",
    "    'Water_DCA_Parameters_MonthsInProd': 'Water_Params_P50_MonthsInProd',\n",
    "    'Water_DCA_Parameters_InitialProd': 'Water_Params_P50_InitialProd',\n",
    "    'Water_DCA_Parameters_DiCoefficient': 'Water_Params_P50_DiCoefficient',\n",
    "    'Water_DCA_Parameters_BCoefficient': 'Water_Params_P50_BCoefficient',\n",
    "    'Water_DCA_Parameters_LimDeclineRate': 'Water_Params_P50_LimDeclineRate'\n",
    "}\n",
    "\n",
    "# Rename the columns using the mapping\n",
    "Testing = Testing.rename(columns=mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c59acd-1557-49a0-a060-c09cbc040e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group by 'Polygon_Name', count occurrences, and sum the 'Unique_TC_IDs' value\n",
    "def group_and_summarize_by_polygon(df):\n",
    "    df_new = df[['Unique_TC_ID', 'Polygon_Name']].drop_duplicates()\n",
    "    grouped_df = df_new.groupby('Polygon_Name', as_index=False).count()\n",
    "    unique_tc_ids_sum = df_new.groupby('Polygon_Name')['Unique_TC_ID'].sum().reset_index()\n",
    "    grouped_df['Unique_TC_IDs_Sum'] = unique_tc_ids_sum['Unique_TC_ID']\n",
    "    return grouped_df, unique_tc_ids_sum['Unique_TC_ID'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d89ba9-8645-4d77-be69-6536e80015c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df, total_unique_tc_ids_sum = group_and_summarize_by_polygon(Testing)\n",
    "print(\"Grouped DataFrame:\")\n",
    "print(grouped_df)\n",
    "print(\"\\nTotal sum of Unique_TC_IDs:\")\n",
    "print(total_unique_tc_ids_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0e50b-871f-4bf1-adf0-a3db61e15f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns ending with '_Method'\n",
    "Testing = Testing.drop(columns=[col for col in Testing.columns if col.endswith('_Method')])\n",
    "\n",
    "columns_to_drop=['Unique_TC_ID','Status','Polygon_Name',\n",
    "                 'Oil_Params_P50_LimDeclineRate',\n",
    "                  'Gas_Params_P50_LimDeclineRate',\n",
    "                 'Water_Params_P50_LimDeclineRate'\n",
    "]\n",
    "# Dropping the columns\n",
    "Testing.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6eed6-4621-4ed6-a6ac-f0bca5c8b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure BasinTC and FORMATION_CONDENSED are categorical\n",
    "Testing['BasinTC'] = Testing['BasinTC'].astype(str)\n",
    "Testing['FORMATION_CONDENSED'] = Testing['FORMATION_CONDENSED'].astype(str)\n",
    "Testing = convert_to_absolute_values(Testing)\n",
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c50959-bb58-45d6-b9f6-4d82cd802969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variations\n",
    "variations = {\n",
    "    'HORIZONTIAL_WELL_LENGTH': np.arange(7000, 14000, 1000)#,\n",
    "   # 'ProppantPerFoot': np.arange(1000, 2200, 200),\n",
    "   # 'FluidPerFoot_bblft': np.arange(40, 70, 10)\n",
    "}\n",
    "# Remove 10000 from the array\n",
    "variations['HORIZONTIAL_WELL_LENGTH'] = np.delete(variations['HORIZONTIAL_WELL_LENGTH'], np.where(variations['HORIZONTIAL_WELL_LENGTH'] == 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970817ac-0465-46a2-a516-0bd049e39f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_inputs(*args):\n",
    "    for arg in args:\n",
    "        if np.isnan(arg) or np.isinf(arg):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Helper function to generate production rates\n",
    "def generate_production_rates_testing(y_pred_denormalized, headers, time, resource_type='Oil', use_baseline=False):\n",
    "    productions = []\n",
    "\n",
    "    if resource_type == 'Oil':\n",
    "        prefix = 'Oil_Params_P50_'\n",
    "    elif resource_type == 'Gas':\n",
    "        prefix = 'Gas_Params_P50_'\n",
    "    elif resource_type == 'Water':\n",
    "        prefix = 'Water_Params_P50_'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resource type. Must be 'Oil', 'Gas', or 'Water'.\")\n",
    "\n",
    "    for idx in range(len(y_pred_denormalized)):\n",
    "        try:\n",
    "            if use_baseline:\n",
    "                qi = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd_baseline')]\n",
    "                di = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient_baseline')]\n",
    "                b = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient_baseline')]\n",
    "                IBU = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate_baseline')]\n",
    "                MBU = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd_baseline')]\n",
    "                Dlim = 7# y_pred_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate_baseline')]\n",
    "            else:\n",
    "                qi = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}InitialProd')]\n",
    "                di = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}DiCoefficient')]\n",
    "                b = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BCoefficient')]\n",
    "                IBU = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}BuildupRate')]\n",
    "                MBU = y_pred_denormalized.iloc[idx, headers.index(f'{prefix}MonthsInProd')]\n",
    "                Dlim =7# y_pred_denormalized.iloc[idx, headers.index(f'{prefix}LimDeclineRate')]\n",
    "\n",
    "            if not validate_inputs(qi, di, b, Dlim, IBU, MBU):\n",
    "                print(f\"Skipping invalid prediction inputs at index {idx}: {qi}, {di}, {b}, {Dlim}, {IBU}, {MBU}\")\n",
    "                productions.append(np.zeros_like(time))\n",
    "                continue\n",
    "\n",
    "            production = modified_hyperbolic(time, qi, di, b, Dlim, IBU, MBU)[1]\n",
    "            productions.append(production)\n",
    "            #print(production)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating production rates: {e}\")\n",
    "            productions.append(np.zeros_like(time))\n",
    "\n",
    "    return productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d918c20-2b77-4809-93cb-3e39666feb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing = calculate_combined_eur(Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37869e2f-46f1-457c-a6ab-b6c8fb404e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the order of the columns in Testing matches the model requirements\n",
    "Testing = Testing[['BasinTC', 'FORMATION_CONDENSED'] + numerical_columns + categorical_columns + y_headers]\n",
    "baseline_columns = [f'{param}_baseline' for param in y_headers]\n",
    "for col in baseline_columns:\n",
    "    Testing[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0a6f2-cdc3-4e6d-bd3c-e6ae14ce23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input and output features\n",
    "Testing[numerical_columns] = input_scaler.transform(Testing[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6481d05-e0e3-473c-9eb6-ff16a1f4e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02116ee0-e4d0-42f7-af4a-5882db01d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (basin, formation, config_str), model in models.items():\n",
    "    # Filter the data by basin and formation\n",
    "    combo_test = filter_by_basin_and_formation(Testing, basin, formation)\n",
    "\n",
    "    # Prepare the data for prediction\n",
    "    numerical_data = combo_test[numerical_columns].values\n",
    "    categorical_data = [combo_test[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "\n",
    "    # Predict baseline decline parameters\n",
    "    y_pred = predict_with_model(model, numerical_data, categorical_data)\n",
    "    y_pred_denormalized = denormalize_data_output(y_pred, output_scaler, log_transform_columns)\n",
    "    # Ensure the indices are aligned\n",
    "    y_pred_denormalized.index = combo_test.index\n",
    "\n",
    "    # Store baseline predictions\n",
    "    for param, baseline_col in zip(y_headers, baseline_columns):\n",
    "        Testing.loc[combo_test.index, baseline_col] = y_pred_denormalized[param]\n",
    "\n",
    "# Ensure there are no NaN values in the baseline columns\n",
    "Testing.dropna(subset=baseline_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425738e8-0008-4509-b106-9c3ddeb46d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip values to prevent overflow\n",
    "def clip_values(df, lower=-1e5, upper=1e5):\n",
    "    return df.clip(lower=lower, upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db439f-81a7-4f48-a4b3-77dcac0f9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316b891-86ae-4cb8-88cd-ac183f867bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dictionary\n",
    "results = {}\n",
    "\n",
    "for (basin, formation, config_str), model in models.items():\n",
    "    results[(basin, formation, config_str)] = {}\n",
    "    \n",
    "    # Filter the rows corresponding to the current basin and formation\n",
    "    formation_df = Testing[(Testing['BasinTC'] == basin) & (Testing['FORMATION_CONDENSED'] == formation)]\n",
    "\n",
    "    for well_length in variations['HORIZONTIAL_WELL_LENGTH']:\n",
    "        varied_dfs = []\n",
    "\n",
    "        # Process each row individually\n",
    "        for index, row in formation_df.iterrows():\n",
    "            # Create a DataFrame for the current row\n",
    "            varied_df = pd.DataFrame([row])\n",
    "\n",
    "            # Denormalize and decode inputs\n",
    "            varied_df = denormalize_and_decode_inputs(varied_df, numerical_columns, categorical_columns, input_scaler, encoders)\n",
    "\n",
    "            # Adjust the specific feature being varied\n",
    "            varied_df['HORIZONTIAL_WELL_LENGTH'] = well_length\n",
    "\n",
    "            # Scale the entire DataFrame again\n",
    "            varied_df[numerical_columns] = input_scaler.transform(varied_df[numerical_columns])\n",
    "\n",
    "            # Prepare the data for prediction with adjusted features\n",
    "            numerical_data_varied = varied_df[numerical_columns].values\n",
    "            categorical_data_varied = [varied_df[col].astype(int).values.reshape(-1, 1) for col in categorical_columns]\n",
    "\n",
    "            # Predict new decline parameters using the selected model\n",
    "            new_predictions = predict_with_model(model, numerical_data_varied, categorical_data_varied)\n",
    "            new_predictions_denormalized = denormalize_data_output(new_predictions, output_scaler, log_transform_columns)\n",
    "            \n",
    "            # Clip the predictions to prevent overflow\n",
    "            new_predictions_denormalized = clip_values(new_predictions_denormalized)\n",
    "            \n",
    "            # Debugging: Print the shape and type of new predictions\n",
    "            #print(\"Well Length\",well_length)\n",
    "            # print(\"New Predictions Denormalized Shape:\", new_predictions_denormalized.shape)\n",
    "            # print(\"New Predictions Denormalized Type:\", type(new_predictions_denormalized))\n",
    "            # print(\"New Predictions Denormalized:\\n\", new_predictions_denormalized)\n",
    "\n",
    "            # Calculate scaling factors\n",
    "            scaling_factors = {}\n",
    "            for param, baseline_param in zip(y_headers, baseline_columns):\n",
    "                try:\n",
    "                    baseline_values = varied_df[baseline_param].values\n",
    "                    new_prediction_values = new_predictions_denormalized[param].values\n",
    "                    scaling_factors[param] = new_prediction_values / baseline_values\n",
    "                    #print(scaling_factors[param])\n",
    "                    # Debugging: Print scaling factors\n",
    "                    print(f\"Scaling factors for {param}: {scaling_factors[param]}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"KeyError: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "            # Apply scaling factors to the original type curves in a new DataFrame\n",
    "            scaled_df = varied_df.copy()\n",
    "            for param in y_headers:\n",
    "                if '_P50_MonthsInProd' not in param and '_P50_LimDeclineRate' not in param:  # Skip scaling for _P50_MonthsInProd columns\n",
    "                    try:\n",
    "                        #print(param)\n",
    "                        scaled_df[param] = varied_df[param] * scaling_factors[param]\n",
    "                        # Handle potential overflow by capping the values\n",
    "                        scaled_df[param] = np.clip(scaled_df[param], -1e5, 1e5)\n",
    "                    except KeyError as e:\n",
    "                        print(f\"KeyError: {e} for parameter {param}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Unexpected error: {e} for parameter {param}\")\n",
    "\n",
    "            # Select only the y_headers columns for production rate generation\n",
    "            scaled_y_df = scaled_df[y_headers].copy()\n",
    "            varied_dfs.append(scaled_y_df)\n",
    "\n",
    "        # Store results for this variation\n",
    "        if (basin, formation, config_str) not in results:\n",
    "            results[(basin, formation, config_str)] = {}\n",
    "        if well_length not in results[(basin, formation, config_str)]:\n",
    "            results[(basin, formation, config_str)][well_length] = []\n",
    "        results[(basin, formation, config_str)][well_length].append(varied_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7a7a8-5af6-4d14-94b3-e79653696e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variable for production rate generation\n",
    "years=20\n",
    "time = np.linspace(1, 12 * years, 12 * years)  # 5 Years  # Example: 360 months (30 years)\n",
    "\n",
    "# Generate production rates for each model, basin, formation, and variation\n",
    "for (basin, formation, config_str), variations in results.items():\n",
    "    for well_length, varied_dfs_list in variations.items():\n",
    "        all_productions = []\n",
    "        for varied_dfs in varied_dfs_list:\n",
    "            for scaled_y_df in varied_dfs:\n",
    "                # Ensure scaled_y_df is a DataFrame\n",
    "                if not isinstance(scaled_y_df, pd.DataFrame):\n",
    "                    scaled_y_df = pd.DataFrame(scaled_y_df)\n",
    "\n",
    "                # Compute production rates using the scaled_y_df\n",
    "                productions = generate_production_rates_testing(scaled_y_df, y_headers, time, resource_type='Oil', use_baseline=False)\n",
    "                all_productions.append(productions)\n",
    "\n",
    "                # Store the production rates back into the dataframe\n",
    "                scaled_y_df[f'Production_Rates_{well_length}'] = productions\n",
    "\n",
    "        # Update the results dictionary with the new production rates\n",
    "        results[(basin, formation, config_str)][well_length] = {\n",
    "            'scaled_y_dfs': varied_dfs_list,\n",
    "            'productions': all_productions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895145f-c46c-4a16-ba9d-c68e43d91084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_type_curves(basin, formation, config_str, original_df, results):\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot the original type curves\n",
    "    original_production = generate_production_rates_testing(original_df[y_headers], y_headers, time, resource_type='Oil', use_baseline=False)\n",
    "    ax.plot(time, np.array(original_production).flatten(), 'k--', label='Original')\n",
    "\n",
    "    # Plot the scaled type curves for each variation\n",
    "    for well_length, data in results.items():\n",
    "        # Ensure 'productions' exists in the data dictionary\n",
    "        if 'productions' in data:\n",
    "            for scaled_production in data['productions']:\n",
    "                scaled_production_flattened = np.array(scaled_production).flatten()\n",
    "\n",
    "                # Check for length mismatch before plotting\n",
    "                if len(time) != len(scaled_production_flattened):\n",
    "                    print(f\"Skipping plot for Well Length {well_length} due to length mismatch: time length {len(time)}, production length {len(scaled_production_flattened)}\")\n",
    "                    continue\n",
    "\n",
    "                # Plot the scaled production\n",
    "                ax.plot(time, scaled_production_flattened, label=f'Well Length {well_length}')\n",
    "\n",
    "    # Set the y-scale to logarithmic and adjust the limits accordingly\n",
    "    ax.set_yscale('log')\n",
    "    #ax.set_ylim(100, 1000000)\n",
    "    ax.set_xlim(0, max(time))  # Ensure the x-axis limits are appropriate\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax.set_title(f'Type Curves for {basin} - {formation} - {config_str}', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Time (Months)')\n",
    "    ax.set_ylabel('Production Rate (bbl/day)')\n",
    "    \n",
    "    # Display the legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True, which='both', linestyle='--')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Loop through each model and plot results\n",
    "for (basin, formation, config_str), variations in results.items():\n",
    "    if 'embedding_output_dim' in config_str or 'dense_layer_sizes' in config_str:\n",
    "        model_type = 'Neural Network'\n",
    "    else:\n",
    "        model_type = config_str.split(' ')[0]\n",
    "    formation_df = Testing[(Testing['BasinTC'] == basin) & (Testing['FORMATION_CONDENSED'] == formation)]\n",
    "    plot_type_curves(basin, formation, model_type, formation_df, variations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c810bb1-656b-429b-bea8-fafca71cd489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fca5a-a655-4284-92db-e3550f655204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdf046-b686-48cf-823c-3a87ed2ae315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
